services:
  spark-iceberg-main:
    image: tabulario/spark-iceberg
    container_name: spark-iceberg-main
    depends_on:
      - rest
      - minio
    networks:
      - iceberg_net_main
    volumes:
      - ./warehouse:/home/iceberg/warehouse
      - ./data:/home/iceberg/data
      - ./notebooks:/home/iceberg/notebooks/notebooks
      - ../../:/workspace
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    ports:
      - "8890:8888"
      - "8081:8080"
      - "4045-4047:4040-4042"

  rest:
    image: tabulario/iceberg-rest
    container_name: iceberg-rest-main
    networks:
      - iceberg_net_main
    ports:
      - "8182:8181"
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000

  minio:
    image: minio/minio
    container_name: minio-main
    networks:
      iceberg_net_main:
        aliases:
          - warehouse.minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    ports:
      - "9002:9000"
      - "9003:9001"
    command: ["server", "/data", "--console-address", ":9001"]

  mc:
    image: minio/mc
    container_name: mc-main
    depends_on:
      - minio
    networks:
      - iceberg_net_main
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc alias set minio http://minio:9000/ admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc mb minio/warehouse || echo 'Warehouse already exists';
      /usr/bin/mc policy set public minio/warehouse;
      tail -f /dev/null
      "

  airflow:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    image: local/airflow-main:2.10.2
    container_name: airflow-main
    depends_on:
      - spark-iceberg-main
    networks:
      - iceberg_net_main
    user: "0:0"
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - SPARK_CONTAINER_NAME=spark-iceberg-main
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ../../:/workspace
      - airflow_home:/opt/airflow
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8085:8080"
    command: standalone

networks:
  iceberg_net_main:

volumes:
  airflow_home:
