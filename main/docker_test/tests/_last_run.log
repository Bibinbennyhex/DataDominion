Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:08:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:08:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 6) / 6]                                                                                [PASS] simple_test
2026-02-12 11:08:35,187 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:08:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:08:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:08:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=========>                                                 (1 + 5) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:=================================================>         (5 + 1) / 6]                                                                                [SETUP] Resetting tables...
[SETUP] Seeding existing summary/latest data...
[SETUP] Loading incoming source batch...
[RUN] Executing main pipeline...
2026-02-12 11:08:54,144 INFO: Cleaned case_1
2026-02-12 11:08:54,167 INFO: Cleaned case_2
2026-02-12 11:08:54,195 INFO: Cleaned case_3a
2026-02-12 11:08:54,217 INFO: Cleaned case_3b
2026-02-12 11:08:54,241 INFO: Cleaned case_4
2026-02-12 11:08:54,242 INFO: CLEANUP COMPLETED
2026-02-12 11:08:54,242 INFO: ============================================================
2026-02-12 11:08:54,243 INFO: ================================================================================
2026-02-12 11:08:54,243 INFO: SUMMARY PIPELINE - START
2026-02-12 11:08:54,244 INFO: ================================================================================
2026-02-12 11:08:54,244 INFO: ================================================================================
2026-02-12 11:08:54,245 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:08:54,245 INFO: ================================================================================
2026-02-12 11:08:56,700 INFO: Loading accounts from primary_catalog.main_test.accounts_all
2026-02-12 11:08:56,865 INFO: Reading from primary_catalog.main_test.accounts_all - (rpt_as_of_mo < 2026-02) & (base_ts > 2026-01-01 00:00:00
2026-02-12 11:08:56,866 INFO: Preparing source data...
2026-02-12 11:08:57,052 INFO: Applied 36 column mappings
2026-02-12 11:08:57,580 INFO: Applied 7 column transformations
2026-02-12 11:08:57,739 INFO: Created 2 inferred columns
2026-02-12 11:08:58,749 INFO: Validated 7 date columns
2026-02-12 11:08:58,928 INFO: Source data preparation complete
2026-02-12 11:08:59,091 INFO: Loading summary metadata from primary_catalog.main_test.latest_summary
2026-02-12 11:08:59,148 INFO: Loaded metadata for existing accounts
2026-02-12 11:08:59,149 INFO: Classifying accounts into Case I/II/III/IV
[Stage 25:====>                                                  (18 + 6) / 200][Stage 25:=======>                                               (28 + 6) / 200][Stage 25:==========>                                            (39 + 6) / 200][Stage 25:==============>                                        (51 + 7) / 200][Stage 25:=================>                                     (63 + 7) / 200][Stage 25:====================>                                  (75 + 6) / 200][Stage 25:=======================>                               (87 + 6) / 200][Stage 25:===========================>                          (103 + 6) / 200][Stage 25:===============================>                      (115 + 6) / 200][Stage 25:==================================>                   (126 + 6) / 200][Stage 25:=====================================>                (138 + 7) / 200][Stage 25:========================================>             (149 + 6) / 200][Stage 25:===========================================>          (161 + 6) / 200][Stage 25:===============================================>      (177 + 6) / 200][Stage 25:==================================================>   (186 + 6) / 200][Stage 28:============>                                          (47 + 6) / 200][Stage 28:==================>                                    (67 + 6) / 200][Stage 28:=======================>                               (85 + 7) / 200][Stage 28:=============================>                        (110 + 7) / 200][Stage 28:===================================>                  (131 + 6) / 200][Stage 28:========================================>             (150 + 7) / 200][Stage 28:=============================================>        (170 + 6) / 200][Stage 28:===================================================>  (189 + 6) / 200]                                                                                2026-02-12 11:09:08,710 INFO: ------------------------------------------------------------
2026-02-12 11:09:08,711 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:09:08,711 INFO:   CASE_I: 2 records
2026-02-12 11:09:08,712 INFO:   CASE_IV: 1 records
2026-02-12 11:09:08,713 INFO:   CASE_III: 1 records
2026-02-12 11:09:08,713 INFO:   CASE_II: 1 records
2026-02-12 11:09:08,714 INFO: ------------------------------------------------------------
2026-02-12 11:09:08,714 INFO: Classification | Time Elapsed: 0.24 minutes
2026-02-12 11:09:08,747 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:09:08,748 INFO: ================================================================================
2026-02-12 11:09:08,748 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:09:08,749 INFO: ================================================================================
2026-02-12 11:09:08,749 INFO: CASE III - Processing backfill records
[Stage 35:======>                                                (25 + 7) / 200][Stage 35:==============>                                        (52 + 6) / 200][Stage 35:====================>                                  (74 + 6) / 200][Stage 35:===========================>                          (103 + 6) / 200][Stage 35:==================================>                   (128 + 6) / 200][Stage 35:==========================================>           (157 + 6) / 200][Stage 35:================================================>     (180 + 6) / 200]                                                                                2026-02-12 11:09:11,196 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:09:11,196 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:09:11,222 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:09:11,704 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:09:13,233 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 42:==========>                                            (38 + 6) / 200][Stage 42:==============>                                        (54 + 7) / 200][Stage 42:=====================>                                 (78 + 7) / 200][Stage 42:==========================>                            (96 + 6) / 200][Stage 42:================================>                     (119 + 6) / 200][Stage 42:======================================>               (144 + 6) / 200][Stage 42:============================================>         (163 + 6) / 200][Stage 42:==================================================>   (188 + 6) / 200]                                                                                [Stage 46:>                                                       (0 + 6) / 200][Stage 46:=>                                                      (6 + 6) / 200][Stage 46:==>                                                     (8 + 6) / 200][Stage 46:===>                                                   (13 + 6) / 200][Stage 46:====>                                                  (17 + 6) / 200][Stage 46:=====>                                                 (21 + 6) / 200][Stage 46:======>                                                (25 + 7) / 200][Stage 46:========>                                              (30 + 6) / 200][Stage 46:=========>                                             (35 + 6) / 200][Stage 46:===========>                                           (41 + 6) / 200][Stage 46:============>                                          (46 + 6) / 200][Stage 46:=============>                                         (50 + 6) / 200][Stage 50:=================================>                      (30 + 6) / 50][Stage 50:=====================================================>  (48 + 2) / 50]                                                                                2026-02-12 11:09:31,794 INFO: Case III Part A Generated | Time Elapsed: 0.38 minutes
2026-02-12 11:09:31,795 INFO: ------------------------------------------------------------
2026-02-12 11:09:31,796 INFO: Part B: Updating future summary rows...
[Stage 53:==================>                                    (66 + 7) / 200][Stage 53:=========================>                             (92 + 6) / 200][Stage 53:===============================>                      (117 + 6) / 200][Stage 53:=====================================>                (138 + 7) / 200][Stage 53:=============================================>        (169 + 6) / 200][Stage 53:=================================================>    (183 + 6) / 200]                                                                                [Stage 66:=================================================>     (90 + 6) / 100]                                                                                2026-02-12 11:09:36,734 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 72:=======================>                               (85 + 6) / 200][Stage 72:============================>                         (106 + 6) / 200][Stage 72:===================================>                  (132 + 6) / 200][Stage 72:========================================>             (149 + 8) / 200][Stage 72:===============================================>      (176 + 6) / 200]                                                                                [Stage 76:============>                                          (46 + 6) / 200][Stage 76:==================>                                    (66 + 7) / 200][Stage 76:=======================>                               (84 + 6) / 200][Stage 76:===========================>                          (101 + 7) / 200][Stage 76:===============================>                      (116 + 6) / 200][Stage 76:==================================>                   (127 + 6) / 200]                                                                                2026-02-12 11:09:37,572 INFO: Case III Part B Generated | Time Elapsed: 0.10 minutes
2026-02-12 11:09:37,573 INFO: ------------------------------------------------------------
2026-02-12 11:09:37,597 INFO: 
>>> PROCESSING NEW ACCOUNTS (2 records)
2026-02-12 11:09:37,597 INFO: ================================================================================
2026-02-12 11:09:37,598 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:09:37,598 INFO: ================================================================================
2026-02-12 11:09:37,598 INFO: Processing new accounts
2026-02-12 11:09:38,257 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=2, scale=1.0)
[Stage 86:=====================>                                 (78 + 6) / 200][Stage 86:===============================>                      (117 + 6) / 200][Stage 86:=========================================>            (152 + 6) / 200][Stage 86:====================================================> (194 + 6) / 200][Stage 90:========================================>               (36 + 6) / 50]                                                                                2026-02-12 11:09:42,988 INFO: Case I Generated | Time Elapsed: 0.09 minutes
2026-02-12 11:09:42,990 INFO: ------------------------------------------------------------
2026-02-12 11:09:43,009 INFO: 
>>> PROCESSING BULK HISTORICAL (1 records)
2026-02-12 11:09:43,010 INFO: ================================================================================
2026-02-12 11:09:43,011 INFO: STEP 2d: Process Case IV (Bulk Historical Load)
2026-02-12 11:09:43,011 INFO: ================================================================================
2026-02-12 11:09:43,012 INFO: Processing bulk historical records
2026-02-12 11:09:43,059 INFO: Building complete history using window functions
[Stage 93:=============================>                        (109 + 6) / 200][Stage 93:=======================================>              (148 + 6) / 200][Stage 93:===============================================>      (177 + 6) / 200]                                                                                2026-02-12 11:09:46,839 INFO: Write partitions [case_4_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 100:=>            (22 + 6) / 200][Stage 101:>              (0 + 0) / 200]                                                                                [Stage 105:===============>                                     (114 + 6) / 400][Stage 105:=================>                                   (134 + 6) / 400][Stage 105:======================>                              (169 + 7) / 400][Stage 105:==========================>                          (200 + 6) / 400][Stage 105:===============================>                     (236 + 6) / 400][Stage 105:====================================>                (276 + 6) / 400][Stage 105:==========================================>          (321 + 6) / 400][Stage 105:===============================================>     (359 + 7) / 400]                                                                                [Stage 112:====>                                                 (34 + 6) / 400][Stage 112:======>                                               (49 + 6) / 400][Stage 112:=========>                                            (68 + 7) / 400][Stage 112:===========>                                          (85 + 6) / 400][Stage 112:=============>                                       (103 + 6) / 400][Stage 112:==============>                                      (113 + 6) / 400][Stage 112:================>                                    (123 + 7) / 400][Stage 112:==================>                                  (140 + 7) / 400][Stage 112:====================>                                (157 + 7) / 400][Stage 112:=======================>                             (174 + 6) / 400][Stage 112:========================>                            (187 + 6) / 400][Stage 112:==========================>                          (200 + 6) / 400][Stage 112:============================>                        (218 + 6) / 400][Stage 112:===============================>                     (236 + 6) / 400][Stage 112:=================================>                   (251 + 5) / 400][Stage 112:==================================>                  (260 + 6) / 400][Stage 112:===================================>                 (271 + 6) / 400][Stage 112:======================================>              (288 + 6) / 400][Stage 112:========================================>            (307 + 6) / 400][Stage 112:===========================================>         (329 + 6) / 400][Stage 112:=============================================>       (347 + 6) / 400][Stage 112:================================================>    (363 + 6) / 400][Stage 112:==================================================>  (381 + 6) / 400][Stage 112:====================================================>(398 + 2) / 400]                                                                                2026-02-12 11:10:01,419 INFO: Case IV Generated | Time Elapsed: 0.31 minutes
2026-02-12 11:10:01,420 INFO: ------------------------------------------------------------
2026-02-12 11:10:01,465 INFO: ------------------------------------------------------------
2026-02-12 11:10:01,466 INFO: MERGING RECORDS:
2026-02-12 11:10:03,036 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.03 minutes
2026-02-12 11:10:03,039 INFO: ------------------------------------------------------------
2026-02-12 11:10:04,574 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.03 minutes
2026-02-12 11:10:04,575 INFO: ------------------------------------------------------------
2026-02-12 11:10:04,576 INFO: APPENDING RECORDS:
2026-02-12 11:10:04,644 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=3, scale=1.0)
2026-02-12 11:10:05,488 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:10:05,489 INFO: ------------------------------------------------------------
2026-02-12 11:10:05,490 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:10:06,478 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:10:06,478 INFO: ------------------------------------------------------------
2026-02-12 11:10:06,515 INFO: 
>>> PROCESSING FORWARD ENTRIES (1 records)
2026-02-12 11:10:06,516 INFO: ================================================================================
2026-02-12 11:10:06,516 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:10:06,517 INFO: ================================================================================
2026-02-12 11:10:06,517 INFO: Processing forward entries
2026-02-12 11:10:06,562 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:10:06,971 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 149:===>          (44 + 6) / 200][Stage 150:>              (0 + 0) / 200][Stage 149:(65 + 7) / 200][Stage 150:>(0 + 0) / 200][Stage 151:>(0 + 0) / 200][Stage 149:(90 + 6) / 200][Stage 150:>(0 + 0) / 200][Stage 151:>(0 + 0) / 200][Stage 149:(102 + 6) / 200][Stage 150:>(0 + 0) / 200][Stage 151:>(0 + 0) / 200][Stage 149:(121 + 6) / 200][Stage 150:>(0 + 0) / 200][Stage 151:>(0 + 0) / 200][Stage 149:(143 + 6) / 200][Stage 150:>(0 + 0) / 200][Stage 151:>(0 + 0) / 200][Stage 149:(170 + 6) / 200][Stage 150:>(0 + 0) / 200][Stage 151:>(0 + 0) / 200][Stage 149:(198 + 2) / 200][Stage 150:>(2 + 4) / 200][Stage 151:>(0 + 0) / 200][Stage 150:==>          (42 + 11) / 200][Stage 151:>              (0 + 0) / 200][Stage 150:=======>     (113 + 6) / 200][Stage 151:>              (0 + 0) / 200][Stage 150:===========> (175 + 7) / 200][Stage 151:>              (0 + 0) / 200][Stage 151:=========>                                            (36 + 6) / 200][Stage 151:==========================>                          (100 + 6) / 200][Stage 151:=======================================>             (150 + 6) / 200][Stage 151:===================================================> (193 + 7) / 200][Stage 152:=======>                                              (29 + 6) / 200][Stage 152:=============>                                        (50 + 7) / 200]26/02/12 11:10:11 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:335)
	org.apache.spark.sql.execution.CacheManager.$anonfun$cacheQuery$2(CacheManager.scala:127)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:122)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)
	org.apache.spark.sql.Dataset.persist(Dataset.scala:3797)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[Stage 152:================>                                     (62 + 6) / 200][Stage 152:========================>                             (91 + 6) / 200][Stage 152:==================================>                  (130 + 6) / 200][Stage 152:===========> (171 + 6) / 200][Stage 153:>              (0 + 0) / 200][Stage 152:============>(199 + 1) / 200][Stage 153:>              (0 + 5) / 200]                                                                                [Stage 153:===========================>                         (104 + 8) / 200][Stage 153:====================================>                (136 + 6) / 200][Stage 153:===========================================>         (166 + 6) / 200]                                                                                [Stage 157:====>                                                 (16 + 6) / 200][Stage 157:======>                                               (25 + 6) / 200][Stage 157:=========>                                            (36 + 6) / 200][Stage 157:=============>                                        (49 + 6) / 200][Stage 157:================>                                     (60 + 6) / 200][Stage 157:==================>                                   (68 + 6) / 200][Stage 157:===================>                                  (74 + 7) / 200][Stage 157:======================>                               (84 + 6) / 200][Stage 157:========================>                             (92 + 6) / 200][Stage 157:===========================>                         (105 + 6) / 200]                                                                                2026-02-12 11:10:18,233 INFO: Case II Generated | Time Elapsed: 0.20 minutes
2026-02-12 11:10:18,234 INFO: ------------------------------------------------------------
2026-02-12 11:10:18,234 INFO: ------------------------------------------------------------
2026-02-12 11:10:18,235 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:10:18,258 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:10:18,900 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:10:18,901 INFO: ------------------------------------------------------------
2026-02-12 11:10:26,092 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.12 minutes
2026-02-12 11:10:26,093 INFO: ------------------------------------------------------------
2026-02-12 11:10:26,093 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[ASSERT] Validating outputs...
[PASS] test_main_all_cases
2026-02-12 11:10:21,981 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:10:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:10:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:10:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6]                                                                                [Stage 4:=========>                                                 (1 + 5) / 6]                                                                                [SETUP] Resetting tables...
[SETUP] Seeding existing summary/latest data...
[SETUP] Loading one backfill input record...
[RUN] Executing main pipeline...
2026-02-12 11:10:43,451 INFO: Cleaned case_1
2026-02-12 11:10:43,478 INFO: Cleaned case_2
2026-02-12 11:10:43,510 INFO: Cleaned case_3a
2026-02-12 11:10:43,534 INFO: Cleaned case_3b
2026-02-12 11:10:43,559 INFO: Cleaned case_4
2026-02-12 11:10:43,559 INFO: CLEANUP COMPLETED
2026-02-12 11:10:43,560 INFO: ============================================================
2026-02-12 11:10:43,560 INFO: ================================================================================
2026-02-12 11:10:43,561 INFO: SUMMARY PIPELINE - START
2026-02-12 11:10:43,561 INFO: ================================================================================
2026-02-12 11:10:43,562 INFO: ================================================================================
2026-02-12 11:10:43,562 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:10:43,563 INFO: ================================================================================
2026-02-12 11:10:45,426 INFO: Loading accounts from primary_catalog.main_test_base_ts.accounts_all
2026-02-12 11:10:45,587 INFO: Reading from primary_catalog.main_test_base_ts.accounts_all - (rpt_as_of_mo < 2026-02) & (base_ts > 2026-01-05 08:00:00
2026-02-12 11:10:45,588 INFO: Preparing source data...
2026-02-12 11:10:45,784 INFO: Applied 36 column mappings
2026-02-12 11:10:46,131 INFO: Applied 7 column transformations
2026-02-12 11:10:46,249 INFO: Created 2 inferred columns
2026-02-12 11:10:47,004 INFO: Validated 7 date columns
2026-02-12 11:10:47,164 INFO: Source data preparation complete
2026-02-12 11:10:47,309 INFO: Loading summary metadata from primary_catalog.main_test_base_ts.latest_summary
2026-02-12 11:10:47,361 INFO: Loaded metadata for existing accounts
2026-02-12 11:10:47,362 INFO: Classifying accounts into Case I/II/III/IV
[Stage 25:====>                                                  (18 + 6) / 200][Stage 25:========>                                              (30 + 6) / 200][Stage 25:==========>                                            (39 + 6) / 200][Stage 25:=============>                                         (50 + 6) / 200][Stage 25:=================>                                     (62 + 6) / 200][Stage 25:====================>                                  (73 + 8) / 200][Stage 25:=======================>                               (87 + 6) / 200][Stage 25:===========================>                           (99 + 6) / 200][Stage 25:=============================>                        (111 + 7) / 200][Stage 25:=================================>                    (125 + 6) / 200][Stage 25:====================================>                 (137 + 7) / 200][Stage 25:=========================================>            (152 + 6) / 200][Stage 25:============================================>         (163 + 6) / 200][Stage 25:================================================>     (179 + 6) / 200][Stage 25:===================================================>  (192 + 7) / 200][Stage 28:=====>                                                 (20 + 6) / 200][Stage 28:==========>                                            (37 + 9) / 200][Stage 28:===============>                                       (57 + 9) / 200][Stage 28:=====================>                                 (78 + 8) / 200][Stage 28:==========================>                            (95 + 7) / 200][Stage 28:==============================>                       (113 + 6) / 200][Stage 28:==================================>                   (127 + 7) / 200][Stage 28:======================================>               (143 + 7) / 200][Stage 28:==========================================>           (159 + 6) / 200][Stage 28:===============================================>      (177 + 6) / 200][Stage 28:=====================================================>(199 + 1) / 200]                                                                                2026-02-12 11:10:57,241 INFO: ------------------------------------------------------------
2026-02-12 11:10:57,241 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:10:57,242 INFO:   CASE_III: 1 records
2026-02-12 11:10:57,243 INFO: ------------------------------------------------------------
2026-02-12 11:10:57,244 INFO: Classification | Time Elapsed: 0.23 minutes
2026-02-12 11:10:57,272 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:10:57,273 INFO: ================================================================================
2026-02-12 11:10:57,273 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:10:57,274 INFO: ================================================================================
2026-02-12 11:10:57,274 INFO: CASE III - Processing backfill records
[Stage 35:=================>                                     (65 + 7) / 200][Stage 35:========================>                              (90 + 7) / 200][Stage 35:=============================>                        (109 + 8) / 200][Stage 35:==================================>                   (126 + 7) / 200][Stage 35:=========================================>            (152 + 6) / 200][Stage 35:=============================================>        (169 + 7) / 200][Stage 35:=================================================>    (182 + 6) / 200]                                                                                2026-02-12 11:10:59,825 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:10:59,826 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:10:59,852 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:11:00,208 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:11:01,558 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 42:=====================>                                 (77 + 6) / 200][Stage 42:============================>                         (106 + 6) / 200][Stage 42:====================================>                 (134 + 7) / 200][Stage 42:==========================================>           (159 + 6) / 200][Stage 42:====================================================> (195 + 5) / 200]                                                                                [Stage 46:>                                                       (0 + 6) / 200][Stage 46:=>                                                      (6 + 6) / 200][Stage 46:===>                                                   (12 + 6) / 200][Stage 46:===>                                                   (14 + 6) / 200][Stage 46:=====>                                                 (19 + 6) / 200][Stage 46:======>                                                (22 + 6) / 200][Stage 46:=======>                                               (26 + 6) / 200][Stage 46:========>                                              (30 + 6) / 200][Stage 46:=========>                                             (33 + 6) / 200][Stage 46:==========>                                            (38 + 6) / 200][Stage 46:===========>                                           (41 + 6) / 200][Stage 46:============>                                          (44 + 6) / 200][Stage 46:=============>                                         (50 + 6) / 200][Stage 46:==============>                                        (54 + 6) / 200][Stage 46:================>                                      (59 + 6) / 200][Stage 46:================>                                      (61 + 6) / 200][Stage 46:==================>                                    (67 + 6) / 200][Stage 46:===================>                                   (72 + 6) / 200][Stage 46:=====================>                                 (77 + 6) / 200][Stage 46:======================>                                (82 + 6) / 200][Stage 46:========================>                              (88 + 6) / 200][Stage 46:========================>                              (90 + 6) / 200][Stage 46:==========================>                            (95 + 6) / 200][Stage 46:===========================>                          (101 + 6) / 200][Stage 46:============================>                         (105 + 6) / 200][Stage 46:=============================>                        (110 + 6) / 200][Stage 46:===============================>                      (116 + 6) / 200][Stage 46:================================>                     (121 + 6) / 200][Stage 46:==================================>                   (127 + 6) / 200][Stage 46:===================================>                  (133 + 7) / 200][Stage 46:====================================>                 (137 + 6) / 200]                                                                                2026-02-12 11:11:15,077 INFO: Case III Part A Generated | Time Elapsed: 0.30 minutes
2026-02-12 11:11:15,078 INFO: ------------------------------------------------------------
2026-02-12 11:11:15,078 INFO: Part B: Updating future summary rows...
2026-02-12 11:11:18,268 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 69:=================================>                    (124 + 7) / 200][Stage 69:===========================================>          (160 + 6) / 200]                                                                                [Stage 73:=========>                                             (36 + 6) / 200][Stage 73:=============>                                         (50 + 6) / 200][Stage 73:=================>                                     (65 + 6) / 200][Stage 73:====================>                                  (76 + 6) / 200][Stage 73:=========================>                             (94 + 6) / 200][Stage 73:=============================>                        (108 + 6) / 200][Stage 73:=================================>                    (123 + 7) / 200][Stage 73:======================================>               (142 + 6) / 200][Stage 73:===========================================>          (161 + 7) / 200][Stage 73:================================================>     (180 + 7) / 200][Stage 77:=============>                                         (48 + 6) / 200][Stage 77:===================>                                   (70 + 6) / 200][Stage 77:========================>                              (89 + 6) / 200][Stage 77:=============================>                        (110 + 6) / 200][Stage 77:===================================>                  (130 + 6) / 200][Stage 77:=======================================>              (147 + 6) / 200][Stage 77:=============================================>        (167 + 6) / 200][Stage 77:================================================>     (181 + 6) / 200]                                                                                2026-02-12 11:11:25,489 INFO: Case III Part B Generated | Time Elapsed: 0.17 minutes
2026-02-12 11:11:25,490 INFO: ------------------------------------------------------------
2026-02-12 11:11:25,527 INFO: ------------------------------------------------------------
2026-02-12 11:11:25,528 INFO: MERGING RECORDS:
2026-02-12 11:11:27,654 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.04 minutes
2026-02-12 11:11:27,655 INFO: ------------------------------------------------------------
2026-02-12 11:11:29,520 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.03 minutes
2026-02-12 11:11:29,521 INFO: ------------------------------------------------------------
2026-02-12 11:11:29,522 INFO: APPENDING RECORDS:
2026-02-12 11:11:29,541 INFO: No Case I/IV records to append | Time Elapsed: 0.00 minutes
2026-02-12 11:11:29,542 INFO: ------------------------------------------------------------
2026-02-12 11:11:29,580 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[ASSERT] Validating base_ts propagation and array patch...
[PASS] test_main_base_ts_propagation
2026-02-12 11:11:30,551 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:11:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:11:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:11:39 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6]                                                                                [Stage 7:>                                                          (0 + 6) / 6][Stage 7:=======================================>                   (4 + 2) / 6]                                                                                2026-02-12 11:11:52,442 INFO: Cleaned case_1
2026-02-12 11:11:52,466 INFO: Cleaned case_2
2026-02-12 11:11:52,492 INFO: Cleaned case_3a
2026-02-12 11:11:52,516 INFO: Cleaned case_3b
2026-02-12 11:11:52,544 INFO: Cleaned case_4
2026-02-12 11:11:52,545 INFO: CLEANUP COMPLETED
2026-02-12 11:11:52,546 INFO: ============================================================
2026-02-12 11:11:52,547 INFO: ================================================================================
2026-02-12 11:11:52,548 INFO: SUMMARY PIPELINE - START
2026-02-12 11:11:52,549 INFO: ================================================================================
2026-02-12 11:11:52,549 INFO: ================================================================================
2026-02-12 11:11:52,550 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:11:52,551 INFO: ================================================================================
[Stage 14:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:11:55,420 INFO: Loading accounts from primary_catalog.main_backfill.accounts_all
2026-02-12 11:11:55,621 INFO: Reading from primary_catalog.main_backfill.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:11:55,627 INFO: Preparing source data...
2026-02-12 11:11:55,869 INFO: Applied 36 column mappings
2026-02-12 11:11:56,333 INFO: Applied 7 column transformations
2026-02-12 11:11:56,503 INFO: Created 2 inferred columns
2026-02-12 11:11:57,230 INFO: Validated 7 date columns
2026-02-12 11:11:57,419 INFO: Source data preparation complete
2026-02-12 11:11:57,550 INFO: Loading summary metadata from primary_catalog.main_backfill.latest_summary
2026-02-12 11:11:57,591 INFO: Loaded metadata for existing accounts
2026-02-12 11:11:57,592 INFO: Classifying accounts into Case I/II/III/IV
[Stage 24:>                                                         (0 + 1) / 1][Stage 29:======>                                                (23 + 6) / 200][Stage 29:=========>                                             (35 + 6) / 200][Stage 29:============>                                          (44 + 6) / 200][Stage 29:==============>                                        (52 + 7) / 200][Stage 29:=================>                                     (62 + 6) / 200][Stage 29:====================>                                  (74 + 6) / 200][Stage 29:======================>                                (82 + 7) / 200][Stage 29:=========================>                             (92 + 6) / 200][Stage 29:==========================>                            (98 + 6) / 200][Stage 29:=============================>                        (109 + 6) / 200][Stage 29:=================================>                    (123 + 6) / 200][Stage 29:====================================>                 (135 + 6) / 200][Stage 29:=========================================>            (152 + 6) / 200][Stage 29:============================================>         (164 + 8) / 200][Stage 29:================================================>     (180 + 6) / 200][Stage 29:====================================================> (193 + 6) / 200][Stage 32:========>                                              (30 + 6) / 200][Stage 32:=============>                                         (48 + 7) / 200][Stage 32:==================>                                    (66 + 6) / 200][Stage 32:======================>                                (83 + 7) / 200][Stage 32:===========================>                          (103 + 6) / 200][Stage 32:=================================>                    (124 + 6) / 200][Stage 32:=======================================>              (146 + 6) / 200][Stage 32:=============================================>        (168 + 6) / 200][Stage 32:==================================================>   (188 + 8) / 200]                                                                                2026-02-12 11:12:07,258 INFO: ------------------------------------------------------------
2026-02-12 11:12:07,259 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:12:07,259 INFO:   CASE_III: 1 records
2026-02-12 11:12:07,260 INFO: ------------------------------------------------------------
2026-02-12 11:12:07,261 INFO: Classification | Time Elapsed: 0.25 minutes
2026-02-12 11:12:07,306 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:12:07,307 INFO: ================================================================================
2026-02-12 11:12:07,308 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:12:07,309 INFO: ================================================================================
2026-02-12 11:12:07,310 INFO: CASE III - Processing backfill records
[Stage 39:================>                                      (60 + 7) / 200][Stage 39:=======================>                               (84 + 7) / 200][Stage 39:=============================>                        (108 + 6) / 200]                                                                                2026-02-12 11:12:07,175 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:12:07,176 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:12:07,203 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:12:07,568 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:12:09,068 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 46:====================>                                 (76 + 10) / 200][Stage 46:=============================>                        (111 + 6) / 200][Stage 46:====================================>                 (135 + 6) / 200][Stage 46:============================================>         (166 + 6) / 200][Stage 46:===================================================>  (191 + 7) / 200]                                                                                [Stage 50:>                                                       (0 + 6) / 200][Stage 50:=>                                                      (6 + 6) / 200][Stage 50:===>                                                   (12 + 6) / 200][Stage 50:====>                                                  (18 + 6) / 200][Stage 50:======>                                                (23 + 6) / 200][Stage 50:======>                                                (25 + 7) / 200][Stage 50:========>                                              (30 + 6) / 200][Stage 50:=========>                                             (35 + 6) / 200][Stage 50:==========>                                            (39 + 6) / 200][Stage 50:============>                                          (44 + 6) / 200][Stage 50:=============>                                         (49 + 6) / 200][Stage 50:==============>                                        (53 + 6) / 200][Stage 50:================>                                      (59 + 6) / 200][Stage 50:================>                                      (61 + 6) / 200][Stage 50:==================>                                    (66 + 6) / 200][Stage 50:===================>                                   (71 + 6) / 200][Stage 50:=====================>                                 (77 + 6) / 200][Stage 50:======================>                                (81 + 6) / 200][Stage 50:=======================>                               (85 + 6) / 200][Stage 50:========================>                              (88 + 6) / 200][Stage 50:=========================>                             (91 + 6) / 200][Stage 50:==========================>                            (98 + 6) / 200][Stage 50:===========================>                          (102 + 6) / 200][Stage 50:=============================>                        (108 + 6) / 200][Stage 50:=============================>                        (111 + 6) / 200][Stage 50:===============================>                      (115 + 6) / 200][Stage 50:================================>                     (121 + 6) / 200][Stage 50:==================================>                   (127 + 6) / 200][Stage 50:===================================>                  (130 + 6) / 200][Stage 50:====================================>                 (135 + 6) / 200][Stage 50:=====================================>                (140 + 6) / 200][Stage 50:=======================================>              (146 + 6) / 200][Stage 50:========================================>             (150 + 6) / 200][Stage 50:=========================================>            (153 + 6) / 200][Stage 50:===========================================>          (160 + 6) / 200][Stage 50:=============================================>        (167 + 6) / 200][Stage 50:==============================================>       (172 + 6) / 200][Stage 50:================================================>     (178 + 6) / 200][Stage 50:=================================================>    (182 + 6) / 200][Stage 50:==================================================>   (187 + 7) / 200][Stage 50:====================================================> (193 + 6) / 200][Stage 54:==================================================>     (45 + 5) / 50]                                                                                2026-02-12 11:12:24,714 INFO: Case III Part A Generated | Time Elapsed: 0.29 minutes
2026-02-12 11:12:24,715 INFO: ------------------------------------------------------------
2026-02-12 11:12:24,715 INFO: Part B: Updating future summary rows...
[Stage 57:==========================>                            (98 + 7) / 200][Stage 57:=======================================>              (147 + 6) / 200][Stage 57:==================================================>   (188 + 8) / 200]                                                                                2026-02-12 11:12:27,876 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 73:=======>      (110 + 6) / 200][Stage 74:>               (0 + 0) / 200][Stage 73:==========>   (157 + 8) / 200][Stage 74:>               (0 + 0) / 200]                                                                                [Stage 74:================================>                     (121 + 6) / 200][Stage 74:=======================================>              (147 + 6) / 200][Stage 74:==============================================>       (171 + 6) / 200]                                                                                [Stage 80:==============>                                        (53 + 6) / 200][Stage 80:=================>                                     (65 + 6) / 200][Stage 80:=====================>                                 (77 + 6) / 200][Stage 80:=========================>                             (94 + 6) / 200][Stage 80:==============================>                       (114 + 6) / 200][Stage 80:===================================>                  (131 + 6) / 200][Stage 80:======================================>               (144 + 6) / 200][Stage 80:==========================================>           (159 + 6) / 200][Stage 80:================================================>     (179 + 6) / 200]                                                                                2026-02-12 11:12:34,061 INFO: Case III Part B Generated | Time Elapsed: 0.16 minutes
2026-02-12 11:12:34,062 INFO: ------------------------------------------------------------
2026-02-12 11:12:34,118 INFO: ------------------------------------------------------------
2026-02-12 11:12:34,119 INFO: MERGING RECORDS:
2026-02-12 11:12:33,981 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: -0.00 minutes
2026-02-12 11:12:33,982 INFO: ------------------------------------------------------------
2026-02-12 11:12:35,802 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.03 minutes
2026-02-12 11:12:35,803 INFO: ------------------------------------------------------------
2026-02-12 11:12:35,804 INFO: APPENDING RECORDS:
2026-02-12 11:12:35,818 INFO: No Case I/IV records to append | Time Elapsed: 0.00 minutes
2026-02-12 11:12:35,819 INFO: ------------------------------------------------------------
2026-02-12 11:12:35,845 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] backfill
2026-02-12 11:12:36,691 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:12:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:12:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:12:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6][Stage 3:=======================================>                   (4 + 2) / 6]                                                                                [Stage 4:=============================>                             (3 + 3) / 6]                                                                                [Stage 7:=========>                                                 (1 + 5) / 6][Stage 7:=======================================>                   (4 + 2) / 6]                                                                                [Stage 8:>                                                          (0 + 6) / 6][Stage 8:=======================================>                   (4 + 2) / 6]                                                                                2026-02-12 11:13:01,476 INFO: Cleaned case_1
2026-02-12 11:13:01,495 INFO: Cleaned case_2
2026-02-12 11:13:01,509 INFO: Cleaned case_3a
2026-02-12 11:13:01,525 INFO: Cleaned case_3b
2026-02-12 11:13:01,542 INFO: Cleaned case_4
2026-02-12 11:13:01,543 INFO: CLEANUP COMPLETED
2026-02-12 11:13:01,543 INFO: ============================================================
2026-02-12 11:13:01,544 INFO: ================================================================================
2026-02-12 11:13:01,544 INFO: SUMMARY PIPELINE - START
2026-02-12 11:13:01,545 INFO: ================================================================================
2026-02-12 11:13:01,545 INFO: ================================================================================
2026-02-12 11:13:01,546 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:13:01,546 INFO: ================================================================================
[Stage 14:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:13:02,464 INFO: Loading accounts from primary_catalog.main_all_scenarios.accounts_all
2026-02-12 11:13:02,621 INFO: Reading from primary_catalog.main_all_scenarios.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:13:02,622 INFO: Preparing source data...
2026-02-12 11:13:02,824 INFO: Applied 36 column mappings
2026-02-12 11:13:03,215 INFO: Applied 7 column transformations
2026-02-12 11:13:03,349 INFO: Created 2 inferred columns
2026-02-12 11:13:04,065 INFO: Validated 7 date columns
2026-02-12 11:13:04,233 INFO: Source data preparation complete
2026-02-12 11:13:04,394 INFO: Loading summary metadata from primary_catalog.main_all_scenarios.latest_summary
2026-02-12 11:13:04,460 INFO: Loaded metadata for existing accounts
2026-02-12 11:13:04,461 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:======>                                                (25 + 7) / 200][Stage 29:=========>                                             (36 + 6) / 200][Stage 29:=============>                                         (50 + 6) / 200][Stage 29:===============>                                       (57 + 6) / 200][Stage 29:=================>                                     (62 + 9) / 200][Stage 29:====================>                                  (75 + 6) / 200][Stage 29:=======================>                               (85 + 6) / 200][Stage 29:=========================>                             (94 + 7) / 200][Stage 29:=============================>                        (109 + 7) / 200][Stage 29:=================================>                    (124 + 7) / 200][Stage 29:=====================================>                (139 + 7) / 200][Stage 29:==========================================>           (156 + 6) / 200][Stage 29:==============================================>       (171 + 6) / 200][Stage 29:=================================================>    (185 + 7) / 200][Stage 29:===================================================>  (190 + 6) / 200][Stage 29:====================================================> (195 + 5) / 200][Stage 32:=======>                                               (28 + 6) / 200][Stage 32:============>                                          (44 + 6) / 200][Stage 32:=================>                                     (65 + 6) / 200][Stage 32:========================>                              (90 + 7) / 200][Stage 32:==============================>                       (113 + 8) / 200][Stage 32:====================================>                 (136 + 6) / 200][Stage 32:=========================================>            (155 + 7) / 200][Stage 32:===============================================>      (175 + 6) / 200][Stage 32:====================================================> (195 + 5) / 200]                                                                                2026-02-12 11:13:14,389 INFO: ------------------------------------------------------------
2026-02-12 11:13:14,390 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:13:14,392 INFO:   CASE_I: 2 records
2026-02-12 11:13:14,393 INFO:   CASE_IV: 1 records
2026-02-12 11:13:14,393 INFO:   CASE_III: 1 records
2026-02-12 11:13:14,394 INFO:   CASE_II: 1 records
2026-02-12 11:13:14,395 INFO: ------------------------------------------------------------
2026-02-12 11:13:14,395 INFO: Classification | Time Elapsed: 0.21 minutes
2026-02-12 11:13:14,449 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:13:14,450 INFO: ================================================================================
2026-02-12 11:13:14,451 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:13:14,452 INFO: ================================================================================
2026-02-12 11:13:14,452 INFO: CASE III - Processing backfill records
[Stage 39:===========>                                           (43 + 6) / 200][Stage 39:===================>                                   (70 + 6) / 200][Stage 39:=========================>                             (94 + 7) / 200][Stage 39:================================>                     (121 + 6) / 200][Stage 39:=======================================>              (148 + 6) / 200][Stage 39:===============================================>      (177 + 6) / 200]                                                                                2026-02-12 11:13:16,690 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:13:16,694 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:13:16,718 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:13:17,279 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:13:18,668 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 46:=======>                                               (29 + 6) / 200][Stage 46:============>                                          (47 + 8) / 200][Stage 46:=================>                                     (63 + 6) / 200][Stage 46:=====================>                                 (79 + 6) / 200][Stage 46:=========================>                             (91 + 7) / 200][Stage 46:==============================>                       (112 + 8) / 200][Stage 46:===================================>                  (131 + 6) / 200][Stage 46:=========================================>            (152 + 6) / 200][Stage 46:============================================>         (166 + 7) / 200][Stage 46:==================================================>   (187 + 9) / 200]                                                                                [Stage 50:>                                                       (0 + 6) / 200][Stage 50:=>                                                      (6 + 6) / 200][Stage 50:==>                                                     (8 + 6) / 200][Stage 50:===>                                                   (12 + 6) / 200][Stage 50:====>                                                  (16 + 6) / 200][Stage 50:=====>                                                 (20 + 6) / 200][Stage 50:======>                                                (25 + 6) / 200][Stage 50:========>                                              (30 + 6) / 200][Stage 50:=========>                                             (33 + 6) / 200][Stage 50:==========>                                            (37 + 7) / 200][Stage 50:===========>                                           (43 + 6) / 200][Stage 50:=============>                                         (48 + 6) / 200][Stage 50:==============>                                        (52 + 6) / 200][Stage 50:===============>                                       (56 + 6) / 200][Stage 50:================>                                      (61 + 6) / 200][Stage 50:==================>                                    (67 + 6) / 200][Stage 50:===================>                                   (71 + 6) / 200][Stage 50:====================>                                  (76 + 6) / 200][Stage 50:======================>                                (81 + 6) / 200][Stage 50:=======================>                               (86 + 6) / 200][Stage 50:=========================>                             (92 + 6) / 200][Stage 50:==========================>                            (97 + 6) / 200][Stage 50:===========================>                          (103 + 6) / 200][Stage 50:============================>                         (107 + 6) / 200][Stage 50:=============================>                        (110 + 6) / 200]                                                                                2026-02-12 11:13:32,363 INFO: Case III Part A Generated | Time Elapsed: 0.30 minutes
2026-02-12 11:13:32,363 INFO: ------------------------------------------------------------
2026-02-12 11:13:32,366 INFO: Part B: Updating future summary rows...
[Stage 57:==============================>                       (112 + 6) / 200][Stage 57:======================================>               (141 + 6) / 200][Stage 57:===========================================>          (160 + 6) / 200][Stage 57:=================================================>    (183 + 6) / 200]                                                                                2026-02-12 11:13:37,122 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 76:===================>                                   (70 + 6) / 200][Stage 76:===========================>                          (102 + 7) / 200][Stage 76:===================================>                  (131 + 6) / 200][Stage 76:======================================>               (141 + 8) / 200][Stage 76:=========================================>            (153 + 6) / 200][Stage 76:=================================================>    (182 + 8) / 200]                                                                                [Stage 80:============>                                          (45 + 6) / 200][Stage 80:===============>                                       (58 + 6) / 200][Stage 80:==================>                                    (69 + 6) / 200][Stage 80:======================>                                (83 + 6) / 200][Stage 80:========================>                              (90 + 6) / 200][Stage 80:==========================>                            (97 + 6) / 200][Stage 80:============================>                         (107 + 7) / 200][Stage 80:================================>                     (119 + 6) / 200][Stage 80:===================================>                  (130 + 6) / 200][Stage 80:======================================>               (142 + 7) / 200][Stage 80:=========================================>            (155 + 6) / 200][Stage 80:============================================>         (166 + 6) / 200][Stage 80:================================================>     (178 + 6) / 200][Stage 80:==================================================>   (188 + 6) / 200][Stage 80:=====================================================>(199 + 1) / 200][Stage 84:=======>                                               (28 + 6) / 200][Stage 84:============>                                          (45 + 6) / 200][Stage 84:================>                                      (59 + 6) / 200][Stage 84:====================>                                  (76 + 6) / 200][Stage 84:==========================>                            (95 + 7) / 200][Stage 84:=============================>                        (111 + 6) / 200][Stage 84:===============================>                      (116 + 6) / 200][Stage 84:===================================>                  (132 + 6) / 200][Stage 84:=========================================>            (155 + 6) / 200][Stage 84:==============================================>       (174 + 8) / 200][Stage 84:====================================================> (194 + 6) / 200]                                                                                2026-02-12 11:13:46,401 INFO: Case III Part B Generated | Time Elapsed: 0.23 minutes
2026-02-12 11:13:46,402 INFO: ------------------------------------------------------------
2026-02-12 11:13:46,425 INFO: 
>>> PROCESSING NEW ACCOUNTS (2 records)
2026-02-12 11:13:46,426 INFO: ================================================================================
2026-02-12 11:13:46,426 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:13:46,426 INFO: ================================================================================
2026-02-12 11:13:46,427 INFO: Processing new accounts
2026-02-12 11:13:47,052 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=2, scale=1.0)
[Stage 87:================>                                      (59 + 6) / 200][Stage 87:======================>                                (80 + 6) / 200][Stage 87:==========================>                            (98 + 7) / 200][Stage 87:================================>                     (121 + 7) / 200][Stage 87:=====================================>                (139 + 6) / 200][Stage 87:===========================================>          (161 + 6) / 200][Stage 87:================================================>     (179 + 6) / 200][Stage 87:=====================================================>(199 + 1) / 200][Stage 90:================>                                      (60 + 6) / 200][Stage 90:===========================>                          (101 + 6) / 200][Stage 90:===================================>                  (133 + 7) / 200][Stage 90:=============================================>        (167 + 6) / 200][Stage 94:==================================================>     (45 + 5) / 50]                                                                                2026-02-12 11:13:51,830 INFO: Case I Generated | Time Elapsed: 0.09 minutes
2026-02-12 11:13:51,831 INFO: ------------------------------------------------------------
2026-02-12 11:13:51,853 INFO: 
>>> PROCESSING BULK HISTORICAL (1 records)
2026-02-12 11:13:51,854 INFO: ================================================================================
2026-02-12 11:13:51,854 INFO: STEP 2d: Process Case IV (Bulk Historical Load)
2026-02-12 11:13:51,855 INFO: ================================================================================
2026-02-12 11:13:51,856 INFO: Processing bulk historical records
2026-02-12 11:13:51,903 INFO: Building complete history using window functions
[Stage 97:========================>                              (90 + 6) / 200][Stage 97:==================================>                   (127 + 6) / 200][Stage 97:============================================>         (164 + 6) / 200]                                                                                2026-02-12 11:13:55,887 INFO: Write partitions [case_4_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 104:(43 + 6) / 200][Stage 105:>(0 + 0) / 200][Stage 106:>(0 + 0) / 200]                                                                                [Stage 109:===================>                                 (147 + 6) / 400][Stage 109:======================>                              (172 + 6) / 400][Stage 109:=========================>                           (193 + 6) / 400][Stage 109:============================>                        (217 + 6) / 400][Stage 109:=================================>                   (255 + 6) / 400][Stage 109:======================================>              (288 + 6) / 400][Stage 109:=========================================>           (315 + 6) / 400][Stage 109:============================================>        (334 + 7) / 400][Stage 109:================================================>    (366 + 6) / 400]                                                                                [Stage 116:====>                                                 (33 + 6) / 400][Stage 116:======>                                               (46 + 6) / 400][Stage 116:========>                                             (66 + 6) / 400][Stage 116:===========>                                          (83 + 6) / 400][Stage 116:=============>                                        (98 + 6) / 400][Stage 116:==============>                                      (108 + 6) / 400][Stage 116:================>                                    (125 + 6) / 400][Stage 116:===================>                                 (146 + 6) / 400][Stage 116:=====================>                               (166 + 6) / 400][Stage 116:========================>                            (182 + 6) / 400][Stage 116:==========================>                          (200 + 6) / 400][Stage 116:============================>                        (215 + 6) / 400][Stage 116:==============================>                      (230 + 6) / 400][Stage 116:================================>                    (244 + 6) / 400][Stage 116:==================================>                  (257 + 6) / 400][Stage 116:===================================>                 (270 + 7) / 400][Stage 116:======================================>              (288 + 6) / 400][Stage 116:========================================>            (309 + 6) / 400][Stage 116:===========================================>         (326 + 6) / 400][Stage 116:=============================================>       (342 + 6) / 400][Stage 116:===============================================>     (355 + 6) / 400][Stage 116:=================================================>   (373 + 6) / 400][Stage 116:====================================================>(394 + 6) / 400]                                                                                2026-02-12 11:14:09,488 INFO: Case IV Generated | Time Elapsed: 0.29 minutes
2026-02-12 11:14:09,489 INFO: ------------------------------------------------------------
2026-02-12 11:14:09,616 INFO: ------------------------------------------------------------
2026-02-12 11:14:09,616 INFO: MERGING RECORDS:
2026-02-12 11:14:11,296 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.03 minutes
2026-02-12 11:14:11,296 INFO: ------------------------------------------------------------
2026-02-12 11:14:13,220 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.03 minutes
2026-02-12 11:14:13,225 INFO: ------------------------------------------------------------
2026-02-12 11:14:13,226 INFO: APPENDING RECORDS:
2026-02-12 11:14:13,291 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=3, scale=1.0)
26/02/12 11:14:14 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:335)
	org.apache.spark.sql.execution.CacheManager.$anonfun$cacheQuery$2(CacheManager.scala:127)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:122)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)
	org.apache.spark.sql.Dataset.persist(Dataset.scala:3797)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
2026-02-12 11:14:14,283 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:14:14,284 INFO: ------------------------------------------------------------
2026-02-12 11:14:14,285 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:14:15,315 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:14:15,317 INFO: ------------------------------------------------------------
2026-02-12 11:14:15,342 INFO: 
>>> PROCESSING FORWARD ENTRIES (1 records)
2026-02-12 11:14:15,345 INFO: ================================================================================
2026-02-12 11:14:15,346 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:14:15,346 INFO: ================================================================================
2026-02-12 11:14:15,347 INFO: Processing forward entries
2026-02-12 11:14:15,383 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:14:15,842 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 153:(57 + 6) / 200][Stage 154:>(0 + 0) / 200][Stage 155:>(0 + 0) / 200][Stage 153:(73 + 7) / 200][Stage 154:>(0 + 0) / 200][Stage 155:>(0 + 0) / 200][Stage 153:(95 + 6) / 200][Stage 154:>(0 + 0) / 200][Stage 155:>(0 + 0) / 200][Stage 153:(120 + 6) / 200][Stage 154:>(0 + 0) / 200][Stage 155:>(0 + 0) / 200][Stage 153:(148 + 7) / 200][Stage 154:>(0 + 0) / 200][Stage 155:>(0 + 0) / 200][Stage 153:(177 + 6) / 200][Stage 154:>(0 + 0) / 200][Stage 155:>(0 + 0) / 200][Stage 154:>              (6 + 9) / 200][Stage 155:>              (0 + 0) / 200][Stage 154:=>            (28 + 6) / 200][Stage 155:>              (0 + 0) / 200][Stage 154:===>          (48 + 6) / 200][Stage 155:>              (0 + 0) / 200][Stage 154:======>       (96 + 6) / 200][Stage 155:>              (0 + 0) / 200][Stage 154:===========> (170 + 6) / 200][Stage 155:>              (0 + 0) / 200][Stage 155:========>                                             (32 + 6) / 200][Stage 155:=================>                                    (64 + 6) / 200][Stage 155:============================>                        (108 + 6) / 200][Stage 155:==================================>                  (130 + 6) / 200][Stage 155:======================================>              (147 + 6) / 200][Stage 155:============>(195 + 5) / 200][Stage 156:>              (0 + 1) / 200][Stage 156:=====>                                                (21 + 6) / 200][Stage 156:==============>                                       (53 + 7) / 200][Stage 156:====================>                                 (77 + 7) / 200][Stage 156:===========================>                         (102 + 6) / 200][Stage 156:=======>     (120 + 6) / 200][Stage 157:>              (0 + 0) / 200][Stage 156:========>    (138 + 8) / 200][Stage 157:>              (0 + 0) / 200][Stage 156:===========> (180 + 6) / 200][Stage 157:>              (0 + 0) / 200]                                                                                [Stage 157:===================>                                  (71 + 8) / 200][Stage 157:=============================>                       (110 + 7) / 200][Stage 157:=====================================>               (140 + 6) / 200][Stage 157:=============================================>       (170 + 7) / 200]                                                                                [Stage 161:====>                                                 (17 + 6) / 200][Stage 161:=====>                                                (21 + 6) / 200][Stage 161:=======>                                              (29 + 6) / 200][Stage 161:==========>                                           (38 + 6) / 200][Stage 161:===========>                                          (42 + 6) / 200]                                                                                2026-02-12 11:14:31,549 INFO: Case II Generated | Time Elapsed: 0.27 minutes
2026-02-12 11:14:31,550 INFO: ------------------------------------------------------------
2026-02-12 11:14:31,551 INFO: ------------------------------------------------------------
2026-02-12 11:14:31,552 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:14:31,590 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:14:26,658 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: -0.08 minutes
2026-02-12 11:14:26,658 INFO: ------------------------------------------------------------
2026-02-12 11:14:28,694 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.03 minutes
2026-02-12 11:14:28,695 INFO: ------------------------------------------------------------
2026-02-12 11:14:28,695 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] all_scenarios
2026-02-12 11:14:30,208 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:14:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:14:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:14:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6][Stage 3:=================================================>         (5 + 1) / 6]                                                                                2026-02-12 11:14:53,935 INFO: Cleaned case_1
2026-02-12 11:14:53,956 INFO: Cleaned case_2
2026-02-12 11:14:53,983 INFO: Cleaned case_3a
2026-02-12 11:14:54,006 INFO: Cleaned case_3b
2026-02-12 11:14:54,029 INFO: Cleaned case_4
2026-02-12 11:14:54,030 INFO: CLEANUP COMPLETED
2026-02-12 11:14:54,030 INFO: ============================================================
2026-02-12 11:14:54,031 INFO: ================================================================================
2026-02-12 11:14:54,031 INFO: SUMMARY PIPELINE - START
2026-02-12 11:14:54,032 INFO: ================================================================================
2026-02-12 11:14:54,032 INFO: ================================================================================
2026-02-12 11:14:54,033 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:14:54,033 INFO: ================================================================================
[Stage 14:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:14:57,886 INFO: Loading accounts from primary_catalog.main_all_scenarios.accounts_all
2026-02-12 11:14:58,163 INFO: Reading from primary_catalog.main_all_scenarios.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:14:58,164 INFO: Preparing source data...
2026-02-12 11:14:58,406 INFO: Applied 36 column mappings
2026-02-12 11:14:59,296 INFO: Applied 7 column transformations
2026-02-12 11:14:59,494 INFO: Created 2 inferred columns
2026-02-12 11:15:00,222 INFO: Validated 7 date columns
2026-02-12 11:15:00,374 INFO: Source data preparation complete
2026-02-12 11:15:00,545 INFO: Loading summary metadata from primary_catalog.main_all_scenarios.latest_summary
2026-02-12 11:15:00,616 INFO: Loaded metadata for existing accounts
2026-02-12 11:15:00,617 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:====>                                                  (16 + 8) / 200][Stage 29:=======>                                               (28 + 6) / 200][Stage 29:===========>                                           (40 + 6) / 200][Stage 29:=============>                                         (49 + 6) / 200][Stage 29:===============>                                       (55 + 8) / 200][Stage 29:==================>                                    (67 + 6) / 200][Stage 29:======================>                                (82 + 6) / 200][Stage 29:==========================>                            (96 + 7) / 200][Stage 29:=============================>                        (111 + 6) / 200][Stage 29:=================================>                    (124 + 6) / 200][Stage 29:====================================>                 (136 + 7) / 200][Stage 29:========================================>             (150 + 6) / 200][Stage 29:============================================>         (165 + 6) / 200][Stage 29:================================================>     (181 + 6) / 200][Stage 29:=====================================================>(198 + 2) / 200][Stage 32:=========>                                             (34 + 6) / 200][Stage 32:================>                                      (61 + 6) / 200][Stage 32:======================>                                (82 + 6) / 200][Stage 32:============================>                         (104 + 8) / 200][Stage 32:================================>                     (122 + 6) / 200][Stage 32:====================================>                 (137 + 6) / 200][Stage 32:=========================================>            (153 + 6) / 200][Stage 32:=============================================>        (168 + 6) / 200][Stage 32:================================================>     (178 + 6) / 200][Stage 32:====================================================> (196 + 4) / 200]                                                                                2026-02-12 11:15:10,493 INFO: ------------------------------------------------------------
2026-02-12 11:15:10,494 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:15:10,494 INFO:   CASE_I: 2 records
2026-02-12 11:15:10,495 INFO:   CASE_IV: 1 records
2026-02-12 11:15:10,495 INFO:   CASE_III: 1 records
2026-02-12 11:15:10,496 INFO:   CASE_II: 1 records
2026-02-12 11:15:10,497 INFO: ------------------------------------------------------------
2026-02-12 11:15:10,497 INFO: Classification | Time Elapsed: 0.27 minutes
2026-02-12 11:15:10,552 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:15:10,552 INFO: ================================================================================
2026-02-12 11:15:10,553 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:15:10,554 INFO: ================================================================================
2026-02-12 11:15:10,554 INFO: CASE III - Processing backfill records
[Stage 39:===========>                                           (43 + 7) / 200][Stage 39:==================>                                    (68 + 7) / 200][Stage 39:========================>                              (90 + 8) / 200][Stage 39:================================>                     (120 + 6) / 200][Stage 39:====================================>                 (136 + 6) / 200][Stage 39:===========================================>          (161 + 6) / 200][Stage 39:=================================================>    (182 + 6) / 200]                                                                                2026-02-12 11:15:13,094 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:15:13,095 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:15:13,127 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:15:13,503 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:15:14,995 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 46:===========>                                           (42 + 6) / 200][Stage 46:=================>                                     (62 + 6) / 200][Stage 46:=====================>                                 (77 + 6) / 200][Stage 46:==========================>                            (96 + 9) / 200][Stage 46:==============================>                       (112 + 8) / 200][Stage 46:===================================>                  (132 + 6) / 200][Stage 46:=========================================>            (155 + 6) / 200]                                                                                [Stage 50:===================>                                   (72 + 6) / 200][Stage 50:=====================>                                 (77 + 6) / 200][Stage 50:======================>                                (83 + 6) / 200][Stage 50:========================>                              (88 + 7) / 200][Stage 50:=========================>                             (92 + 6) / 200][Stage 50:==========================>                            (98 + 6) / 200][Stage 50:============================>                         (104 + 6) / 200][Stage 50:=============================>                        (108 + 6) / 200][Stage 50:==============================>                       (114 + 6) / 200][Stage 50:================================>                     (120 + 6) / 200][Stage 50:==================================>                   (126 + 6) / 200][Stage 50:===================================>                  (130 + 6) / 200][Stage 50:====================================>                 (136 + 6) / 200][Stage 50:======================================>               (141 + 7) / 200][Stage 50:=======================================>              (147 + 6) / 200][Stage 50:=========================================>            (153 + 6) / 200][Stage 50:==========================================>           (159 + 6) / 200][Stage 50:============================================>         (164 + 6) / 200][Stage 50:=============================================>        (170 + 6) / 200][Stage 50:===============================================>      (176 + 6) / 200][Stage 50:================================================>     (180 + 6) / 200][Stage 50:==================================================>   (186 + 6) / 200][Stage 50:===================================================>  (190 + 6) / 200][Stage 50:====================================================> (196 + 4) / 200]                                                                                2026-02-12 11:15:28,474 INFO: Case III Part A Generated | Time Elapsed: 0.30 minutes
2026-02-12 11:15:28,475 INFO: ------------------------------------------------------------
2026-02-12 11:15:28,475 INFO: Part B: Updating future summary rows...
[Stage 57:==============>                                        (53 + 7) / 200][Stage 57:====================>                                  (75 + 6) / 200][Stage 57:=========================>                             (93 + 7) / 200][Stage 57:===============================>                      (115 + 8) / 200][Stage 57:======================================>               (142 + 6) / 200][Stage 57:================================================>     (181 + 6) / 200]                                                                                [Stage 70:=========================================>             (76 + 7) / 100]                                                                                2026-02-12 11:15:33,224 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 76:=====================>                                 (78 + 6) / 200][Stage 76:==============================>                       (113 + 6) / 200][Stage 76:=====================================>                (140 + 7) / 200][Stage 76:=============================================>        (169 + 8) / 200]                                                                                [Stage 80:============>                                          (47 + 6) / 200][Stage 80:=================>                                     (62 + 6) / 200][Stage 80:======================>                                (81 + 6) / 200][Stage 80:==========================>                            (97 + 8) / 200][Stage 80:===============================>                      (115 + 7) / 200][Stage 80:====================================>                 (135 + 6) / 200][Stage 80:========================================>             (149 + 6) / 200][Stage 80:============================================>         (163 + 6) / 200][Stage 80:================================================>     (178 + 6) / 200][Stage 80:===================================================>  (191 + 6) / 200][Stage 84:==============>                                        (53 + 6) / 200][Stage 84:==================>                                    (69 + 6) / 200][Stage 84:========================>                              (88 + 6) / 200][Stage 84:=============================>                        (110 + 6) / 200][Stage 84:==================================>                   (128 + 7) / 200][Stage 84:======================================>               (142 + 6) / 200][Stage 84:=========================================>            (152 + 6) / 200][Stage 84:===========================================>          (161 + 6) / 200][Stage 84:================================================>     (180 + 6) / 200][Stage 84:====================================================> (195 + 5) / 200]                                                                                2026-02-12 11:15:40,868 INFO: Case III Part B Generated | Time Elapsed: 0.21 minutes
2026-02-12 11:15:40,869 INFO: ------------------------------------------------------------
2026-02-12 11:15:40,896 INFO: 
>>> PROCESSING NEW ACCOUNTS (2 records)
2026-02-12 11:15:40,897 INFO: ================================================================================
2026-02-12 11:15:40,898 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:15:40,898 INFO: ================================================================================
2026-02-12 11:15:40,899 INFO: Processing new accounts
2026-02-12 11:15:41,599 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=2, scale=1.0)
[Stage 87:=========>                                             (33 + 6) / 200][Stage 87:============>                                          (47 + 7) / 200][Stage 87:==================>                                    (68 + 6) / 200][Stage 87:=======================>                               (86 + 6) / 200][Stage 87:===========================>                          (102 + 6) / 200][Stage 87:=================================>                    (125 + 6) / 200][Stage 87:=======================================>              (147 + 7) / 200][Stage 87:==============================================>       (171 + 6) / 200][Stage 87:====================================================> (195 + 5) / 200][Stage 90:===================================>                  (131 + 6) / 200][Stage 90:============================================>         (163 + 6) / 200][Stage 94:========================================================(50 + 0) / 50]                                                                                2026-02-12 11:15:46,055 INFO: Case I Generated | Time Elapsed: 0.09 minutes
2026-02-12 11:15:46,056 INFO: ------------------------------------------------------------
2026-02-12 11:15:46,085 INFO: 
>>> PROCESSING BULK HISTORICAL (1 records)
2026-02-12 11:15:46,086 INFO: ================================================================================
2026-02-12 11:15:46,087 INFO: STEP 2d: Process Case IV (Bulk Historical Load)
2026-02-12 11:15:46,088 INFO: ================================================================================
2026-02-12 11:15:46,088 INFO: Processing bulk historical records
2026-02-12 11:15:46,141 INFO: Building complete history using window functions
[Stage 97:========================>                              (89 + 6) / 200]                                                                                2026-02-12 11:15:47,263 INFO: Write partitions [case_4_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 104:(152 + 7) / 200][Stage 105:>(0 + 0) / 200][Stage 106:>(0 + 0) / 200][Stage 104:(192 + 6) / 200][Stage 105:>(0 + 0) / 200][Stage 106:>(0 + 0) / 200]                                                                                [Stage 105:====>         (69 + 6) / 200][Stage 106:>              (0 + 0) / 200][Stage 105:======>       (87 + 9) / 200][Stage 106:>              (0 + 0) / 200][Stage 105:=======>     (117 + 6) / 200][Stage 106:>              (0 + 0) / 200][Stage 105:=========>   (150 + 6) / 200][Stage 106:>              (0 + 0) / 200][Stage 105:============>(188 + 6) / 200][Stage 106:>              (0 + 0) / 200]                                                                                [Stage 106:=============>                                        (50 + 6) / 200][Stage 106:================>                                     (62 + 7) / 200][Stage 106:======================>                               (83 + 6) / 200][Stage 106:==========================>                           (98 + 6) / 200][Stage 106:======================================>              (144 + 6) / 200][Stage 106:==============================================>      (175 + 6) / 200]                                                                                [Stage 107:=============>                                       (101 + 7) / 400][Stage 107:================>                                    (123 + 7) / 400][Stage 107:====================>                                (152 + 6) / 400][Stage 107:========================>                            (184 + 7) / 400][Stage 107:===========================>                         (208 + 6) / 400][Stage 107:===============================>                     (240 + 7) / 400][Stage 107:=====================================>               (283 + 6) / 400][Stage 107:==========================================>          (322 + 6) / 400][Stage 107:================================================>    (365 + 6) / 400]                                                                                [Stage 114:=====>                                                (40 + 6) / 400][Stage 114:=======>                                              (53 + 7) / 400][Stage 114:=========>                                            (70 + 6) / 400][Stage 114:===========>                                          (87 + 6) / 400][Stage 114:=============>                                       (103 + 6) / 400][Stage 114:================>                                    (121 + 6) / 400][Stage 114:==================>                                  (136 + 6) / 400][Stage 114:====================>                                (154 + 7) / 400][Stage 114:======================>                              (171 + 7) / 400][Stage 114:========================>                            (188 + 7) / 400][Stage 114:==========================>                          (199 + 6) / 400][Stage 114:===========================>                         (207 + 6) / 400][Stage 114:=============================>                       (220 + 7) / 400][Stage 114:==============================>                      (233 + 7) / 400][Stage 114:================================>                    (249 + 6) / 400][Stage 114:===================================>                 (269 + 6) / 400][Stage 114:=====================================>               (285 + 8) / 400][Stage 114:========================================>            (306 + 6) / 400][Stage 114:==========================================>          (324 + 6) / 400][Stage 114:=============================================>       (342 + 6) / 400][Stage 114:===============================================>     (359 + 6) / 400][Stage 114:=================================================>   (374 + 6) / 400][Stage 114:===================================================> (390 + 6) / 400][Stage 118:===================================>                   (32 + 7) / 50]                                                                                2026-02-12 11:16:02,951 INFO: Case IV Generated | Time Elapsed: 0.28 minutes
2026-02-12 11:16:02,951 INFO: ------------------------------------------------------------
2026-02-12 11:16:02,964 INFO: ------------------------------------------------------------
2026-02-12 11:16:02,968 INFO: MERGING RECORDS:
2026-02-12 11:16:04,749 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.03 minutes
2026-02-12 11:16:04,750 INFO: ------------------------------------------------------------
2026-02-12 11:16:06,541 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.03 minutes
2026-02-12 11:16:06,542 INFO: ------------------------------------------------------------
2026-02-12 11:16:06,542 INFO: APPENDING RECORDS:
2026-02-12 11:16:06,603 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=3, scale=1.0)
2026-02-12 11:16:07,535 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:16:07,536 INFO: ------------------------------------------------------------
2026-02-12 11:16:07,537 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:16:08,623 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:16:08,624 INFO: ------------------------------------------------------------
2026-02-12 11:16:08,661 INFO: 
>>> PROCESSING FORWARD ENTRIES (1 records)
2026-02-12 11:16:08,662 INFO: ================================================================================
2026-02-12 11:16:08,663 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:16:08,664 INFO: ================================================================================
2026-02-12 11:16:08,664 INFO: Processing forward entries
2026-02-12 11:16:08,708 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:16:09,296 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 151:(36 + 6) / 200][Stage 152:>(0 + 0) / 200][Stage 153:>(0 + 0) / 200][Stage 151:(59 + 6) / 200][Stage 152:>(0 + 0) / 200][Stage 153:>(0 + 0) / 200][Stage 151:(82 + 7) / 200][Stage 152:>(0 + 0) / 200][Stage 153:>(0 + 0) / 200][Stage 151:(109 + 6) / 200][Stage 152:>(0 + 0) / 200][Stage 153:>(0 + 0) / 200][Stage 151:(130 + 6) / 200][Stage 152:>(0 + 0) / 200][Stage 153:>(0 + 0) / 200][Stage 151:(153 + 7) / 200][Stage 152:>(0 + 0) / 200][Stage 153:>(0 + 0) / 200][Stage 151:(181 + 8) / 200][Stage 152:>(0 + 0) / 200][Stage 153:>(0 + 0) / 200][Stage 152:>              (7 + 6) / 200][Stage 153:>              (0 + 0) / 200][Stage 152:===>          (45 + 7) / 200][Stage 153:>              (0 + 0) / 200][Stage 152:=====>        (80 + 7) / 200][Stage 153:>              (0 + 0) / 200][Stage 152:=======>     (123 + 6) / 200][Stage 153:>              (0 + 0) / 200][Stage 152:===========> (174 + 8) / 200][Stage 153:>              (0 + 0) / 200][Stage 153:====>                                                (16 + 10) / 200][Stage 153:================>                                     (61 + 6) / 200][Stage 153:============================>                        (107 + 6) / 200][Stage 153:===========================================>         (163 + 7) / 200][Stage 154:======>                                               (23 + 7) / 200]26/02/12 11:16:12 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:335)
	org.apache.spark.sql.execution.CacheManager.$anonfun$cacheQuery$2(CacheManager.scala:127)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:122)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)
	org.apache.spark.sql.Dataset.persist(Dataset.scala:3797)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
                                                                                [Stage 159:=======================>                              (87 + 6) / 200][Stage 159:==========================>                           (97 + 6) / 200][Stage 159:===========================>                         (104 + 6) / 200][Stage 159:==============================>                      (116 + 6) / 200][Stage 159:=================================>                   (125 + 6) / 200][Stage 159:===================================>                 (133 + 6) / 200][Stage 159:======================================>              (144 + 6) / 200][Stage 159:=========================================>           (155 + 6) / 200][Stage 159:=========================================>           (158 + 6) / 200][Stage 159:============================================>        (169 + 6) / 200][Stage 159:===============================================>     (181 + 6) / 200][Stage 159:==================================================>  (191 + 6) / 200]                                                                                2026-02-12 11:16:20,556 INFO: Case II Generated | Time Elapsed: 0.20 minutes
2026-02-12 11:16:20,558 INFO: ------------------------------------------------------------
2026-02-12 11:16:20,559 INFO: ------------------------------------------------------------
2026-02-12 11:16:20,560 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:16:20,592 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:16:21,182 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:16:21,183 INFO: ------------------------------------------------------------
2026-02-12 11:16:23,157 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.03 minutes
2026-02-12 11:16:23,158 INFO: ------------------------------------------------------------
2026-02-12 11:16:23,162 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] all_scenarios
2026-02-12 11:16:24,616 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:16:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:16:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:16:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 6:>                                                          (0 + 1) / 1]                                                                                2026-02-12 11:16:46,818 INFO: Cleaned case_1
2026-02-12 11:16:46,842 INFO: Cleaned case_2
2026-02-12 11:16:46,878 INFO: Cleaned case_3a
2026-02-12 11:16:46,905 INFO: Cleaned case_3b
2026-02-12 11:16:46,937 INFO: Cleaned case_4
2026-02-12 11:16:46,938 INFO: CLEANUP COMPLETED
2026-02-12 11:16:46,939 INFO: ============================================================
2026-02-12 11:16:46,940 INFO: ================================================================================
2026-02-12 11:16:46,940 INFO: SUMMARY PIPELINE - START
2026-02-12 11:16:46,941 INFO: ================================================================================
2026-02-12 11:16:46,941 INFO: ================================================================================
2026-02-12 11:16:46,942 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:16:46,942 INFO: ================================================================================
2026-02-12 11:16:49,953 INFO: Loading accounts from primary_catalog.main_bulk.accounts_all
2026-02-12 11:16:50,103 INFO: Reading from primary_catalog.main_bulk.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2020-01-02 00:00:00
2026-02-12 11:16:50,105 INFO: Preparing source data...
2026-02-12 11:16:50,308 INFO: Applied 36 column mappings
2026-02-12 11:16:50,694 INFO: Applied 7 column transformations
2026-02-12 11:16:50,830 INFO: Created 2 inferred columns
2026-02-12 11:16:51,737 INFO: Validated 7 date columns
2026-02-12 11:16:51,912 INFO: Source data preparation complete
2026-02-12 11:16:52,043 INFO: Loading summary metadata from primary_catalog.main_bulk.latest_summary
2026-02-12 11:16:52,101 INFO: Loaded metadata for existing accounts
2026-02-12 11:16:52,101 INFO: Classifying accounts into Case I/II/III/IV
[Stage 20:>                                                        (0 + 6) / 12][Stage 20:====>                                                    (1 + 6) / 12][Stage 20:============================>                            (6 + 6) / 12][Stage 20:======================================>                  (8 + 4) / 12][Stage 25:=======>                                               (26 + 6) / 200][Stage 25:===========>                                           (40 + 6) / 200][Stage 25:==============>                                        (51 + 6) / 200][Stage 25:=================>                                     (64 + 6) / 200][Stage 25:======================>                                (81 + 6) / 200][Stage 25:========================>                              (90 + 6) / 200][Stage 25:===========================>                          (100 + 7) / 200][Stage 25:==============================>                       (112 + 6) / 200][Stage 25:=================================>                    (123 + 7) / 200][Stage 25:=====================================>                (138 + 6) / 200][Stage 25:=========================================>            (153 + 6) / 200][Stage 25:===========================================>          (162 + 6) / 200][Stage 25:==============================================>       (171 + 6) / 200][Stage 25:=================================================>    (182 + 6) / 200][Stage 25:==================================================>   (188 + 7) / 200][Stage 28:==============>                                        (53 + 6) / 200][Stage 28:====================>                                  (76 + 6) / 200][Stage 28:=========================>                             (92 + 6) / 200][Stage 28:============================>                         (105 + 8) / 200][Stage 28:================================>                     (121 + 6) / 200][Stage 28:=======================================>              (145 + 6) / 200][Stage 28:===========================================>          (161 + 7) / 200][Stage 28:===============================================>      (177 + 6) / 200][Stage 28:=====================================================>(198 + 2) / 200]                                                                                2026-02-12 11:17:02,932 INFO: ------------------------------------------------------------
2026-02-12 11:17:02,933 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:17:02,934 INFO:   CASE_I: 1 records
2026-02-12 11:17:02,935 INFO:   CASE_IV: 47 records
2026-02-12 11:17:02,936 INFO: ------------------------------------------------------------
2026-02-12 11:17:02,936 INFO: Classification | Time Elapsed: 0.27 minutes
2026-02-12 11:17:03,036 INFO: 
>>> PROCESSING NEW ACCOUNTS (1 records)
2026-02-12 11:17:03,037 INFO: ================================================================================
2026-02-12 11:17:03,037 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:17:03,038 INFO: ================================================================================
2026-02-12 11:17:03,038 INFO: Processing new accounts
2026-02-12 11:17:03,903 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 35:=====>                                                 (21 + 6) / 200][Stage 35:===========>                                           (41 + 7) / 200][Stage 35:===============>                                       (57 + 6) / 200][Stage 35:===================>                                   (72 + 7) / 200][Stage 35:=========================>                             (91 + 6) / 200][Stage 35:============================>                         (105 + 6) / 200][Stage 35:===============================>                      (118 + 6) / 200][Stage 35:===================================>                  (132 + 6) / 200][Stage 35:=======================================>              (147 + 6) / 200][Stage 35:============================================>         (165 + 6) / 200][Stage 35:=================================================>    (184 + 7) / 200][Stage 38:==============================>                       (113 + 6) / 200][Stage 38:========================================>             (150 + 6) / 200][Stage 38:================================================>     (180 + 6) / 200][Stage 42:==========================>                             (24 + 8) / 50]                                                                                2026-02-12 11:17:06,808 INFO: Case I Generated | Time Elapsed: 0.06 minutes
2026-02-12 11:17:06,809 INFO: ------------------------------------------------------------
2026-02-12 11:17:06,828 INFO: 
>>> PROCESSING BULK HISTORICAL (47 records)
2026-02-12 11:17:06,829 INFO: ================================================================================
2026-02-12 11:17:06,829 INFO: STEP 2d: Process Case IV (Bulk Historical Load)
2026-02-12 11:17:06,830 INFO: ================================================================================
2026-02-12 11:17:06,830 INFO: Processing bulk historical records
2026-02-12 11:17:06,905 INFO: Building complete history using window functions
2026-02-12 11:17:12,087 INFO: Write partitions [case_4_temp]: 50 (base=200, expected_rows=47, scale=1.0)
[Stage 52:>(88 + 6) / 200][Stage 53:> (0 + 0) / 200][Stage 54:> (0 + 0) / 200][Stage 52:(112 + 8) / 200][Stage 53:> (0 + 0) / 200][Stage 54:> (0 + 0) / 200][Stage 52:(140 + 7) / 200][Stage 53:> (0 + 0) / 200][Stage 54:> (0 + 0) / 200][Stage 52:(180 + 6) / 200][Stage 53:> (0 + 0) / 200][Stage 54:> (0 + 0) / 200]                                                                                [Stage 53:======>        (84 + 6) / 200][Stage 54:>               (0 + 0) / 200][Stage 53:=======>      (105 + 6) / 200][Stage 54:>               (0 + 0) / 200][Stage 53:=========>    (131 + 6) / 200][Stage 54:>               (0 + 0) / 200][Stage 53:=========>    (137 + 7) / 200][Stage 54:>               (0 + 0) / 200][Stage 53:=============>(196 + 4) / 200][Stage 54:>               (0 + 3) / 200]                                                                                [Stage 54:==>            (38 + 6) / 200][Stage 55:>               (0 + 0) / 400][Stage 54:===>           (49 + 6) / 200][Stage 55:>               (0 + 0) / 400][Stage 54:=====>         (69 + 6) / 200][Stage 55:>               (0 + 0) / 400][Stage 54:======>        (89 + 6) / 200][Stage 55:>               (0 + 0) / 400][Stage 54:=======>      (105 + 7) / 200][Stage 55:>               (0 + 0) / 400][Stage 54:========>     (116 + 6) / 200][Stage 55:>               (0 + 0) / 400][Stage 54:=========>    (132 + 6) / 200][Stage 55:>               (0 + 0) / 400][Stage 54:===========>  (158 + 6) / 200][Stage 55:>               (0 + 0) / 400][Stage 54:============> (185 + 6) / 200][Stage 55:>               (0 + 0) / 400]                                                                                [Stage 55:==========>                                            (76 + 8) / 400][Stage 55:=============>                                        (102 + 6) / 400][Stage 55:================>                                     (124 + 6) / 400][Stage 55:===================>                                  (148 + 6) / 400][Stage 55:=======================>                              (175 + 7) / 400][Stage 55:===========================>                          (200 + 6) / 400][Stage 55:============================>                         (211 + 7) / 400][Stage 55:==============================>                       (228 + 6) / 400][Stage 55:=================================>                    (247 + 6) / 400][Stage 55:====================================>                 (272 + 6) / 400][Stage 55:========================================>             (299 + 8) / 400][Stage 55:===========================================>          (323 + 6) / 400][Stage 55:===============================================>      (349 + 7) / 400][Stage 55:=================================================>    (366 + 6) / 400][Stage 55:===================================================>  (384 + 6) / 400]                                                                                [Stage 62:===>                                                   (28 + 6) / 400][Stage 62:=====>                                                 (42 + 6) / 400][Stage 62:=======>                                               (57 + 6) / 400][Stage 62:=========>                                             (69 + 6) / 400][Stage 62:===========>                                           (85 + 6) / 400][Stage 62:=============>                                        (100 + 7) / 400][Stage 62:===============>                                      (112 + 6) / 400][Stage 62:================>                                     (124 + 6) / 400][Stage 62:==================>                                   (136 + 6) / 400][Stage 62:===================>                                  (148 + 6) / 400][Stage 62:======================>                               (165 + 6) / 400][Stage 62:========================>                             (181 + 6) / 400][Stage 62:==========================>                           (196 + 6) / 400][Stage 62:============================>                         (208 + 6) / 400][Stage 62:==============================>                       (223 + 6) / 400][Stage 62:===============================>                      (236 + 6) / 400][Stage 62:=================================>                    (247 + 7) / 400][Stage 62:===================================>                  (263 + 6) / 400][Stage 62:====================================>                 (271 + 6) / 400][Stage 62:=====================================>                (280 + 6) / 400][Stage 62:=======================================>              (293 + 6) / 400][Stage 62:=========================================>            (311 + 7) / 400][Stage 62:============================================>         (327 + 6) / 400][Stage 62:==============================================>       (341 + 6) / 400][Stage 62:================================================>     (356 + 6) / 400][Stage 62:=================================================>    (364 + 6) / 400][Stage 62:==================================================>   (375 + 7) / 400][Stage 62:====================================================> (390 + 6) / 400][Stage 66:============================>                           (25 + 6) / 50][Stage 66:========================================>               (36 + 6) / 50]                                                                                2026-02-12 11:17:32,527 INFO: Case IV Generated | Time Elapsed: 0.43 minutes
2026-02-12 11:17:32,528 INFO: ------------------------------------------------------------
2026-02-12 11:17:32,543 INFO: ------------------------------------------------------------
2026-02-12 11:17:32,549 INFO: MERGING RECORDS:
2026-02-12 11:17:32,556 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:17:32,557 INFO: ------------------------------------------------------------
2026-02-12 11:17:32,563 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:17:32,564 INFO: ------------------------------------------------------------
2026-02-12 11:17:32,564 INFO: APPENDING RECORDS:
2026-02-12 11:17:32,655 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=48, scale=1.0)
[Stage 67:==============================>                          (7 + 6) / 13][Stage 72:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:17:34,059 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:17:34,060 INFO: ------------------------------------------------------------
2026-02-12 11:17:34,060 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:17:35,645 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:17:35,646 INFO: ------------------------------------------------------------
2026-02-12 11:17:35,672 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] bulk_historical_load
2026-02-12 11:17:36,646 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:17:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:17:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:17:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=========>                                                 (1 + 5) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 7:=========>                                                 (1 + 5) / 6]                                                                                [Stage 8:>                                                          (0 + 6) / 6][Stage 8:=================================================>         (5 + 1) / 6]                                                                                2026-02-12 11:18:01,580 INFO: Cleaned case_1
2026-02-12 11:18:01,601 INFO: Cleaned case_2
2026-02-12 11:18:01,623 INFO: Cleaned case_3a
2026-02-12 11:18:01,647 INFO: Cleaned case_3b
2026-02-12 11:18:01,676 INFO: Cleaned case_4
2026-02-12 11:18:01,680 INFO: CLEANUP COMPLETED
2026-02-12 11:18:01,681 INFO: ============================================================
2026-02-12 11:18:01,682 INFO: ================================================================================
2026-02-12 11:18:01,683 INFO: SUMMARY PIPELINE - START
2026-02-12 11:18:01,684 INFO: ================================================================================
2026-02-12 11:18:01,685 INFO: ================================================================================
2026-02-12 11:18:01,686 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:18:01,686 INFO: ================================================================================
[Stage 14:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:18:02,707 INFO: Loading accounts from primary_catalog.main_all_scenarios.accounts_all
2026-02-12 11:18:02,944 INFO: Reading from primary_catalog.main_all_scenarios.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:18:02,945 INFO: Preparing source data...
2026-02-12 11:18:03,110 INFO: Applied 36 column mappings
2026-02-12 11:18:03,560 INFO: Applied 7 column transformations
2026-02-12 11:18:03,675 INFO: Created 2 inferred columns
2026-02-12 11:18:04,439 INFO: Validated 7 date columns
2026-02-12 11:18:04,629 INFO: Source data preparation complete
2026-02-12 11:18:04,767 INFO: Loading summary metadata from primary_catalog.main_all_scenarios.latest_summary
2026-02-12 11:18:04,813 INFO: Loaded metadata for existing accounts
2026-02-12 11:18:04,814 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:===>                                                   (13 + 7) / 200][Stage 29:====>                                                  (15 + 6) / 200][Stage 29:======>                                                (22 + 6) / 200][Stage 29:========>                                              (31 + 7) / 200][Stage 29:===========>                                           (42 + 7) / 200][Stage 29:==============>                                        (52 + 6) / 200][Stage 29:=================>                                     (62 + 7) / 200][Stage 29:===================>                                   (70 + 6) / 200][Stage 29:======================>                                (80 + 6) / 200][Stage 29:========================>                              (88 + 6) / 200][Stage 29:==========================>                            (95 + 6) / 200][Stage 29:===========================>                          (103 + 6) / 200][Stage 29:==============================>                       (112 + 7) / 200][Stage 29:================================>                     (122 + 6) / 200][Stage 29:==================================>                   (127 + 6) / 200][Stage 29:====================================>                 (135 + 6) / 200][Stage 29:=======================================>              (146 + 6) / 200][Stage 29:============================================>         (163 + 6) / 200][Stage 29:===============================================>      (176 + 6) / 200][Stage 29:===================================================>  (191 + 6) / 200][Stage 32:============>                                          (46 + 6) / 200][Stage 32:=================>                                     (64 + 6) / 200][Stage 32:=====================>                                 (77 + 6) / 200][Stage 32:=======================>                               (87 + 6) / 200][Stage 32:===========================>                          (100 + 6) / 200][Stage 32:================================>                     (120 + 7) / 200][Stage 32:====================================>                 (137 + 6) / 200][Stage 32:=========================================>            (155 + 6) / 200][Stage 32:===============================================>      (175 + 6) / 200][Stage 32:====================================================> (194 + 6) / 200]                                                                                2026-02-12 11:18:16,932 INFO: ------------------------------------------------------------
2026-02-12 11:18:16,933 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:18:16,934 INFO:   CASE_I: 2 records
2026-02-12 11:18:16,934 INFO:   CASE_IV: 1 records
2026-02-12 11:18:16,935 INFO:   CASE_III: 1 records
2026-02-12 11:18:16,935 INFO:   CASE_II: 1 records
2026-02-12 11:18:16,936 INFO: ------------------------------------------------------------
2026-02-12 11:18:16,936 INFO: Classification | Time Elapsed: 0.25 minutes
2026-02-12 11:18:16,980 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:18:16,981 INFO: ================================================================================
2026-02-12 11:18:16,982 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:18:16,982 INFO: ================================================================================
2026-02-12 11:18:16,983 INFO: CASE III - Processing backfill records
[Stage 39:=============>                                         (49 + 6) / 200][Stage 39:=================>                                     (64 + 6) / 200][Stage 39:========================>                              (88 + 6) / 200][Stage 39:==============================>                       (113 + 7) / 200][Stage 39:======================================>               (144 + 6) / 200][Stage 39:============================================>         (164 + 6) / 200][Stage 39:================================================>     (181 + 6) / 200][Stage 39:=====================================================>(199 + 1) / 200]                                                                                2026-02-12 11:18:19,456 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:18:19,457 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:18:19,482 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:18:19,841 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:18:21,727 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 46:============>                                          (46 + 6) / 200][Stage 46:=================>                                     (63 + 6) / 200][Stage 46:=====================>                                 (79 + 7) / 200][Stage 46:============================>                         (106 + 6) / 200][Stage 46:==================================>                   (127 + 6) / 200][Stage 46:=======================================>              (146 + 6) / 200][Stage 46:============================================>         (166 + 6) / 200][Stage 46:================================================>     (181 + 7) / 200]                                                                                [Stage 50:>                                                       (0 + 6) / 200][Stage 50:=>                                                      (6 + 6) / 200][Stage 50:==================>                                    (68 + 6) / 200][Stage 50:===================>                                   (72 + 6) / 200][Stage 50:====================>                                  (76 + 6) / 200][Stage 50:======================>                                (80 + 6) / 200][Stage 50:=======================>                               (84 + 6) / 200][Stage 50:========================>                              (88 + 6) / 200][Stage 50:========================>                              (90 + 6) / 200][Stage 50:==========================>                            (95 + 6) / 200][Stage 50:===========================>                           (99 + 6) / 200][Stage 50:===========================>                          (102 + 6) / 200][Stage 50:=============================>                        (108 + 6) / 200][Stage 50:==============================>                       (112 + 7) / 200][Stage 50:===============================>                      (116 + 6) / 200][Stage 50:================================>                     (120 + 6) / 200][Stage 50:=================================>                    (123 + 6) / 200][Stage 50:==================================>                   (128 + 6) / 200][Stage 50:===================================>                  (130 + 6) / 200][Stage 50:====================================>                 (135 + 6) / 200][Stage 50:=====================================>                (140 + 6) / 200][Stage 50:======================================>               (143 + 6) / 200][Stage 50:=======================================>              (146 + 6) / 200][Stage 50:========================================>             (149 + 6) / 200][Stage 50:=========================================>            (153 + 6) / 200][Stage 50:==========================================>           (157 + 6) / 200][Stage 50:==========================================>           (159 + 7) / 200][Stage 50:============================================>         (164 + 6) / 200][Stage 50:============================================>         (165 + 6) / 200][Stage 50:==============================================>       (171 + 6) / 200][Stage 50:===============================================>      (175 + 6) / 200][Stage 50:================================================>     (179 + 6) / 200][Stage 50:=================================================>    (185 + 6) / 200][Stage 50:===================================================>  (189 + 6) / 200][Stage 50:====================================================> (193 + 6) / 200][Stage 50:=====================================================>(199 + 1) / 200][Stage 54:=======================================>                (35 + 6) / 50][Stage 54:==================================================>     (45 + 5) / 50]                                                                                2026-02-12 11:18:42,050 INFO: Case III Part A Generated | Time Elapsed: 0.42 minutes
2026-02-12 11:18:42,051 INFO: ------------------------------------------------------------
2026-02-12 11:18:42,051 INFO: Part B: Updating future summary rows...
[Stage 57:===================>                                   (71 + 7) / 200][Stage 57:========================>                              (90 + 6) / 200][Stage 57:=============================>                        (110 + 7) / 200][Stage 57:====================================>                 (136 + 6) / 200][Stage 57:============================================>         (163 + 6) / 200][Stage 57:=================================================>    (184 + 6) / 200]                                                                                [Stage 70:=============================>                         (54 + 7) / 100][Stage 70:==============================================>        (85 + 7) / 100]                                                                                [Stage 73:=========================================>              (56 + 8) / 75]                                                                                2026-02-12 11:18:48,351 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 76:================>                                      (60 + 6) / 200][Stage 76:=====================>                                 (79 + 6) / 200][Stage 76:=========================>                             (92 + 6) / 200][Stage 76:==============================>                       (114 + 7) / 200][Stage 76:=======================================>              (148 + 6) / 200][Stage 76:================================================>     (179 + 6) / 200]                                                                                [Stage 80:========>                                              (31 + 6) / 200][Stage 80:============>                                          (44 + 6) / 200][Stage 80:===============>                                       (55 + 6) / 200][Stage 80:=================>                                     (65 + 7) / 200][Stage 80:======================>                                (82 + 6) / 200][Stage 80:==========================>                            (98 + 7) / 200][Stage 80:===============================>                      (116 + 6) / 200][Stage 80:====================================>                 (134 + 6) / 200][Stage 80:========================================>             (151 + 7) / 200][Stage 80:=============================================>        (168 + 6) / 200][Stage 80:=================================================>    (184 + 6) / 200][Stage 80:=====================================================>(199 + 1) / 200][Stage 84:==============>                                        (53 + 6) / 200][Stage 84:=================>                                     (63 + 7) / 200][Stage 84:======================>                                (83 + 6) / 200][Stage 84:============================>                         (104 + 6) / 200][Stage 84:==================================>                   (126 + 6) / 200][Stage 84:=======================================>              (146 + 6) / 200][Stage 84:============================================>         (164 + 7) / 200][Stage 84:=================================================>    (185 + 6) / 200]                                                                                2026-02-12 11:18:56,734 INFO: Case III Part B Generated | Time Elapsed: 0.24 minutes
2026-02-12 11:18:56,735 INFO: ------------------------------------------------------------
2026-02-12 11:18:56,765 INFO: 
>>> PROCESSING NEW ACCOUNTS (2 records)
2026-02-12 11:18:56,766 INFO: ================================================================================
2026-02-12 11:18:56,767 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:18:56,767 INFO: ================================================================================
2026-02-12 11:18:56,768 INFO: Processing new accounts
2026-02-12 11:18:57,451 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=2, scale=1.0)
[Stage 87:>                                                       (0 + 6) / 200][Stage 87:=====>                                                 (20 + 7) / 200]                                                                                2026-02-12 11:18:59,959 INFO: Case I Generated | Time Elapsed: 0.05 minutes
2026-02-12 11:18:59,960 INFO: ------------------------------------------------------------
2026-02-12 11:18:59,981 INFO: 
>>> PROCESSING BULK HISTORICAL (1 records)
2026-02-12 11:18:59,982 INFO: ================================================================================
2026-02-12 11:18:59,983 INFO: STEP 2d: Process Case IV (Bulk Historical Load)
2026-02-12 11:18:59,983 INFO: ================================================================================
2026-02-12 11:18:59,984 INFO: Processing bulk historical records
2026-02-12 11:19:00,040 INFO: Building complete history using window functions
[Stage 97:======================================>               (141 + 6) / 200][Stage 97:===============================================>      (175 + 7) / 200]                                                                                2026-02-12 11:19:04,242 INFO: Write partitions [case_4_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 104:============>                                         (47 + 8) / 200][Stage 104:(66 + 7) / 200][Stage 105:>(0 + 0) / 200][Stage 106:>(0 + 0) / 200][Stage 104:(91 + 7) / 200][Stage 105:>(0 + 0) / 200][Stage 106:>(0 + 0) / 200][Stage 104:(110 + 6) / 200][Stage 105:>(0 + 0) / 200][Stage 106:>(0 + 0) / 200][Stage 104:(136 + 7) / 200][Stage 105:>(0 + 0) / 200][Stage 106:>(0 + 0) / 200][Stage 104:(165 + 6) / 200][Stage 105:>(0 + 0) / 200][Stage 106:>(0 + 0) / 200]                                                                                [Stage 105:=====>        (75 + 6) / 200][Stage 106:>              (0 + 0) / 200][Stage 105:======>       (99 + 6) / 200][Stage 106:>              (0 + 0) / 200][Stage 105:========>    (130 + 6) / 200][Stage 106:>              (0 + 0) / 200][Stage 105:==========>  (164 + 6) / 200][Stage 106:>              (0 + 0) / 200][Stage 105:============>(191 + 5) / 200][Stage 106:>              (0 + 0) / 200]                                                                                [Stage 106:=================>                                    (66 + 7) / 200][Stage 106:=====================>                                (79 + 6) / 200][Stage 106:=========================>                            (94 + 6) / 200][Stage 106:================================>                    (124 + 8) / 200][Stage 106:=========================================>           (155 + 6) / 200][Stage 106:==================================================>  (189 + 6) / 200]                                                                                [Stage 109:=======>                                              (53 + 6) / 400][Stage 109:==========>                                           (79 + 8) / 400][Stage 109:==============>                                      (109 + 6) / 400][Stage 109:=================>                                   (131 + 7) / 400][Stage 109:=====================>                               (160 + 6) / 400][Stage 109:========================>                            (184 + 7) / 400][Stage 109:============================>                        (212 + 6) / 400][Stage 109:================================>                    (249 + 7) / 400][Stage 109:======================================>              (294 + 6) / 400][Stage 109:============================================>        (334 + 8) / 400][Stage 109:=================================================>   (370 + 6) / 400][Stage 109:====================================================>(395 + 5) / 400]                                                                                [Stage 116:=====>                                                (43 + 6) / 400][Stage 116:=======>                                              (58 + 7) / 400][Stage 116:==========>                                           (76 + 6) / 400][Stage 116:============>                                         (91 + 6) / 400][Stage 116:==============>                                      (113 + 5) / 400][Stage 116:=================>                                   (130 + 6) / 400][Stage 116:===================>                                 (146 + 6) / 400][Stage 116:====================>                                (156 + 6) / 400][Stage 116:======================>                              (173 + 6) / 400][Stage 116:=========================>                           (190 + 6) / 400][Stage 116:==========================>                          (202 + 6) / 400][Stage 116:=============================>                       (220 + 6) / 400][Stage 116:===============================>                     (238 + 7) / 400][Stage 116:=================================>                   (252 + 6) / 400][Stage 116:===================================>                 (267 + 6) / 400][Stage 116:=====================================>               (285 + 6) / 400][Stage 116:========================================>            (305 + 6) / 400][Stage 116:==========================================>          (319 + 6) / 400][Stage 116:===========================================>         (328 + 6) / 400][Stage 116:=============================================>       (345 + 7) / 400][Stage 116:================================================>    (364 + 6) / 400][Stage 116:===================================================> (386 + 6) / 400]                                                                                2026-02-12 11:19:20,273 INFO: Case IV Generated | Time Elapsed: 0.34 minutes
2026-02-12 11:19:20,274 INFO: ------------------------------------------------------------
2026-02-12 11:19:20,299 INFO: ------------------------------------------------------------
2026-02-12 11:19:20,300 INFO: MERGING RECORDS:
2026-02-12 11:19:22,289 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.03 minutes
2026-02-12 11:19:22,291 INFO: ------------------------------------------------------------
2026-02-12 11:19:24,658 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.04 minutes
2026-02-12 11:19:24,659 INFO: ------------------------------------------------------------
2026-02-12 11:19:24,663 INFO: APPENDING RECORDS:
2026-02-12 11:19:24,711 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=3, scale=1.0)
2026-02-12 11:19:28,293 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.06 minutes
2026-02-12 11:19:28,294 INFO: ------------------------------------------------------------
2026-02-12 11:19:28,294 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:19:24,059 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.06 minutes
2026-02-12 11:19:24,060 INFO: ------------------------------------------------------------
2026-02-12 11:19:24,092 INFO: 
>>> PROCESSING FORWARD ENTRIES (1 records)
2026-02-12 11:19:24,093 INFO: ================================================================================
2026-02-12 11:19:24,094 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:19:24,094 INFO: ================================================================================
2026-02-12 11:19:24,095 INFO: Processing forward entries
2026-02-12 11:19:24,133 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:19:24,642 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=1, scale=1.0)
26/02/12 11:19:26 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:335)
	org.apache.spark.sql.execution.CacheManager.$anonfun$cacheQuery$2(CacheManager.scala:127)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:122)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)
	org.apache.spark.sql.Dataset.persist(Dataset.scala:3797)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[Stage 155:=================================================>   (186 + 6) / 200][Stage 156:====>                                                 (16 + 6) / 200][Stage 156:============>                                         (46 + 6) / 200][Stage 156:==================>                                   (68 + 7) / 200][Stage 156:===========================>                         (103 + 7) / 200][Stage 156:===================================>                 (135 + 8) / 200][Stage 156:==========>  (169 + 6) / 200][Stage 157:>              (0 + 0) / 200][Stage 156:============>(197 + 3) / 200][Stage 157:>              (0 + 4) / 200]                                                                                [Stage 157:=================>                                    (65 + 6) / 200][Stage 157:=======================>                              (88 + 6) / 200][Stage 157:=============================>                       (111 + 6) / 200][Stage 157:=================================>                   (127 + 7) / 200][Stage 157:======================================>              (145 + 6) / 200][Stage 157:==============================================>      (175 + 6) / 200]                                                                                [Stage 161:====>                                                 (17 + 7) / 200][Stage 161:=======>                                              (26 + 6) / 200][Stage 161:=========>                                            (36 + 6) / 200][Stage 161:===========>                                          (44 + 6) / 200][Stage 161:==============>                                       (55 + 6) / 200][Stage 161:================>                                     (60 + 6) / 200][Stage 161:=================>                                    (65 + 6) / 200][Stage 161:==================>                                   (70 + 6) / 200][Stage 161:=====================>                                (79 + 6) / 200][Stage 161:========================>                             (89 + 6) / 200][Stage 161:==========================>                          (100 + 6) / 200][Stage 161:=============================>                       (110 + 6) / 200][Stage 161:================================>                    (122 + 6) / 200][Stage 161:===================================>                 (133 + 6) / 200][Stage 161:=====================================>               (142 + 6) / 200][Stage 161:========================================>            (152 + 6) / 200][Stage 161:==========================================>          (161 + 6) / 200][Stage 161:=============================================>       (173 + 5) / 200][Stage 161:===============================================>     (179 + 6) / 200][Stage 161:==================================================>  (189 + 6) / 200]                                                                                2026-02-12 11:19:37,647 INFO: Case II Generated | Time Elapsed: 0.23 minutes
2026-02-12 11:19:37,648 INFO: ------------------------------------------------------------
2026-02-12 11:19:37,649 INFO: ------------------------------------------------------------
2026-02-12 11:19:37,649 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:19:37,675 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:19:38,259 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:19:38,259 INFO: ------------------------------------------------------------
2026-02-12 11:19:40,466 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.04 minutes
2026-02-12 11:19:40,467 INFO: ------------------------------------------------------------
2026-02-12 11:19:40,467 INFO: PROCESS COMPLETED - Deleting the persisted classification results
26/02/12 11:19:41 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$19(DataSourceV2Strategy.scala:331)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.run(WriteToDataSourceV2Exec.scala:276)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	jdk.internal.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[PASS] all_scenarios
2026-02-12 11:19:42,376 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:19:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:19:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:19:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6][Stage 3:=================================================>         (5 + 1) / 6]                                                                                [Stage 4:>                                                          (0 + 6) / 6][Stage 4:=============================>                             (3 + 3) / 6]                                                                                [Stage 7:===================>                                       (2 + 4) / 6]                                                                                [Stage 8:>                                                          (0 + 6) / 6][Stage 8:=============================>                             (3 + 3) / 6]                                                                                2026-02-12 11:20:06,875 INFO: Cleaned case_1
2026-02-12 11:20:06,900 INFO: Cleaned case_2
2026-02-12 11:20:06,929 INFO: Cleaned case_3a
2026-02-12 11:20:06,952 INFO: Cleaned case_3b
2026-02-12 11:20:06,975 INFO: Cleaned case_4
2026-02-12 11:20:06,976 INFO: CLEANUP COMPLETED
2026-02-12 11:20:06,977 INFO: ============================================================
2026-02-12 11:20:06,977 INFO: ================================================================================
2026-02-12 11:20:06,978 INFO: SUMMARY PIPELINE - START
2026-02-12 11:20:06,979 INFO: ================================================================================
2026-02-12 11:20:06,979 INFO: ================================================================================
2026-02-12 11:20:06,980 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:20:06,980 INFO: ================================================================================
2026-02-12 11:20:10,416 INFO: Loading accounts from primary_catalog.main_50_cases.accounts_all
2026-02-12 11:20:10,649 INFO: Reading from primary_catalog.main_50_cases.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:20:10,649 INFO: Preparing source data...
2026-02-12 11:20:10,862 INFO: Applied 36 column mappings
2026-02-12 11:20:11,367 INFO: Applied 7 column transformations
2026-02-12 11:20:11,523 INFO: Created 2 inferred columns
2026-02-12 11:20:12,357 INFO: Validated 7 date columns
2026-02-12 11:20:12,584 INFO: Source data preparation complete
2026-02-12 11:20:12,778 INFO: Loading summary metadata from primary_catalog.main_50_cases.latest_summary
2026-02-12 11:20:12,846 INFO: Loaded metadata for existing accounts
2026-02-12 11:20:12,847 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:==>                                                     (9 + 7) / 200][Stage 29:=====>                                                 (19 + 6) / 200][Stage 29:=======>                                               (27 + 6) / 200][Stage 29:==========>                                            (38 + 7) / 200][Stage 29:==============>                                        (54 + 6) / 200][Stage 29:==================>                                    (66 + 6) / 200][Stage 29:====================>                                  (74 + 6) / 200][Stage 29:======================>                                (83 + 7) / 200][Stage 29:==========================>                            (97 + 6) / 200][Stage 29:=============================>                        (111 + 6) / 200][Stage 29:=================================>                    (125 + 6) / 200][Stage 29:====================================>                 (135 + 7) / 200][Stage 29:========================================>             (149 + 6) / 200][Stage 29:============================================>         (164 + 6) / 200][Stage 29:================================================>     (180 + 8) / 200]                                                                                2026-02-12 11:20:20,908 INFO: ------------------------------------------------------------
2026-02-12 11:20:20,909 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:20:20,910 INFO:   CASE_I: 30 records
2026-02-12 11:20:20,912 INFO:   CASE_IV: 10 records
2026-02-12 11:20:20,913 INFO:   CASE_II: 10 records
2026-02-12 11:20:20,914 INFO: ------------------------------------------------------------
2026-02-12 11:20:20,915 INFO: Classification | Time Elapsed: 0.23 minutes
2026-02-12 11:20:21,024 INFO: 
>>> PROCESSING NEW ACCOUNTS (30 records)
2026-02-12 11:20:21,025 INFO: ================================================================================
2026-02-12 11:20:21,026 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:20:21,026 INFO: ================================================================================
2026-02-12 11:20:21,027 INFO: Processing new accounts
2026-02-12 11:20:26,940 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=30, scale=1.0)
[Stage 39:===========>                                           (41 + 6) / 200][Stage 39:==============>                                        (54 + 6) / 200][Stage 39:===================>                                   (70 + 6) / 200][Stage 39:=======================>                               (86 + 6) / 200][Stage 39:===========================>                          (101 + 6) / 200][Stage 39:===============================>                      (117 + 7) / 200][Stage 39:===================================>                  (132 + 6) / 200][Stage 39:=======================================>              (146 + 6) / 200][Stage 39:===========================================>          (160 + 6) / 200][Stage 39:==============================================>       (173 + 6) / 200][Stage 39:===================================================>  (190 + 6) / 200][Stage 42:==================>                                    (68 + 6) / 200][Stage 42:===========================>                           (99 + 6) / 200][Stage 42:===================================>                  (132 + 8) / 200][Stage 42:============================================>         (163 + 8) / 200][Stage 42:====================================================> (196 + 4) / 200][Stage 46:================================>                       (29 + 6) / 50][Stage 46:================================================>       (43 + 7) / 50]                                                                                2026-02-12 11:20:27,841 INFO: Case I Generated | Time Elapsed: 0.11 minutes
2026-02-12 11:20:27,842 INFO: ------------------------------------------------------------
2026-02-12 11:20:27,878 INFO: 
>>> PROCESSING BULK HISTORICAL (10 records)
2026-02-12 11:20:27,879 INFO: ================================================================================
2026-02-12 11:20:27,880 INFO: STEP 2d: Process Case IV (Bulk Historical Load)
2026-02-12 11:20:27,880 INFO: ================================================================================
2026-02-12 11:20:27,881 INFO: Processing bulk historical records
2026-02-12 11:20:28,002 INFO: Building complete history using window functions
[Stage 49:=====================>                                 (79 + 6) / 200][Stage 49:===========================>                          (101 + 6) / 200][Stage 49:================================>                     (121 + 6) / 200][Stage 49:=========================================>            (153 + 6) / 200][Stage 49:=================================================>    (184 + 7) / 200]                                                                                2026-02-12 11:20:33,403 INFO: Write partitions [case_4_temp]: 50 (base=200, expected_rows=10, scale=1.0)
[Stage 56:>(61 + 6) / 200][Stage 57:> (0 + 0) / 200][Stage 58:> (0 + 0) / 200][Stage 56:>(80 + 6) / 200][Stage 57:> (0 + 0) / 200][Stage 58:> (0 + 0) / 200][Stage 56:>(94 + 6) / 200][Stage 57:> (0 + 0) / 200][Stage 58:> (0 + 0) / 200][Stage 56:(112 + 6) / 200][Stage 57:> (0 + 0) / 200][Stage 58:> (0 + 0) / 200][Stage 56:(132 + 6) / 200][Stage 57:> (0 + 0) / 200][Stage 58:> (0 + 0) / 200][Stage 56:(154 + 7) / 200][Stage 57:> (0 + 0) / 200][Stage 58:> (0 + 0) / 200][Stage 56:(174 + 6) / 200][Stage 57:> (0 + 0) / 200][Stage 58:> (0 + 0) / 200][Stage 56:(197 + 3) / 200][Stage 57:> (0 + 3) / 200][Stage 58:> (0 + 0) / 200]                                                                                [Stage 57:===>           (50 + 6) / 200][Stage 58:>               (0 + 0) / 200][Stage 57:=====>         (73 + 6) / 200][Stage 58:>               (0 + 0) / 200][Stage 57:=======>       (96 + 6) / 200][Stage 58:>               (0 + 0) / 200][Stage 57:=======>      (114 + 8) / 200][Stage 58:>               (0 + 0) / 200][Stage 57:=========>    (129 + 6) / 200][Stage 58:>               (0 + 0) / 200][Stage 57:==========>   (146 + 6) / 200][Stage 58:>               (0 + 0) / 200][Stage 57:============> (176 + 8) / 200][Stage 58:>               (0 + 0) / 200]                                                                                [Stage 58:====>          (62 + 6) / 200][Stage 59:>               (0 + 0) / 400][Stage 58:======>        (91 + 6) / 200][Stage 59:>               (0 + 0) / 400][Stage 58:========>     (118 + 6) / 200][Stage 59:>               (0 + 0) / 400][Stage 58:(145 + 6) / 200][Stage 59:> (0 + 0) / 400][Stage 60:> (0 + 0) / 400][Stage 58:(169 + 8) / 200][Stage 59:> (0 + 0) / 400][Stage 60:> (0 + 0) / 400][Stage 58:(188 + 6) / 200][Stage 59:> (0 + 0) / 400][Stage 60:> (0 + 0) / 400]                                                                                [Stage 59:==>            (58 + 7) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:===>           (83 + 9) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:====>         (118 + 9) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:====>         (139 + 7) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:======>       (174 + 6) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:=======>      (200 + 6) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:========>     (237 + 6) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:=========>    (274 + 6) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:==========>   (309 + 7) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:============> (344 + 6) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:=============>(381 + 6) / 400][Stage 60:>               (0 + 0) / 400]                                                                                [Stage 60:==========>                                            (78 + 6) / 400][Stage 60:=============>                                        (102 + 6) / 400][Stage 60:================>                                     (122 + 6) / 400][Stage 60:===================>                                  (142 + 6) / 400][Stage 60:=======================>                              (172 + 6) / 400][Stage 60:===========================>                          (200 + 6) / 400][Stage 60:===============================>                      (230 + 7) / 400]                                                                                2026-02-12 11:20:47,654 INFO: Case IV Generated | Time Elapsed: 0.33 minutes
2026-02-12 11:20:47,655 INFO: ------------------------------------------------------------
2026-02-12 11:20:47,671 INFO: ------------------------------------------------------------
2026-02-12 11:20:47,671 INFO: MERGING RECORDS:
2026-02-12 11:20:47,680 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:20:47,681 INFO: ------------------------------------------------------------
2026-02-12 11:20:47,690 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:20:47,691 INFO: ------------------------------------------------------------
2026-02-12 11:20:47,691 INFO: APPENDING RECORDS:
2026-02-12 11:20:47,767 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=40, scale=1.0)
2026-02-12 11:20:49,604 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.03 minutes
2026-02-12 11:20:49,605 INFO: ------------------------------------------------------------
2026-02-12 11:20:49,606 INFO: UPDATING LATEST SUMMARY:
[Stage 76:===================================================>      (8 + 1) / 9]                                                                                2026-02-12 11:20:51,774 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.03 minutes
2026-02-12 11:20:51,775 INFO: ------------------------------------------------------------
2026-02-12 11:20:51,811 INFO: 
>>> PROCESSING FORWARD ENTRIES (10 records)
2026-02-12 11:20:51,812 INFO: ================================================================================
2026-02-12 11:20:51,812 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:20:51,813 INFO: ================================================================================
2026-02-12 11:20:51,814 INFO: Processing forward entries
2026-02-12 11:20:51,865 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:20:52,583 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=10, scale=1.0)
[Stage 85:=========>                                             (33 + 6) / 200][Stage 85:>(56 + 6) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200][Stage 85:>(79 + 6) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200][Stage 85:(100 + 6) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200][Stage 85:(120 + 7) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200][Stage 85:(140 + 6) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200][Stage 85:(154 + 8) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200][Stage 85:(174 + 6) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200][Stage 85:(189 + 7) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200][Stage 86:=>             (24 + 7) / 200][Stage 87:>               (0 + 0) / 200][Stage 86:====>          (60 + 6) / 200][Stage 87:>               (0 + 0) / 200][Stage 86:=======>      (109 + 7) / 200][Stage 87:>               (0 + 0) / 200][Stage 86:===========>  (158 + 6) / 200][Stage 87:>               (0 + 0) / 200][Stage 87:===>                                                   (12 + 7) / 200][Stage 87:===============>                                       (56 + 6) / 200][Stage 87:============================>                         (105 + 7) / 200][Stage 87:========================================>             (150 + 6) / 200][Stage 87:=====================================================>(198 + 2) / 200][Stage 88:==========>                                            (39 + 8) / 200][Stage 88:==================>                                    (67 + 6) / 200][Stage 88:===========================>                           (99 + 6) / 200][Stage 88:================================>                     (122 + 6) / 200][Stage 88:==========>   (157 + 6) / 200][Stage 89:>               (0 + 0) / 200][Stage 88:=============>(190 + 7) / 200][Stage 89:>               (0 + 0) / 200]                                                                                [Stage 89:===============>                                       (57 + 6) / 200][Stage 89:=====================>                                 (77 + 6) / 200][Stage 89:============================>                         (106 + 7) / 200][Stage 89:=====================================>                (139 + 6) / 200][Stage 89:============================================>         (166 + 6) / 200][Stage 89:====================================================> (195 + 5) / 200]                                                                                [Stage 93:=>                                                      (6 + 6) / 200][Stage 93:===>                                                   (12 + 6) / 200][Stage 93:====>                                                  (17 + 6) / 200][Stage 93:======>                                                (24 + 6) / 200][Stage 93:========>                                              (31 + 7) / 200][Stage 93:==========>                                            (37 + 6) / 200][Stage 93:============>                                          (44 + 6) / 200][Stage 93:==============>                                        (53 + 6) / 200][Stage 93:================>                                      (59 + 6) / 200][Stage 93:==================>                                    (67 + 7) / 200][Stage 93:====================>                                  (75 + 6) / 200][Stage 93:======================>                                (83 + 7) / 200][Stage 93:==========================>                            (95 + 5) / 200][Stage 93:===========================>                          (101 + 6) / 200][Stage 93:============================>                         (107 + 6) / 200][Stage 93:===============================>                      (116 + 7) / 200][Stage 93:==================================>                   (127 + 6) / 200][Stage 93:====================================>                 (135 + 6) / 200][Stage 93:=====================================>                (140 + 6) / 200][Stage 93:=======================================>              (148 + 7) / 200][Stage 93:==========================================>           (156 + 6) / 200][Stage 93:============================================>         (165 + 7) / 200][Stage 93:==============================================>       (174 + 6) / 200][Stage 93:=================================================>    (184 + 6) / 200][Stage 93:===================================================>  (191 + 6) / 200][Stage 97:===========================================>            (39 + 6) / 50]                                                                                2026-02-12 11:21:07,763 INFO: Case II Generated | Time Elapsed: 0.27 minutes
2026-02-12 11:21:07,764 INFO: ------------------------------------------------------------
2026-02-12 11:21:07,767 INFO: ------------------------------------------------------------
2026-02-12 11:21:07,768 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:21:07,804 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=10, scale=1.0)
2026-02-12 11:21:08,579 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:21:08,579 INFO: ------------------------------------------------------------
2026-02-12 11:21:11,759 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.05 minutes
2026-02-12 11:21:11,759 INFO: ------------------------------------------------------------
2026-02-12 11:21:11,760 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] comprehensive_50_cases
2026-02-12 11:21:13,053 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:21:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:21:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:21:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:===================>                                       (2 + 4) / 6]                                                                                [Stage 4:=============================>                             (3 + 3) / 6]                                                                                [Stage 7:=============================>                             (3 + 3) / 6]                                                                                [Stage 8:===================>                                       (2 + 4) / 6]                                                                                2026-02-12 11:21:36,231 INFO: Cleaned case_1
2026-02-12 11:21:36,251 INFO: Cleaned case_2
2026-02-12 11:21:36,271 INFO: Cleaned case_3a
2026-02-12 11:21:36,293 INFO: Cleaned case_3b
2026-02-12 11:21:36,316 INFO: Cleaned case_4
2026-02-12 11:21:36,317 INFO: CLEANUP COMPLETED
2026-02-12 11:21:36,317 INFO: ============================================================
2026-02-12 11:21:36,318 INFO: ================================================================================
2026-02-12 11:21:36,319 INFO: SUMMARY PIPELINE - START
2026-02-12 11:21:36,319 INFO: ================================================================================
2026-02-12 11:21:36,320 INFO: ================================================================================
2026-02-12 11:21:36,321 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:21:36,321 INFO: ================================================================================
2026-02-12 11:21:39,180 INFO: Loading accounts from primary_catalog.main_edge_null.accounts_all
2026-02-12 11:21:39,351 INFO: Reading from primary_catalog.main_edge_null.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:21:39,352 INFO: Preparing source data...
2026-02-12 11:21:39,557 INFO: Applied 36 column mappings
2026-02-12 11:21:40,112 INFO: Applied 7 column transformations
2026-02-12 11:21:40,294 INFO: Created 2 inferred columns
2026-02-12 11:21:41,245 INFO: Validated 7 date columns
2026-02-12 11:21:41,434 INFO: Source data preparation complete
2026-02-12 11:21:41,634 INFO: Loading summary metadata from primary_catalog.main_edge_null.latest_summary
2026-02-12 11:21:41,708 INFO: Loaded metadata for existing accounts
2026-02-12 11:21:41,709 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:=================================>                    (124 + 6) / 200][Stage 29:====================================>                 (137 + 6) / 200][Stage 29:========================================>             (150 + 6) / 200][Stage 29:============================================>         (164 + 6) / 200][Stage 29:================================================>     (179 + 7) / 200][Stage 29:====================================================> (193 + 7) / 200][Stage 32:======>                                                (22 + 6) / 200][Stage 32:==========>                                            (38 + 6) / 200][Stage 32:==============>                                        (53 + 7) / 200][Stage 32:===================>                                   (70 + 6) / 200][Stage 32:========================>                              (88 + 6) / 200][Stage 32:============================>                         (105 + 6) / 200][Stage 32:================================>                     (122 + 6) / 200][Stage 32:======================================>               (141 + 7) / 200][Stage 32:===========================================>          (160 + 8) / 200][Stage 32:=================================================>    (183 + 6) / 200]                                                                                2026-02-12 11:21:48,890 INFO: ------------------------------------------------------------
2026-02-12 11:21:48,890 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:21:48,891 INFO:   CASE_II: 1 records
2026-02-12 11:21:48,892 INFO:   CASE_I: 1 records
2026-02-12 11:21:48,892 INFO: ------------------------------------------------------------
2026-02-12 11:21:48,893 INFO: Classification | Time Elapsed: 0.21 minutes
2026-02-12 11:21:48,957 INFO: 
>>> PROCESSING NEW ACCOUNTS (1 records)
2026-02-12 11:21:48,958 INFO: ================================================================================
2026-02-12 11:21:48,959 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:21:48,959 INFO: ================================================================================
2026-02-12 11:21:48,960 INFO: Processing new accounts
2026-02-12 11:21:49,992 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 39:=======>                                               (27 + 6) / 200][Stage 39:==========>                                            (38 + 7) / 200][Stage 39:============>                                          (46 + 6) / 200][Stage 39:===============>                                       (55 + 7) / 200][Stage 39:==================>                                    (68 + 6) / 200][Stage 39:======================>                                (81 + 6) / 200][Stage 39:==========================>                            (96 + 6) / 200][Stage 39:=============================>                        (110 + 6) / 200][Stage 39:================================>                     (122 + 8) / 200][Stage 39:=====================================>                (140 + 6) / 200][Stage 39:=========================================>            (152 + 6) / 200][Stage 39:=============================================>        (169 + 6) / 200][Stage 39:================================================>     (181 + 7) / 200][Stage 39:====================================================> (195 + 5) / 200][Stage 42:===================>                                   (71 + 6) / 200][Stage 42:==========================>                            (97 + 7) / 200][Stage 42:==================================>                   (126 + 8) / 200][Stage 42:=========================================>            (155 + 7) / 200][Stage 42:===================================================>  (192 + 6) / 200]                                                                                2026-02-12 11:21:56,348 INFO: Case I Generated | Time Elapsed: 0.12 minutes
2026-02-12 11:21:56,348 INFO: ------------------------------------------------------------
2026-02-12 11:21:56,374 INFO: ------------------------------------------------------------
2026-02-12 11:21:56,374 INFO: MERGING RECORDS:
2026-02-12 11:21:56,387 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:21:56,387 INFO: ------------------------------------------------------------
2026-02-12 11:21:56,395 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:21:56,395 INFO: ------------------------------------------------------------
2026-02-12 11:21:56,396 INFO: APPENDING RECORDS:
2026-02-12 11:21:56,433 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:21:57,326 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:21:57,327 INFO: ------------------------------------------------------------
2026-02-12 11:21:57,327 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:21:59,024 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:21:59,025 INFO: ------------------------------------------------------------
2026-02-12 11:21:59,053 INFO: 
>>> PROCESSING FORWARD ENTRIES (1 records)
2026-02-12 11:21:59,054 INFO: ================================================================================
2026-02-12 11:21:59,056 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:21:59,056 INFO: ================================================================================
2026-02-12 11:21:59,057 INFO: Processing forward entries
2026-02-12 11:21:59,099 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:22:00,071 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 62:>(43 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:>(59 + 7) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:>(70 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:>(81 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:>(91 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(104 + 7) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(118 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(132 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(147 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(166 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(184 + 7) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 63:=>             (14 + 7) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:===>           (42 + 7) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:=====>         (73 + 7) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:========>     (118 + 6) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:===========>  (161 + 6) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:=============>(191 + 6) / 200][Stage 64:>               (0 + 0) / 200][Stage 64:===========>                                           (42 + 6) / 200][Stage 64:======================>                                (83 + 7) / 200][Stage 64:==================================>                   (127 + 7) / 200][Stage 64:==============================================>       (173 + 7) / 200][Stage 65:============>                                          (46 + 6) / 200][Stage 65:===================>                                   (71 + 7) / 200][Stage 65:=========================>                             (92 + 6) / 200][Stage 65:=============================>                        (110 + 7) / 200][Stage 65:=========>    (133 + 6) / 200][Stage 66:>               (0 + 0) / 200][Stage 65:==========>   (157 + 7) / 200][Stage 66:>               (0 + 0) / 200][Stage 65:============> (180 + 6) / 200][Stage 66:>               (0 + 0) / 200]                                                                                [Stage 66:================>                                      (60 + 9) / 200][Stage 66:=======================>                               (85 + 6) / 200][Stage 66:============================>                         (104 + 6) / 200][Stage 66:===================================>                  (130 + 6) / 200][Stage 66:=======================================>              (148 + 7) / 200]                                                                                [Stage 70:===================================>                  (130 + 6) / 200][Stage 70:=====================================>                (138 + 6) / 200][Stage 70:=======================================>              (145 + 6) / 200][Stage 70:=========================================>            (153 + 6) / 200][Stage 70:===========================================>          (160 + 6) / 200][Stage 70:=============================================>        (169 + 6) / 200][Stage 70:===============================================>      (177 + 6) / 200][Stage 70:=================================================>    (185 + 6) / 200][Stage 70:====================================================> (194 + 6) / 200][Stage 74:=======================================>                (35 + 6) / 50]                                                                                2026-02-12 11:22:15,197 INFO: Case II Generated | Time Elapsed: 0.27 minutes
2026-02-12 11:22:15,198 INFO: ------------------------------------------------------------
2026-02-12 11:22:15,199 INFO: ------------------------------------------------------------
2026-02-12 11:22:15,199 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:22:15,225 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:22:15,850 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:22:15,851 INFO: ------------------------------------------------------------
2026-02-12 11:22:18,688 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.05 minutes
2026-02-12 11:22:18,688 INFO: ------------------------------------------------------------
2026-02-12 11:22:18,693 INFO: PROCESS COMPLETED - Deleting the persisted classification results
26/02/12 11:22:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=========>                                                 (1 + 5) / 6]                                                                                [Stage 3:>                                                          (0 + 6) / 6][Stage 3:===================>                                       (2 + 4) / 6]                                                                                [Stage 4:>                                                          (0 + 6) / 6][Stage 4:=======================================>                   (4 + 2) / 6]                                                                                [PASS] null_update
2026-02-12 11:22:26,333 INFO: Cleaned case_1
2026-02-12 11:22:26,347 INFO: Cleaned case_2
2026-02-12 11:22:26,363 INFO: Cleaned case_3a
2026-02-12 11:22:26,379 INFO: Cleaned case_3b
2026-02-12 11:22:26,392 INFO: Cleaned case_4
2026-02-12 11:22:26,393 INFO: CLEANUP COMPLETED
2026-02-12 11:22:26,393 INFO: ============================================================
2026-02-12 11:22:26,394 INFO: ================================================================================
2026-02-12 11:22:26,394 INFO: SUMMARY PIPELINE - START
2026-02-12 11:22:26,395 INFO: ================================================================================
2026-02-12 11:22:26,395 INFO: ================================================================================
2026-02-12 11:22:26,395 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:22:26,396 INFO: ================================================================================
26/02/12 11:22:26 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$19(DataSourceV2Strategy.scala:331)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.run(WriteToDataSourceV2Exec.scala:276)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
26/02/12 11:22:26 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$19(DataSourceV2Strategy.scala:331)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.run(WriteToDataSourceV2Exec.scala:276)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
2026-02-12 11:22:27,512 INFO: Loading accounts from primary_catalog.main_edge_dup.accounts_all
2026-02-12 11:22:27,548 INFO: Reading from primary_catalog.main_edge_dup.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2020-01-02 00:00:00
2026-02-12 11:22:27,549 INFO: Preparing source data...
2026-02-12 11:22:27,704 INFO: Applied 36 column mappings
2026-02-12 11:22:27,886 INFO: Applied 7 column transformations
2026-02-12 11:22:27,943 INFO: Created 2 inferred columns
2026-02-12 11:22:28,358 INFO: Validated 7 date columns
2026-02-12 11:22:28,433 INFO: Source data preparation complete
2026-02-12 11:22:28,458 INFO: Loading summary metadata from primary_catalog.main_edge_dup.latest_summary
2026-02-12 11:22:28,472 INFO: Loaded metadata for existing accounts
2026-02-12 11:22:28,473 INFO: Classifying accounts into Case I/II/III/IV
[Stage 25:==========>                                            (39 + 6) / 200][Stage 25:==============>                                        (52 + 6) / 200][Stage 25:====================>                                  (74 + 7) / 200][Stage 25:=========================>                             (94 + 6) / 200][Stage 25:===============================>                      (118 + 6) / 200][Stage 25:===================================>                  (132 + 7) / 200][Stage 25:=========================================>            (154 + 6) / 200][Stage 25:==============================================>       (173 + 6) / 200][Stage 25:====================================================> (193 + 6) / 200][Stage 28:==================>                                    (68 + 6) / 200][Stage 28:========================>                              (89 + 7) / 200][Stage 28:=============================>                        (111 + 6) / 200][Stage 28:===================================>                  (131 + 6) / 200][Stage 28:=======================================>              (148 + 7) / 200][Stage 28:=============================================>        (170 + 6) / 200][Stage 28:==================================================>   (188 + 6) / 200]                                                                                2026-02-12 11:22:34,535 INFO: ------------------------------------------------------------
2026-02-12 11:22:34,541 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:22:34,541 INFO:   CASE_I: 1 records
2026-02-12 11:22:34,542 INFO: ------------------------------------------------------------
2026-02-12 11:22:34,542 INFO: Classification | Time Elapsed: 0.14 minutes
2026-02-12 11:22:34,591 INFO: 
>>> PROCESSING NEW ACCOUNTS (1 records)
2026-02-12 11:22:34,592 INFO: ================================================================================
2026-02-12 11:22:34,593 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:22:34,593 INFO: ================================================================================
2026-02-12 11:22:34,594 INFO: Processing new accounts
2026-02-12 11:22:35,098 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 35:=======>                                               (28 + 7) / 200]26/02/12 11:22:36 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$9(DataSourceV2Strategy.scala:272)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)
	org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[Stage 35:============>                                          (46 + 6) / 200][Stage 35:==================>                                    (66 + 6) / 200]                                                                                2026-02-12 11:22:42,147 INFO: Case I Generated | Time Elapsed: 0.13 minutes
2026-02-12 11:22:42,147 INFO: ------------------------------------------------------------
2026-02-12 11:22:42,166 INFO: ------------------------------------------------------------
2026-02-12 11:22:42,167 INFO: MERGING RECORDS:
2026-02-12 11:22:42,173 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:22:42,173 INFO: ------------------------------------------------------------
2026-02-12 11:22:42,180 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:22:42,180 INFO: ------------------------------------------------------------
2026-02-12 11:22:42,181 INFO: APPENDING RECORDS:
2026-02-12 11:22:42,207 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:22:37,345 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: -0.08 minutes
2026-02-12 11:22:37,346 INFO: ------------------------------------------------------------
2026-02-12 11:22:37,346 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:22:39,012 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: -0.08 minutes
2026-02-12 11:22:39,014 INFO: ------------------------------------------------------------
2026-02-12 11:22:39,047 INFO: PROCESS COMPLETED - Deleting the persisted classification results
26/02/12 11:22:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=========>                                                 (1 + 5) / 6]                                                                                [Stage 3:>                                                          (0 + 6) / 6]                                                                                26/02/12 11:22:44 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$9(DataSourceV2Strategy.scala:272)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)
	org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
26/02/12 11:22:44 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$9(DataSourceV2Strategy.scala:272)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)
	org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[Stage 7:=======================================>                   (4 + 2) / 6]                                                                                26/02/12 11:22:45 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:335)
	org.apache.spark.sql.execution.CacheManager.$anonfun$cacheQuery$2(CacheManager.scala:127)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:122)
	org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)
	org.apache.spark.sql.Dataset.persist(Dataset.scala:3797)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[Stage 8:=======================================>                   (4 + 2) / 6]                                                                                [PASS] duplicate_records
2026-02-12 11:22:46,530 INFO: Cleaned case_1
2026-02-12 11:22:46,544 INFO: Cleaned case_2
2026-02-12 11:22:46,559 INFO: Cleaned case_3a
2026-02-12 11:22:46,577 INFO: Cleaned case_3b
2026-02-12 11:22:46,598 INFO: Cleaned case_4
2026-02-12 11:22:46,599 INFO: CLEANUP COMPLETED
2026-02-12 11:22:46,600 INFO: ============================================================
2026-02-12 11:22:46,601 INFO: ================================================================================
2026-02-12 11:22:46,601 INFO: SUMMARY PIPELINE - START
2026-02-12 11:22:46,602 INFO: ================================================================================
2026-02-12 11:22:46,603 INFO: ================================================================================
2026-02-12 11:22:46,603 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:22:46,604 INFO: ================================================================================
2026-02-12 11:22:47,580 INFO: Loading accounts from primary_catalog.main_edge_backfill.accounts_all
2026-02-12 11:22:47,666 INFO: Reading from primary_catalog.main_edge_backfill.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:22:47,667 INFO: Preparing source data...
2026-02-12 11:22:47,847 INFO: Applied 36 column mappings
2026-02-12 11:22:47,994 INFO: Applied 7 column transformations
2026-02-12 11:22:48,054 INFO: Created 2 inferred columns
2026-02-12 11:22:48,407 INFO: Validated 7 date columns
2026-02-12 11:22:48,578 INFO: Source data preparation complete
2026-02-12 11:22:48,650 INFO: Loading summary metadata from primary_catalog.main_edge_backfill.latest_summary
2026-02-12 11:22:48,667 INFO: Loaded metadata for existing accounts
2026-02-12 11:22:48,668 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:============>                                          (45 + 6) / 200][Stage 29:=================>                                     (62 + 7) / 200][Stage 29:=======================>                               (87 + 8) / 200][Stage 29:=============================>                        (108 + 6) / 200][Stage 29:================================>                     (119 + 7) / 200][Stage 29:====================================>                 (136 + 6) / 200][Stage 29:========================================>             (150 + 6) / 200][Stage 29:============================================>         (164 + 6) / 200][Stage 29:==============================================>       (174 + 7) / 200][Stage 29:=====================================================>(197 + 3) / 200][Stage 32:=================>                                     (62 + 6) / 200][Stage 32:====================>                                  (74 + 9) / 200][Stage 32:==========================>                            (97 + 6) / 200][Stage 32:===================================>                  (132 + 6) / 200][Stage 32:=========================================>            (153 + 7) / 200][Stage 32:================================================>     (179 + 6) / 200]                                                                                2026-02-12 11:22:54,655 INFO: ------------------------------------------------------------
2026-02-12 11:22:54,656 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:22:54,657 INFO:   CASE_III: 1 records
2026-02-12 11:22:54,657 INFO: ------------------------------------------------------------
2026-02-12 11:22:54,658 INFO: Classification | Time Elapsed: 0.13 minutes
2026-02-12 11:22:54,681 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:22:54,681 INFO: ================================================================================
2026-02-12 11:22:54,682 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:22:54,682 INFO: ================================================================================
2026-02-12 11:22:54,683 INFO: CASE III - Processing backfill records
[Stage 39:=======================>                               (87 + 6) / 200][Stage 39:===============================>                      (118 + 6) / 200]26/02/12 11:22:55 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$9(DataSourceV2Strategy.scala:272)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)
	org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[Stage 39:====================================>                 (134 + 6) / 200][Stage 39:==========================================>           (156 + 6) / 200][Stage 39:================================================>     (178 + 6) / 200]                                                                                2026-02-12 11:22:56,639 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:22:56,640 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:22:56,660 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:22:57,102 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:22:58,432 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 46:======================>                                (81 + 6) / 200][Stage 46:============================>                         (106 + 7) / 200][Stage 46:=====================================>                (140 + 7) / 200][Stage 46:==================================================>   (188 + 6) / 200]                                                                                [Stage 50:>                                                       (0 + 6) / 200][Stage 50:=>                                                      (6 + 6) / 200][Stage 50:==>                                                     (9 + 6) / 200][Stage 50:=============================================>        (170 + 6) / 200][Stage 50:===============================================>      (176 + 6) / 200][Stage 50:=================================================>    (185 + 6) / 200][Stage 50:===================================================>  (191 + 6) / 200][Stage 50:====================================================> (195 + 5) / 200]                                                                                2026-02-12 11:23:08,288 INFO: Case III Part A Generated | Time Elapsed: 0.23 minutes
2026-02-12 11:23:08,289 INFO: ------------------------------------------------------------
2026-02-12 11:23:08,289 INFO: Part B: Updating future summary rows...
[Stage 57:==================>                                    (69 + 6) / 200][Stage 57:===========================>                          (101 + 6) / 200][Stage 57:===================================>                  (131 + 6) / 200][Stage 57:========================================>             (151 + 6) / 200][Stage 57:================================================>     (180 + 6) / 200]                                                                                2026-02-12 11:23:11,852 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 73:============> (185 + 7) / 200][Stage 74:>               (0 + 0) / 200]                                                                                [Stage 74:========================>                              (88 + 6) / 200][Stage 74:==================================>                   (129 + 6) / 200][Stage 74:=================================================>    (182 + 8) / 200]                                                                                [Stage 80:=================>                                     (62 + 6) / 200][Stage 80:=====================>                                 (77 + 6) / 200][Stage 80:==========================>                            (98 + 6) / 200][Stage 80:=================================>                    (125 + 6) / 200][Stage 80:========================================>             (151 + 6) / 200][Stage 80:==============================================>       (173 + 6) / 200]                                                                                2026-02-12 11:23:16,587 INFO: Case III Part B Generated | Time Elapsed: 0.14 minutes
2026-02-12 11:23:16,590 INFO: ------------------------------------------------------------
2026-02-12 11:23:16,640 INFO: ------------------------------------------------------------
2026-02-12 11:23:16,641 INFO: MERGING RECORDS:
2026-02-12 11:23:18,708 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.03 minutes
2026-02-12 11:23:18,709 INFO: ------------------------------------------------------------
2026-02-12 11:23:20,657 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.03 minutes
2026-02-12 11:23:20,658 INFO: ------------------------------------------------------------
2026-02-12 11:23:20,658 INFO: APPENDING RECORDS:
2026-02-12 11:23:20,672 INFO: No Case I/IV records to append | Time Elapsed: 0.00 minutes
2026-02-12 11:23:20,673 INFO: ------------------------------------------------------------
2026-02-12 11:23:20,691 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] backfill
[PASS] comprehensive_edge_cases
2026-02-12 11:23:21,846 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:23:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:23:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:23:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=============================>                             (3 + 3) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6][Stage 3:=================================================>         (5 + 1) / 6]                                                                                [Stage 7:=========>                                                 (1 + 5) / 6]                                                                                2026-02-12 11:23:46,771 INFO: Cleaned case_1
2026-02-12 11:23:46,794 INFO: Cleaned case_2
2026-02-12 11:23:46,835 INFO: Cleaned case_3a
2026-02-12 11:23:46,877 INFO: Cleaned case_3b
2026-02-12 11:23:46,915 INFO: Cleaned case_4
2026-02-12 11:23:46,915 INFO: CLEANUP COMPLETED
2026-02-12 11:23:46,916 INFO: ============================================================
2026-02-12 11:23:46,917 INFO: ================================================================================
2026-02-12 11:23:46,918 INFO: SUMMARY PIPELINE - START
2026-02-12 11:23:46,919 INFO: ================================================================================
2026-02-12 11:23:46,919 INFO: ================================================================================
2026-02-12 11:23:46,920 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:23:46,921 INFO: ================================================================================
2026-02-12 11:23:50,326 INFO: Loading accounts from primary_catalog.main_backfill.accounts_all
2026-02-12 11:23:50,535 INFO: Reading from primary_catalog.main_backfill.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:23:50,536 INFO: Preparing source data...
2026-02-12 11:23:50,768 INFO: Applied 36 column mappings
2026-02-12 11:23:51,192 INFO: Applied 7 column transformations
2026-02-12 11:23:51,416 INFO: Created 2 inferred columns
2026-02-12 11:23:52,814 INFO: Validated 7 date columns
2026-02-12 11:23:53,160 INFO: Source data preparation complete
2026-02-12 11:23:53,535 INFO: Loading summary metadata from primary_catalog.main_backfill.latest_summary
2026-02-12 11:23:53,635 INFO: Loaded metadata for existing accounts
2026-02-12 11:23:53,635 INFO: Classifying accounts into Case I/II/III/IV
[Stage 26:>                                                         (0 + 1) / 1][Stage 32:====>                                                  (16 + 8) / 200][Stage 32:=========>                                             (34 + 6) / 200][Stage 32:==============>                                        (52 + 7) / 200][Stage 32:====================>                                  (75 + 7) / 200][Stage 32:=========================>                             (92 + 6) / 200][Stage 32:===========================>                          (103 + 6) / 200][Stage 32:===============================>                      (116 + 6) / 200][Stage 32:===================================>                  (132 + 6) / 200][Stage 32:=======================================>              (147 + 6) / 200][Stage 32:============================================>         (166 + 6) / 200][Stage 32:=================================================>    (183 + 8) / 200]                                                                                2026-02-12 11:24:03,726 INFO: ------------------------------------------------------------
2026-02-12 11:24:03,727 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:24:03,727 INFO:   CASE_III: 1 records
2026-02-12 11:24:03,728 INFO: ------------------------------------------------------------
2026-02-12 11:24:03,728 INFO: Classification | Time Elapsed: 0.28 minutes
2026-02-12 11:24:03,764 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:24:03,764 INFO: ================================================================================
2026-02-12 11:24:03,765 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:24:03,765 INFO: ================================================================================
2026-02-12 11:24:03,766 INFO: CASE III - Processing backfill records
[Stage 39:===========>                                           (43 + 6) / 200][Stage 39:==================>                                    (66 + 6) / 200][Stage 39:=======================>                               (86 + 7) / 200][Stage 39:===========================>                          (103 + 7) / 200][Stage 39:================================>                     (122 + 6) / 200][Stage 39:====================================>                 (136 + 8) / 200][Stage 39:==========================================>           (156 + 7) / 200][Stage 39:=================================================>    (182 + 6) / 200][Stage 39:=====================================================>(199 + 1) / 200]                                                                                2026-02-12 11:24:06,474 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:24:06,475 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:24:06,507 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:24:06,852 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:24:08,228 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 46:===============>                                       (58 + 6) / 200][Stage 46:====================>                                  (75 + 6) / 200][Stage 46:============================>                         (104 + 6) / 200][Stage 46:===================================>                  (130 + 8) / 200][Stage 46:========================================>             (149 + 6) / 200][Stage 46:==============================================>       (174 + 6) / 200]                                                                                [Stage 50:>                                                       (0 + 6) / 200][Stage 50:=>                                                      (6 + 6) / 200][Stage 50:==>                                                     (9 + 6) / 200][Stage 50:===>                                                   (13 + 7) / 200][Stage 50:====>                                                  (16 + 6) / 200][Stage 50:=====>                                                 (21 + 6) / 200][Stage 50:======>                                                (25 + 6) / 200][Stage 50:========>                                              (30 + 6) / 200][Stage 50:=========>                                             (34 + 6) / 200][Stage 50:==========>                                            (39 + 6) / 200][Stage 50:============>                                          (45 + 6) / 200][Stage 50:=============>                                         (48 + 6) / 200][Stage 50:==============>                                        (52 + 6) / 200][Stage 50:===============>                                       (57 + 6) / 200][Stage 50:================>                                      (61 + 6) / 200][Stage 50:=================>                                     (65 + 6) / 200][Stage 50:===================>                                   (70 + 6) / 200][Stage 50:====================>                                  (74 + 6) / 200][Stage 50:=====================>                                 (77 + 6) / 200][Stage 50:======================>                                (82 + 6) / 200][Stage 50:=======================>                               (87 + 6) / 200][Stage 50:=========================>                             (93 + 6) / 200][Stage 50:==========================>                            (96 + 6) / 200][Stage 50:===========================>                          (101 + 6) / 200][Stage 50:============================>                         (107 + 6) / 200][Stage 50:==============================>                       (112 + 6) / 200][Stage 50:===============================>                      (116 + 6) / 200][Stage 50:================================>                     (122 + 6) / 200][Stage 50:==================================>                   (126 + 6) / 200][Stage 50:===================================>                  (130 + 6) / 200][Stage 50:===================================>                  (132 + 6) / 200][Stage 50:====================================>                 (137 + 6) / 200][Stage 50:======================================>               (143 + 6) / 200][Stage 50:========================================>             (149 + 6) / 200][Stage 50:=========================================>            (154 + 6) / 200][Stage 50:==========================================>           (159 + 6) / 200][Stage 50:============================================>         (164 + 6) / 200][Stage 50:=============================================>        (169 + 6) / 200][Stage 50:==============================================>       (174 + 6) / 200][Stage 50:================================================>     (180 + 6) / 200][Stage 50:=================================================>    (185 + 6) / 200][Stage 50:==================================================>   (187 + 7) / 200][Stage 50:===================================================>  (192 + 6) / 200][Stage 50:====================================================> (195 + 5) / 200][Stage 54:===================================================>    (46 + 4) / 50]                                                                                2026-02-12 11:24:24,522 INFO: Case III Part A Generated | Time Elapsed: 0.35 minutes
2026-02-12 11:24:24,523 INFO: ------------------------------------------------------------
2026-02-12 11:24:24,523 INFO: Part B: Updating future summary rows...
[Stage 57:========>                                              (32 + 7) / 200][Stage 57:=================>                                     (65 + 7) / 200]                                                                                2026-02-12 11:24:25,514 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 80:==============>                                        (52 + 6) / 200][Stage 80:==================>                                    (67 + 6) / 200][Stage 80:=======================>                               (86 + 6) / 200][Stage 80:===========================>                          (103 + 6) / 200][Stage 80:================================>                     (119 + 9) / 200][Stage 80:====================================>                 (136 + 6) / 200][Stage 80:=======================================>              (148 + 6) / 200][Stage 80:===========================================>          (160 + 6) / 200][Stage 80:===============================================>      (176 + 6) / 200][Stage 80:==================================================>   (188 + 6) / 200]                                                                                2026-02-12 11:24:32,098 INFO: Case III Part B Generated | Time Elapsed: 0.13 minutes
2026-02-12 11:24:32,099 INFO: ------------------------------------------------------------
2026-02-12 11:24:32,146 INFO: ------------------------------------------------------------
2026-02-12 11:24:32,147 INFO: MERGING RECORDS:
2026-02-12 11:24:34,507 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.04 minutes
2026-02-12 11:24:34,508 INFO: ------------------------------------------------------------
2026-02-12 11:24:36,673 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.04 minutes
2026-02-12 11:24:36,674 INFO: ------------------------------------------------------------
2026-02-12 11:24:36,674 INFO: APPENDING RECORDS:
2026-02-12 11:24:36,691 INFO: No Case I/IV records to append | Time Elapsed: 0.00 minutes
2026-02-12 11:24:36,692 INFO: ------------------------------------------------------------
2026-02-12 11:24:36,723 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] backfill
2026-02-12 11:24:37,987 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:24:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:24:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:24:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6][Stage 3:=================================================>         (5 + 1) / 6]                                                                                [Stage 4:>                                                          (0 + 6) / 6][Stage 4:===================>                                       (2 + 4) / 6]                                                                                2026-02-12 11:25:04,609 INFO: Cleaned case_1
2026-02-12 11:25:04,650 INFO: Cleaned case_2
2026-02-12 11:25:04,695 INFO: Cleaned case_3a
2026-02-12 11:25:04,726 INFO: Cleaned case_3b
2026-02-12 11:25:04,767 INFO: Cleaned case_4
2026-02-12 11:25:04,768 INFO: CLEANUP COMPLETED
2026-02-12 11:25:04,769 INFO: ============================================================
2026-02-12 11:25:04,770 INFO: ================================================================================
2026-02-12 11:25:04,771 INFO: SUMMARY PIPELINE - START
2026-02-12 11:25:04,771 INFO: ================================================================================
2026-02-12 11:25:04,772 INFO: ================================================================================
2026-02-12 11:25:04,773 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:25:04,773 INFO: ================================================================================
2026-02-12 11:25:08,552 INFO: Loading accounts from primary_catalog.main_duplicates.accounts_all
2026-02-12 11:25:08,782 INFO: Reading from primary_catalog.main_duplicates.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2020-01-02 00:00:00
2026-02-12 11:25:08,782 INFO: Preparing source data...
2026-02-12 11:25:09,038 INFO: Applied 36 column mappings
2026-02-12 11:25:09,931 INFO: Applied 7 column transformations
2026-02-12 11:25:10,462 INFO: Created 2 inferred columns
2026-02-12 11:25:12,128 INFO: Validated 7 date columns
2026-02-12 11:25:12,348 INFO: Source data preparation complete
2026-02-12 11:25:12,810 INFO: Loading summary metadata from primary_catalog.main_duplicates.latest_summary
2026-02-12 11:25:12,887 INFO: Loaded metadata for existing accounts
2026-02-12 11:25:12,888 INFO: Classifying accounts into Case I/II/III/IV
[Stage 25:======>                                                (23 + 6) / 200][Stage 25:=========>                                             (33 + 6) / 200][Stage 25:============>                                          (47 + 6) / 200][Stage 25:================>                                      (61 + 6) / 200][Stage 25:====================>                                  (73 + 7) / 200][Stage 25:========================>                              (89 + 6) / 200][Stage 25:===========================>                          (102 + 6) / 200][Stage 25:==============================>                       (114 + 6) / 200][Stage 25:=================================>                    (123 + 8) / 200][Stage 25:=====================================>                (138 + 7) / 200][Stage 25:=========================================>            (153 + 6) / 200][Stage 25:=============================================>        (169 + 6) / 200][Stage 25:=================================================>    (184 + 6) / 200][Stage 25:=====================================================>(199 + 1) / 200][Stage 28:>                                                       (0 + 6) / 200]                                                                                2026-02-12 11:25:19,825 INFO: ------------------------------------------------------------
2026-02-12 11:25:19,826 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:25:19,827 INFO:   CASE_I: 1 records
2026-02-12 11:25:19,827 INFO: ------------------------------------------------------------
2026-02-12 11:25:19,828 INFO: Classification | Time Elapsed: 0.25 minutes
2026-02-12 11:25:19,895 INFO: 
>>> PROCESSING NEW ACCOUNTS (1 records)
2026-02-12 11:25:19,895 INFO: ================================================================================
2026-02-12 11:25:19,896 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:25:19,896 INFO: ================================================================================
2026-02-12 11:25:19,897 INFO: Processing new accounts
2026-02-12 11:25:20,653 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 35:===============================>                      (118 + 6) / 200][Stage 35:====================================>                 (134 + 7) / 200][Stage 35:========================================>             (151 + 7) / 200][Stage 35:==============================================>       (173 + 6) / 200][Stage 35:===================================================>  (190 + 6) / 200][Stage 38:==========================>                            (97 + 7) / 200][Stage 38:====================================>                 (134 + 7) / 200][Stage 38:==============================================>       (173 + 8) / 200][Stage 42:=========================================>              (37 + 6) / 50]                                                                                2026-02-12 11:25:26,080 INFO: Case I Generated | Time Elapsed: 0.10 minutes
2026-02-12 11:25:26,081 INFO: ------------------------------------------------------------
2026-02-12 11:25:26,114 INFO: ------------------------------------------------------------
2026-02-12 11:25:26,115 INFO: MERGING RECORDS:
2026-02-12 11:25:26,121 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:25:26,122 INFO: ------------------------------------------------------------
2026-02-12 11:25:26,129 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:25:26,130 INFO: ------------------------------------------------------------
2026-02-12 11:25:26,130 INFO: APPENDING RECORDS:
2026-02-12 11:25:26,162 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:25:26,952 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.01 minutes
2026-02-12 11:25:26,953 INFO: ------------------------------------------------------------
2026-02-12 11:25:26,953 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:25:28,675 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.01 minutes
2026-02-12 11:25:28,676 INFO: ------------------------------------------------------------
2026-02-12 11:25:28,709 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] duplicate_records
2026-02-12 11:25:30,104 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:25:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:25:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:25:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=======================================>                   (4 + 2) / 6]                                                                                2026-02-12 11:25:50,693 INFO: Cleaned case_1
2026-02-12 11:25:50,719 INFO: Cleaned case_2
2026-02-12 11:25:50,748 INFO: Cleaned case_3a
2026-02-12 11:25:50,774 INFO: Cleaned case_3b
2026-02-12 11:25:50,799 INFO: Cleaned case_4
2026-02-12 11:25:50,800 INFO: CLEANUP COMPLETED
2026-02-12 11:25:50,800 INFO: ============================================================
2026-02-12 11:25:50,801 INFO: ================================================================================
2026-02-12 11:25:50,802 INFO: SUMMARY PIPELINE - START
2026-02-12 11:25:50,802 INFO: ================================================================================
2026-02-12 11:25:50,803 INFO: ================================================================================
2026-02-12 11:25:50,804 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:25:50,804 INFO: ================================================================================
2026-02-12 11:25:53,786 INFO: Loading accounts from primary_catalog.main_full46.accounts_all
2026-02-12 11:25:53,981 INFO: Reading from primary_catalog.main_full46.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2020-01-02 00:00:00
2026-02-12 11:25:53,982 INFO: Preparing source data...
2026-02-12 11:25:54,173 INFO: Applied 36 column mappings
2026-02-12 11:25:54,662 INFO: Applied 7 column transformations
2026-02-12 11:25:54,847 INFO: Created 2 inferred columns
2026-02-12 11:25:55,608 INFO: Validated 7 date columns
2026-02-12 11:25:55,787 INFO: Source data preparation complete
2026-02-12 11:25:55,954 INFO: Loading summary metadata from primary_catalog.main_full46.latest_summary
2026-02-12 11:25:56,002 INFO: Loaded metadata for existing accounts
2026-02-12 11:25:56,003 INFO: Classifying accounts into Case I/II/III/IV
[Stage 25:==>                                                     (9 + 6) / 200][Stage 25:===>                                                   (14 + 6) / 200][Stage 25:=====>                                                 (20 + 6) / 200][Stage 25:========>                                              (30 + 6) / 200][Stage 25:===========>                                           (41 + 6) / 200][Stage 25:=============>                                         (49 + 6) / 200][Stage 25:================>                                      (60 + 6) / 200][Stage 25:==================>                                    (69 + 6) / 200][Stage 25:======================>                                (81 + 6) / 200][Stage 25:=========================>                             (94 + 6) / 200][Stage 25:============================>                         (105 + 6) / 200][Stage 25:=============================>                        (111 + 7) / 200][Stage 25:================================>                     (122 + 7) / 200][Stage 25:==================================>                   (129 + 6) / 200][Stage 25:=====================================>                (140 + 6) / 200][Stage 25:========================================>             (149 + 6) / 200][Stage 25:============================================>         (163 + 6) / 200][Stage 25:==============================================>       (171 + 6) / 200][Stage 25:================================================>     (179 + 7) / 200][Stage 25:==================================================>   (188 + 6) / 200][Stage 28:====>                                                  (16 + 6) / 200][Stage 28:======>                                                (24 + 8) / 200][Stage 28:==========>                                            (37 + 7) / 200][Stage 28:=============>                                         (48 + 6) / 200][Stage 28:=================>                                     (63 + 7) / 200][Stage 28:=====================>                                 (78 + 6) / 200][Stage 28:=========================>                             (94 + 6) / 200][Stage 28:=============================>                        (110 + 6) / 200][Stage 28:================================>                     (122 + 6) / 200][Stage 28:=====================================>                (138 + 7) / 200][Stage 28:=========================================>            (155 + 7) / 200][Stage 28:==============================================>       (174 + 8) / 200][Stage 28:====================================================> (194 + 6) / 200]                                                                                2026-02-12 11:26:07,469 INFO: ------------------------------------------------------------
2026-02-12 11:26:07,470 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:26:07,471 INFO:   CASE_I: 1 records
2026-02-12 11:26:07,472 INFO: ------------------------------------------------------------
2026-02-12 11:26:07,473 INFO: Classification | Time Elapsed: 0.28 minutes
2026-02-12 11:26:07,558 INFO: 
>>> PROCESSING NEW ACCOUNTS (1 records)
2026-02-12 11:26:07,559 INFO: ================================================================================
2026-02-12 11:26:07,560 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:26:07,560 INFO: ================================================================================
2026-02-12 11:26:07,561 INFO: Processing new accounts
2026-02-12 11:26:08,297 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 35:====>                                                  (15 + 6) / 200][Stage 35:=======>                                               (27 + 6) / 200][Stage 35:==========>                                            (38 + 6) / 200][Stage 35:==============>                                        (52 + 8) / 200][Stage 35:==================>                                    (69 + 5) / 200][Stage 35:=======================>                               (84 + 6) / 200][Stage 35:===========================>                           (99 + 6) / 200][Stage 35:=============================>                        (110 + 6) / 200][Stage 35:===============================>                      (117 + 6) / 200][Stage 35:====================================>                 (135 + 6) / 200][Stage 35:=========================================>            (153 + 6) / 200][Stage 35:=============================================>        (169 + 7) / 200][Stage 35:==================================================>   (186 + 6) / 200][Stage 38:==================>                                    (68 + 6) / 200][Stage 38:============================>                         (104 + 6) / 200][Stage 38:===================================>                  (130 + 6) / 200][Stage 38:========================================>             (150 + 6) / 200][Stage 38:==============================================>       (174 + 6) / 200][Stage 42:===========================================>            (39 + 6) / 50]                                                                                2026-02-12 11:26:17,513 INFO: Case I Generated | Time Elapsed: 0.17 minutes
2026-02-12 11:26:17,514 INFO: ------------------------------------------------------------
2026-02-12 11:26:12,082 INFO: ------------------------------------------------------------
2026-02-12 11:26:12,082 INFO: MERGING RECORDS:
2026-02-12 11:26:12,092 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:26:12,093 INFO: ------------------------------------------------------------
2026-02-12 11:26:12,102 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:26:12,103 INFO: ------------------------------------------------------------
2026-02-12 11:26:12,103 INFO: APPENDING RECORDS:
2026-02-12 11:26:12,146 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:26:13,138 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:26:13,142 INFO: ------------------------------------------------------------
2026-02-12 11:26:13,142 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:26:14,852 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:26:14,853 INFO: ------------------------------------------------------------
2026-02-12 11:26:14,880 INFO: PROCESS COMPLETED - Deleting the persisted classification results
26/02/12 11:26:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$9(DataSourceV2Strategy.scala:272)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)
	org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
26/02/12 11:26:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$9(DataSourceV2Strategy.scala:272)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)
	org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[PASS] full_46_columns
2026-02-12 11:26:15,907 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:26:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:26:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:26:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=========>                                                 (1 + 5) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6]                                                                                [Stage 7:=======================================>                   (4 + 2) / 6]                                                                                [Stage 8:=============================>                             (3 + 3) / 6]                                                                                2026-02-12 11:26:40,378 INFO: Cleaned case_1
2026-02-12 11:26:40,400 INFO: Cleaned case_2
2026-02-12 11:26:40,427 INFO: Cleaned case_3a
2026-02-12 11:26:40,457 INFO: Cleaned case_3b
2026-02-12 11:26:40,484 INFO: Cleaned case_4
2026-02-12 11:26:40,485 INFO: CLEANUP COMPLETED
2026-02-12 11:26:40,486 INFO: ============================================================
2026-02-12 11:26:40,486 INFO: ================================================================================
2026-02-12 11:26:40,487 INFO: SUMMARY PIPELINE - START
2026-02-12 11:26:40,488 INFO: ================================================================================
2026-02-12 11:26:40,488 INFO: ================================================================================
2026-02-12 11:26:40,489 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:26:40,489 INFO: ================================================================================
[Stage 14:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:26:40,788 INFO: Loading accounts from primary_catalog.main_backfill.accounts_all
2026-02-12 11:26:40,981 INFO: Reading from primary_catalog.main_backfill.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:26:40,982 INFO: Preparing source data...
2026-02-12 11:26:41,210 INFO: Applied 36 column mappings
2026-02-12 11:26:41,761 INFO: Applied 7 column transformations
2026-02-12 11:26:41,916 INFO: Created 2 inferred columns
2026-02-12 11:26:42,256 INFO: Validated 7 date columns
2026-02-12 11:26:42,410 INFO: Source data preparation complete
2026-02-12 11:26:42,558 INFO: Loading summary metadata from primary_catalog.main_backfill.latest_summary
2026-02-12 11:26:42,616 INFO: Loaded metadata for existing accounts
2026-02-12 11:26:42,617 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:===>                                                   (14 + 7) / 200][Stage 29:======>                                                (24 + 6) / 200][Stage 29:========>                                              (30 + 6) / 200][Stage 29:=========>                                             (35 + 6) / 200][Stage 29:============>                                          (46 + 6) / 200][Stage 29:================>                                      (59 + 6) / 200][Stage 29:===================>                                   (72 + 6) / 200][Stage 29:======================>                                (83 + 6) / 200][Stage 29:=========================>                             (92 + 6) / 200][Stage 29:===========================>                          (100 + 6) / 200][Stage 29:=============================>                        (111 + 6) / 200][Stage 29:================================>                     (121 + 6) / 200][Stage 29:===================================>                  (130 + 6) / 200][Stage 29:=====================================>                (138 + 6) / 200][Stage 29:=====================================>                (140 + 6) / 200][Stage 29:========================================>             (151 + 6) / 200][Stage 29:============================================>         (166 + 6) / 200][Stage 29:===============================================>      (177 + 6) / 200][Stage 29:==================================================>   (188 + 6) / 200][Stage 32:===>                                                   (14 + 9) / 200][Stage 32:=======>                                               (27 + 6) / 200][Stage 32:===========>                                           (42 + 6) / 200][Stage 32:===============>                                       (56 + 7) / 200][Stage 32:===================>                                   (70 + 7) / 200][Stage 32:=======================>                               (86 + 6) / 200][Stage 32:===========================>                          (101 + 6) / 200][Stage 32:===============================>                      (116 + 6) / 200][Stage 32:===================================>                  (133 + 6) / 200][Stage 32:=======================================>              (148 + 7) / 200][Stage 32:=============================================>        (167 + 6) / 200][Stage 32:================================================>     (178 + 6) / 200][Stage 32:=====================================================>(197 + 3) / 200]                                                                                2026-02-12 11:26:53,967 INFO: ------------------------------------------------------------
2026-02-12 11:26:53,967 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:26:53,968 INFO:   CASE_III: 1 records
2026-02-12 11:26:53,968 INFO: ------------------------------------------------------------
2026-02-12 11:26:53,969 INFO: Classification | Time Elapsed: 0.22 minutes
2026-02-12 11:26:53,989 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:26:53,990 INFO: ================================================================================
2026-02-12 11:26:53,991 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:26:53,991 INFO: ================================================================================
2026-02-12 11:26:53,992 INFO: CASE III - Processing backfill records
[Stage 39:===============>                                       (58 + 6) / 200][Stage 39:=====================>                                 (79 + 6) / 200][Stage 39:============================>                         (105 + 6) / 200][Stage 39:================================>                     (122 + 6) / 200][Stage 39:=====================================>                (140 + 7) / 200][Stage 39:==========================================>           (157 + 6) / 200][Stage 39:================================================>     (180 + 6) / 200]                                                                                2026-02-12 11:26:56,533 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:26:56,533 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:26:56,552 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:26:56,882 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:26:58,012 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 46:====================>                                  (74 + 6) / 200][Stage 46:============================>                         (104 + 6) / 200][Stage 46:===================================>                  (130 + 6) / 200][Stage 46:=========================================>            (152 + 7) / 200][Stage 46:=============================================>        (169 + 9) / 200][Stage 46:==================================================>   (188 + 6) / 200]                                                                                [Stage 50:>                                                       (0 + 6) / 200][Stage 50:>                                                       (1 + 7) / 200][Stage 50:=>                                                      (7 + 6) / 200][Stage 50:===>                                                   (12 + 6) / 200][Stage 50:====>                                                  (17 + 6) / 200][Stage 50:=====>                                                 (20 + 6) / 200][Stage 50:======>                                                (25 + 6) / 200][Stage 50:=======>                                               (28 + 6) / 200][Stage 50:=========>                                             (34 + 7) / 200][Stage 50:==========>                                            (38 + 6) / 200][Stage 50:===========>                                           (43 + 6) / 200][Stage 50:=============>                                         (49 + 6) / 200][Stage 50:===============>                                       (55 + 6) / 200][Stage 50:===============>                                       (57 + 6) / 200][Stage 50:=================>                                     (62 + 6) / 200][Stage 50:=================>                                     (63 + 6) / 200][Stage 50:==================>                                    (68 + 6) / 200][Stage 50:====================>                                  (74 + 6) / 200][Stage 50:=====================>                                 (78 + 6) / 200][Stage 50:======================>                                (81 + 6) / 200][Stage 50:=======================>                               (87 + 6) / 200][Stage 50:========================>                              (90 + 6) / 200][Stage 50:==========================>                            (95 + 6) / 200][Stage 50:===========================>                          (100 + 6) / 200][Stage 50:============================>                         (105 + 6) / 200]                                                                                2026-02-12 11:27:11,447 INFO: Case III Part A Generated | Time Elapsed: 0.29 minutes
2026-02-12 11:27:11,448 INFO: ------------------------------------------------------------
2026-02-12 11:27:11,449 INFO: Part B: Updating future summary rows...
[Stage 57:================================>                     (122 + 7) / 200][Stage 57:=========================================>            (154 + 7) / 200][Stage 57:=====================================================>(199 + 1) / 200]                                                                                [Stage 70:====================================================>  (95 + 5) / 100]                                                                                2026-02-12 11:27:14,969 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 73:=======>      (102 + 6) / 200][Stage 74:>               (0 + 0) / 200][Stage 73:==========>   (145 + 6) / 200][Stage 74:>               (0 + 0) / 200][Stage 73:============> (180 + 6) / 200][Stage 74:>               (0 + 0) / 200]                                                                                [Stage 74:==========================>                            (97 + 8) / 200][Stage 74:=======================================>              (147 + 6) / 200][Stage 74:===================================================>  (190 + 7) / 200]                                                                                [Stage 80:==============>                                        (51 + 6) / 200][Stage 80:==================>                                    (69 + 6) / 200][Stage 80:========================>                              (88 + 6) / 200][Stage 80:==========================>                            (98 + 6) / 200][Stage 80:===============================>                      (117 + 6) / 200][Stage 80:====================================>                 (134 + 6) / 200][Stage 80:========================================>             (149 + 6) / 200][Stage 80:=============================================>        (169 + 6) / 200][Stage 80:==================================================>   (186 + 6) / 200]                                                                                2026-02-12 11:27:21,257 INFO: Case III Part B Generated | Time Elapsed: 0.16 minutes
2026-02-12 11:27:21,258 INFO: ------------------------------------------------------------
2026-02-12 11:27:21,301 INFO: ------------------------------------------------------------
2026-02-12 11:27:21,302 INFO: MERGING RECORDS:
2026-02-12 11:27:23,684 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.04 minutes
2026-02-12 11:27:23,690 INFO: ------------------------------------------------------------
2026-02-12 11:27:25,702 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.03 minutes
2026-02-12 11:27:25,703 INFO: ------------------------------------------------------------
2026-02-12 11:27:25,706 INFO: APPENDING RECORDS:
2026-02-12 11:27:25,723 INFO: No Case I/IV records to append | Time Elapsed: 0.00 minutes
2026-02-12 11:27:25,723 INFO: ------------------------------------------------------------
2026-02-12 11:27:25,739 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] backfill
2026-02-12 11:27:26,605 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:27:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:27:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:27:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=========>                                                 (1 + 5) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6]                                                                                [Stage 7:=======================================>                   (4 + 2) / 6]                                                                                [Stage 8:=========>                                                 (1 + 5) / 6]                                                                                2026-02-12 11:27:48,334 INFO: Cleaned case_1
2026-02-12 11:27:48,359 INFO: Cleaned case_2
2026-02-12 11:27:48,385 INFO: Cleaned case_3a
2026-02-12 11:27:48,409 INFO: Cleaned case_3b
2026-02-12 11:27:48,434 INFO: Cleaned case_4
2026-02-12 11:27:48,435 INFO: CLEANUP COMPLETED
2026-02-12 11:27:48,436 INFO: ============================================================
2026-02-12 11:27:48,436 INFO: ================================================================================
2026-02-12 11:27:48,437 INFO: SUMMARY PIPELINE - START
2026-02-12 11:27:48,437 INFO: ================================================================================
2026-02-12 11:27:48,438 INFO: ================================================================================
2026-02-12 11:27:48,439 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:27:48,439 INFO: ================================================================================
[Stage 14:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:27:51,395 INFO: Loading accounts from primary_catalog.main_backfill.accounts_all
2026-02-12 11:27:51,583 INFO: Reading from primary_catalog.main_backfill.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:27:51,583 INFO: Preparing source data...
2026-02-12 11:27:51,743 INFO: Applied 36 column mappings
2026-02-12 11:27:52,300 INFO: Applied 7 column transformations
2026-02-12 11:27:52,484 INFO: Created 2 inferred columns
2026-02-12 11:27:53,222 INFO: Validated 7 date columns
2026-02-12 11:27:53,384 INFO: Source data preparation complete
2026-02-12 11:27:53,507 INFO: Loading summary metadata from primary_catalog.main_backfill.latest_summary
2026-02-12 11:27:53,567 INFO: Loaded metadata for existing accounts
2026-02-12 11:27:53,570 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:===>                                                   (12 + 6) / 200][Stage 29:======>                                                (24 + 6) / 200][Stage 29:=========>                                             (34 + 6) / 200][Stage 29:===========>                                           (43 + 6) / 200][Stage 29:==============>                                        (54 + 6) / 200][Stage 29:==================>                                    (67 + 6) / 200][Stage 29:=====================>                                 (79 + 6) / 200][Stage 29:=========================>                             (93 + 7) / 200][Stage 29:===========================>                          (103 + 7) / 200][Stage 29:===============================>                      (117 + 6) / 200][Stage 29:==================================>                   (127 + 6) / 200][Stage 29:=====================================>                (138 + 6) / 200][Stage 29:========================================>             (149 + 7) / 200][Stage 29:============================================>         (163 + 6) / 200][Stage 29:================================================>     (178 + 6) / 200][Stage 29:===================================================>  (189 + 6) / 200][Stage 32:==>                                                    (10 + 6) / 200][Stage 32:======>                                                (23 + 6) / 200][Stage 32:============>                                          (45 + 6) / 200][Stage 32:=================>                                     (62 + 8) / 200][Stage 32:=====================>                                 (79 + 6) / 200][Stage 32:==========================>                            (95 + 6) / 200][Stage 32:=============================>                        (111 + 6) / 200][Stage 32:=================================>                    (124 + 7) / 200][Stage 32:====================================>                 (135 + 6) / 200][Stage 32:======================================>               (144 + 6) / 200][Stage 32:==========================================>           (157 + 9) / 200][Stage 32:==============================================>       (172 + 6) / 200][Stage 32:===================================================>  (189 + 7) / 200]                                                                                2026-02-12 11:28:01,356 INFO: ------------------------------------------------------------
2026-02-12 11:28:01,357 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:28:01,358 INFO:   CASE_III: 1 records
2026-02-12 11:28:01,358 INFO: ------------------------------------------------------------
2026-02-12 11:28:01,359 INFO: Classification | Time Elapsed: 0.22 minutes
2026-02-12 11:28:01,401 INFO: 
>>> PROCESSING BACKFILL (1 records)
2026-02-12 11:28:01,402 INFO: ================================================================================
2026-02-12 11:28:01,403 INFO: STEP 2c: Process Case III (Backfill)
2026-02-12 11:28:01,404 INFO: ================================================================================
2026-02-12 11:28:01,404 INFO: CASE III - Processing backfill records
2026-02-12 11:28:03,912 INFO: Backfill month range: 2025-12 to 2025-12
2026-02-12 11:28:03,913 INFO: Reading summary partitions from 2022-12 to 2028-12
2026-02-12 11:28:03,933 INFO: Applied partition filter: rpt_as_of_mo BETWEEN '2022-12' AND '2028-12'
2026-02-12 11:28:04,332 INFO: Part A: Creating new summary rows for backfill months...
2026-02-12 11:28:05,670 INFO: Write partitions [case_3a_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 46:==============>                                        (51 + 7) / 200][Stage 46:================>                                     (60 + 10) / 200][Stage 46:=====================>                                 (78 + 6) / 200][Stage 46:==========================>                            (98 + 6) / 200][Stage 46:===============================>                      (116 + 7) / 200][Stage 46:====================================>                 (137 + 7) / 200][Stage 46:=============================================>        (168 + 6) / 200][Stage 46:===================================================>  (192 + 7) / 200]                                                                                [Stage 50:>                                                       (0 + 6) / 200][Stage 50:=>                                                      (6 + 6) / 200][Stage 50:==>                                                     (8 + 6) / 200][Stage 50:===>                                                   (12 + 6) / 200][Stage 50:===>                                                   (14 + 6) / 200][Stage 50:====>                                                  (18 + 6) / 200][Stage 50:======>                                                (23 + 6) / 200][Stage 50:=======>                                               (26 + 6) / 200][Stage 50:========>                                              (30 + 6) / 200][Stage 50:=========>                                             (35 + 6) / 200][Stage 50:===========>                                           (41 + 6) / 200][Stage 50:===========>                                           (43 + 6) / 200][Stage 50:=============>                                         (48 + 6) / 200][Stage 50:==============>                                        (51 + 6) / 200][Stage 50:===============>                                       (56 + 6) / 200][Stage 50:================>                                      (59 + 6) / 200][Stage 50:=================>                                     (64 + 7) / 200][Stage 50:===================>                                   (70 + 6) / 200][Stage 50:====================>                                  (74 + 6) / 200][Stage 50:=====================>                                 (79 + 6) / 200][Stage 50:======================>                                (83 + 6) / 200][Stage 50:=======================>                               (87 + 6) / 200][Stage 50:=========================>                             (93 + 6) / 200][Stage 50:==========================>                            (97 + 6) / 200][Stage 50:===========================>                          (103 + 6) / 200][Stage 50:============================>                         (106 + 7) / 200][Stage 50:=============================>                        (111 + 7) / 200][Stage 50:==============================>                       (113 + 6) / 200][Stage 50:===============================>                      (117 + 6) / 200][Stage 50:=================================>                    (123 + 6) / 200][Stage 50:==================================>                   (127 + 6) / 200][Stage 50:===================================>                  (132 + 6) / 200][Stage 50:====================================>                 (137 + 6) / 200][Stage 50:======================================>               (141 + 6) / 200][Stage 50:=======================================>              (146 + 6) / 200][Stage 50:========================================>             (149 + 6) / 200][Stage 50:=========================================>            (153 + 6) / 200][Stage 50:=========================================>            (154 + 6) / 200][Stage 50:==========================================>           (158 + 6) / 200][Stage 50:============================================>         (163 + 6) / 200][Stage 50:============================================>         (166 + 6) / 200][Stage 50:==============================================>       (171 + 7) / 200][Stage 50:================================================>     (178 + 7) / 200][Stage 50:=================================================>    (183 + 6) / 200][Stage 50:==================================================>   (187 + 6) / 200][Stage 50:===================================================>  (192 + 6) / 200][Stage 50:=====================================================>(199 + 1) / 200][Stage 54:=================================>                      (30 + 6) / 50]                                                                                2026-02-12 11:28:23,149 INFO: Case III Part A Generated | Time Elapsed: 0.36 minutes
2026-02-12 11:28:23,149 INFO: ------------------------------------------------------------
2026-02-12 11:28:23,150 INFO: Part B: Updating future summary rows...
[Stage 57:======================>                                (81 + 6) / 200][Stage 57:===========================>                           (99 + 7) / 200][Stage 57:================================>                    (122 + 10) / 200][Stage 57:=======================================>              (145 + 6) / 200][Stage 57:==============================================>       (171 + 8) / 200]                                                                                2026-02-12 11:28:27,234 INFO: Write partitions [case_3b_temp]: 200 (base=200, expected_rows=None, scale=1.0)
[Stage 73:======>        (86 + 5) / 200][Stage 74:>               (0 + 0) / 200][Stage 73:========>     (120 + 6) / 200][Stage 74:>               (0 + 0) / 200][Stage 73:===========>  (162 + 6) / 200][Stage 74:>               (0 + 0) / 200]                                                                                [Stage 74:=============================>                        (110 + 6) / 200][Stage 74:========================================>             (150 + 6) / 200][Stage 74:=================================================>    (185 + 6) / 200]                                                                                2026-02-12 11:28:31,098 INFO: Case III Part B Generated | Time Elapsed: 0.13 minutes
2026-02-12 11:28:31,099 INFO: ------------------------------------------------------------
2026-02-12 11:28:31,171 INFO: ------------------------------------------------------------
2026-02-12 11:28:31,172 INFO: MERGING RECORDS:
2026-02-12 11:28:33,154 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.03 minutes
2026-02-12 11:28:33,154 INFO: ------------------------------------------------------------
2026-02-12 11:28:34,975 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.03 minutes
2026-02-12 11:28:34,976 INFO: ------------------------------------------------------------
2026-02-12 11:28:34,977 INFO: APPENDING RECORDS:
2026-02-12 11:28:34,992 INFO: No Case I/IV records to append | Time Elapsed: 0.00 minutes
2026-02-12 11:28:34,993 INFO: ------------------------------------------------------------
2026-02-12 11:28:35,020 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] backfill
2026-02-12 11:28:35,602 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:28:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:28:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:28:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=========>                                                 (1 + 5) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 4:=================================================>         (5 + 1) / 6]                                                                                2026-02-12 11:28:57,850 INFO: Cleaned case_1
2026-02-12 11:28:57,873 INFO: Cleaned case_2
2026-02-12 11:28:57,896 INFO: Cleaned case_3a
2026-02-12 11:28:57,920 INFO: Cleaned case_3b
2026-02-12 11:28:57,946 INFO: Cleaned case_4
2026-02-12 11:28:57,947 INFO: CLEANUP COMPLETED
2026-02-12 11:28:57,947 INFO: ============================================================
2026-02-12 11:28:57,948 INFO: ================================================================================
2026-02-12 11:28:57,949 INFO: SUMMARY PIPELINE - START
2026-02-12 11:28:57,949 INFO: ================================================================================
2026-02-12 11:28:57,950 INFO: ================================================================================
2026-02-12 11:28:57,950 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:28:57,951 INFO: ================================================================================
2026-02-12 11:29:00,976 INFO: Loading accounts from primary_catalog.main_null.accounts_all
2026-02-12 11:29:01,208 INFO: Reading from primary_catalog.main_null.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:29:01,209 INFO: Preparing source data...
2026-02-12 11:29:01,460 INFO: Applied 36 column mappings
2026-02-12 11:29:02,125 INFO: Applied 7 column transformations
2026-02-12 11:29:02,291 INFO: Created 2 inferred columns
2026-02-12 11:29:02,978 INFO: Validated 7 date columns
2026-02-12 11:29:03,126 INFO: Source data preparation complete
2026-02-12 11:29:03,298 INFO: Loading summary metadata from primary_catalog.main_null.latest_summary
2026-02-12 11:29:03,356 INFO: Loaded metadata for existing accounts
2026-02-12 11:29:03,357 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:====>                                                  (16 + 6) / 200][Stage 29:=======>                                               (29 + 6) / 200][Stage 29:==========>                                            (38 + 6) / 200][Stage 29:=============>                                         (49 + 6) / 200][Stage 29:===============>                                       (58 + 6) / 200][Stage 29:==================>                                    (66 + 6) / 200][Stage 29:====================>                                  (74 + 7) / 200][Stage 29:=======================>                               (87 + 6) / 200][Stage 29:==========================>                            (98 + 7) / 200][Stage 29:==============================>                       (114 + 6) / 200][Stage 29:=================================>                    (125 + 6) / 200][Stage 29:====================================>                 (136 + 6) / 200][Stage 29:========================================>             (151 + 6) / 200][Stage 29:=============================================>        (169 + 6) / 200][Stage 29:================================================>     (179 + 6) / 200][Stage 29:=================================================>    (184 + 6) / 200][Stage 29:====================================================> (194 + 6) / 200][Stage 32:=======>                                               (26 + 6) / 200][Stage 32:===========>                                           (41 + 7) / 200][Stage 32:===============>                                       (58 + 6) / 200][Stage 32:===================>                                   (71 + 6) / 200][Stage 32:======================>                                (83 + 6) / 200][Stage 32:=========================>                             (91 + 6) / 200][Stage 32:============================>                         (104 + 7) / 200][Stage 32:==============================>                       (112 + 7) / 200][Stage 32:==================================>                   (128 + 6) / 200][Stage 32:======================================>               (143 + 8) / 200][Stage 32:===========================================>          (161 + 7) / 200][Stage 32:================================================>     (180 + 7) / 200][Stage 32:=====================================================>(197 + 3) / 200]                                                                                2026-02-12 11:29:13,754 INFO: ------------------------------------------------------------
2026-02-12 11:29:13,754 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:29:13,755 INFO:   CASE_II: 1 records
2026-02-12 11:29:13,755 INFO:   CASE_I: 1 records
2026-02-12 11:29:13,756 INFO: ------------------------------------------------------------
2026-02-12 11:29:13,760 INFO: Classification | Time Elapsed: 0.26 minutes
2026-02-12 11:29:13,808 INFO: 
>>> PROCESSING NEW ACCOUNTS (1 records)
2026-02-12 11:29:13,809 INFO: ================================================================================
2026-02-12 11:29:13,809 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:29:13,810 INFO: ================================================================================
2026-02-12 11:29:13,810 INFO: Processing new accounts
2026-02-12 11:29:14,600 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 39:==========>                                            (39 + 6) / 200][Stage 39:==============>                                        (54 + 6) / 200][Stage 39:==================>                                    (67 + 6) / 200][Stage 39:======================>                                (82 + 6) / 200][Stage 39:==========================>                            (98 + 6) / 200][Stage 39:=============================>                        (111 + 6) / 200][Stage 39:====================================>                 (134 + 6) / 200][Stage 39:=========================================>            (154 + 6) / 200][Stage 39:============================================>         (166 + 6) / 200][Stage 39:================================================>     (180 + 6) / 200][Stage 39:====================================================> (196 + 4) / 200][Stage 42:============================>                         (104 + 6) / 200][Stage 42:==================================>                   (128 + 6) / 200][Stage 42:========================================>             (151 + 7) / 200][Stage 42:===============================================>      (177 + 6) / 200]                                                                                2026-02-12 11:29:20,238 INFO: Case I Generated | Time Elapsed: 0.11 minutes
2026-02-12 11:29:20,239 INFO: ------------------------------------------------------------
2026-02-12 11:29:20,274 INFO: ------------------------------------------------------------
2026-02-12 11:29:20,275 INFO: MERGING RECORDS:
2026-02-12 11:29:20,283 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:29:20,284 INFO: ------------------------------------------------------------
2026-02-12 11:29:20,292 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:29:20,293 INFO: ------------------------------------------------------------
2026-02-12 11:29:20,293 INFO: APPENDING RECORDS:
2026-02-12 11:29:20,327 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:29:21,300 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:29:21,301 INFO: ------------------------------------------------------------
2026-02-12 11:29:21,302 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:29:23,007 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.02 minutes
2026-02-12 11:29:23,007 INFO: ------------------------------------------------------------
2026-02-12 11:29:23,062 INFO: 
>>> PROCESSING FORWARD ENTRIES (1 records)
2026-02-12 11:29:23,063 INFO: ================================================================================
2026-02-12 11:29:23,064 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:29:23,065 INFO: ================================================================================
2026-02-12 11:29:23,065 INFO: Processing forward entries
2026-02-12 11:29:23,120 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:29:24,067 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 62:>(19 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200]                                                                                [Stage 70:>                                                       (0 + 6) / 200][Stage 70:=>                                                      (6 + 6) / 200][Stage 70:===>                                                   (12 + 6) / 200][Stage 70:====>                                                  (18 + 6) / 200][Stage 70:======>                                                (24 + 6) / 200][Stage 70:========>                                              (30 + 7) / 200][Stage 70:=========>                                             (36 + 6) / 200][Stage 70:============>                                          (44 + 6) / 200][Stage 70:==============>                                        (52 + 6) / 200][Stage 70:===============>                                       (58 + 6) / 200][Stage 70:=================>                                     (62 + 6) / 200][Stage 70:=================>                                     (65 + 6) / 200][Stage 70:===================>                                   (72 + 6) / 200][Stage 70:=====================>                                 (79 + 6) / 200][Stage 70:======================>                                (83 + 6) / 200][Stage 70:========================>                              (88 + 6) / 200][Stage 70:=========================>                             (94 + 6) / 200][Stage 70:===========================>                          (100 + 6) / 200][Stage 70:============================>                         (106 + 6) / 200][Stage 70:=============================>                        (111 + 6) / 200][Stage 70:===============================>                      (118 + 6) / 200][Stage 70:==================================>                   (128 + 6) / 200][Stage 70:====================================>                 (137 + 5) / 200][Stage 70:======================================>               (144 + 6) / 200][Stage 70:========================================>             (151 + 6) / 200][Stage 70:==========================================>           (159 + 6) / 200][Stage 70:=============================================>        (167 + 6) / 200][Stage 70:===============================================>      (175 + 6) / 200][Stage 70:=================================================>    (182 + 6) / 200][Stage 70:==================================================>   (188 + 6) / 200][Stage 70:=====================================================>(197 + 3) / 200]                                                                                2026-02-12 11:29:38,358 INFO: Case II Generated | Time Elapsed: 0.25 minutes
2026-02-12 11:29:38,360 INFO: ------------------------------------------------------------
2026-02-12 11:29:38,361 INFO: ------------------------------------------------------------
2026-02-12 11:29:38,362 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:29:38,395 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:29:39,125 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:29:39,126 INFO: ------------------------------------------------------------
2026-02-12 11:29:41,984 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.05 minutes
2026-02-12 11:29:41,989 INFO: ------------------------------------------------------------
2026-02-12 11:29:41,991 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] null_update
2026-02-12 11:29:43,610 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:29:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:29:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:29:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=======================================>                   (4 + 2) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:=========>                                                 (1 + 5) / 6][Stage 3:=================================================>         (5 + 1) / 6]                                                                                [Stage 4:===================>                                       (2 + 4) / 6]                                                                                [Stage 7:>                                                          (0 + 6) / 6]                                                                                2026-02-12 11:30:05,409 INFO: Cleaned case_1
2026-02-12 11:30:05,431 INFO: Cleaned case_2
2026-02-12 11:30:05,459 INFO: Cleaned case_3a
2026-02-12 11:30:05,488 INFO: Cleaned case_3b
2026-02-12 11:30:05,511 INFO: Cleaned case_4
2026-02-12 11:30:05,512 INFO: CLEANUP COMPLETED
2026-02-12 11:30:05,513 INFO: ============================================================
2026-02-12 11:30:05,513 INFO: ================================================================================
2026-02-12 11:30:05,514 INFO: SUMMARY PIPELINE - START
2026-02-12 11:30:05,514 INFO: ================================================================================
2026-02-12 11:30:05,515 INFO: ================================================================================
2026-02-12 11:30:05,515 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:30:05,515 INFO: ================================================================================
2026-02-12 11:30:08,262 INFO: Loading accounts from primary_catalog.main_null.accounts_all
2026-02-12 11:30:08,439 INFO: Reading from primary_catalog.main_null.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:30:08,440 INFO: Preparing source data...
2026-02-12 11:30:08,613 INFO: Applied 36 column mappings
2026-02-12 11:30:09,121 INFO: Applied 7 column transformations
2026-02-12 11:30:09,308 INFO: Created 2 inferred columns
2026-02-12 11:30:10,053 INFO: Validated 7 date columns
2026-02-12 11:30:10,317 INFO: Source data preparation complete
2026-02-12 11:30:10,654 INFO: Loading summary metadata from primary_catalog.main_null.latest_summary
2026-02-12 11:30:10,699 INFO: Loaded metadata for existing accounts
2026-02-12 11:30:10,700 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:===>                                                   (13 + 7) / 200][Stage 29:======>                                                (24 + 6) / 200][Stage 29:=========>                                             (33 + 6) / 200][Stage 29:===========>                                           (41 + 7) / 200][Stage 29:==============>                                        (52 + 6) / 200][Stage 29:=================>                                     (63 + 6) / 200][Stage 29:====================>                                  (75 + 6) / 200][Stage 29:=======================>                               (84 + 7) / 200][Stage 29:==========================>                            (96 + 6) / 200][Stage 29:=============================>                        (108 + 6) / 200][Stage 29:================================>                     (119 + 7) / 200][Stage 29:==================================>                   (127 + 6) / 200][Stage 29:=====================================>                (139 + 6) / 200][Stage 29:=========================================>            (152 + 6) / 200][Stage 29:=============================================>        (168 + 6) / 200][Stage 29:================================================>     (180 + 6) / 200][Stage 29:====================================================> (193 + 6) / 200][Stage 32:======>                                                (22 + 6) / 200][Stage 32:==========>                                            (38 + 6) / 200][Stage 32:==============>                                        (51 + 6) / 200][Stage 32:==================>                                    (66 + 6) / 200][Stage 32:======================>                                (82 + 6) / 200][Stage 32:==========================>                            (98 + 7) / 200][Stage 32:===============================>                      (117 + 7) / 200]                                                                                2026-02-12 11:30:18,436 INFO: ------------------------------------------------------------
2026-02-12 11:30:18,439 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:30:18,440 INFO:   CASE_II: 1 records
2026-02-12 11:30:18,441 INFO:   CASE_I: 1 records
2026-02-12 11:30:18,442 INFO: ------------------------------------------------------------
2026-02-12 11:30:18,442 INFO: Classification | Time Elapsed: 0.22 minutes
2026-02-12 11:30:18,496 INFO: 
>>> PROCESSING NEW ACCOUNTS (1 records)
2026-02-12 11:30:18,496 INFO: ================================================================================
2026-02-12 11:30:18,497 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:30:18,497 INFO: ================================================================================
2026-02-12 11:30:18,498 INFO: Processing new accounts
2026-02-12 11:30:19,467 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 42:==========================>                            (96 + 6) / 200][Stage 42:===================================>                  (131 + 8) / 200][Stage 42:=============================================>        (169 + 6) / 200][Stage 46:================================================>       (43 + 6) / 50]                                                                                2026-02-12 11:30:24,809 INFO: Case I Generated | Time Elapsed: 0.11 minutes
2026-02-12 11:30:24,809 INFO: ------------------------------------------------------------
2026-02-12 11:30:24,838 INFO: ------------------------------------------------------------
2026-02-12 11:30:24,839 INFO: MERGING RECORDS:
2026-02-12 11:30:24,848 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:30:24,849 INFO: ------------------------------------------------------------
2026-02-12 11:30:24,857 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:30:24,858 INFO: ------------------------------------------------------------
2026-02-12 11:30:24,859 INFO: APPENDING RECORDS:
2026-02-12 11:30:24,893 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:30:25,684 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.01 minutes
2026-02-12 11:30:25,684 INFO: ------------------------------------------------------------
2026-02-12 11:30:25,685 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:30:27,340 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.01 minutes
2026-02-12 11:30:27,341 INFO: ------------------------------------------------------------
2026-02-12 11:30:27,377 INFO: 
>>> PROCESSING FORWARD ENTRIES (1 records)
2026-02-12 11:30:27,377 INFO: ================================================================================
2026-02-12 11:30:27,378 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:30:27,379 INFO: ================================================================================
2026-02-12 11:30:27,379 INFO: Processing forward entries
2026-02-12 11:30:27,423 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:30:28,215 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 62:>(28 + 7) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:>(47 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:>(66 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:>(83 + 7) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(101 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(119 + 7) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(138 + 7) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(161 + 7) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(180 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(197 + 3) / 200][Stage 63:> (3 + 3) / 200][Stage 64:> (0 + 0) / 200][Stage 63:=>             (23 + 6) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:====>          (63 + 7) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:=======>       (99 + 7) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:=========>    (134 + 6) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:============> (178 + 6) / 200][Stage 64:>               (0 + 0) / 200][Stage 64:=======>                                               (28 + 6) / 200][Stage 64:===================>                                   (70 + 6) / 200][Stage 64:==============================>                       (112 + 6) / 200][Stage 64:=========================================>            (155 + 9) / 200][Stage 65:====>                                                  (15 + 7) / 200][Stage 65:==========>                                            (38 + 6) / 200][Stage 65:================>                                      (59 + 7) / 200][Stage 65:=========================>                             (92 + 7) / 200][Stage 65:==============================>                       (114 + 7) / 200][Stage 65:=========>    (135 + 6) / 200][Stage 66:>               (0 + 0) / 200][Stage 65:===========>  (159 + 7) / 200][Stage 66:>               (0 + 0) / 200][Stage 65:=============>(191 + 6) / 200][Stage 66:>               (0 + 0) / 200]                                                                                [Stage 66:==============>                                        (52 + 7) / 200][Stage 66:=====================>                                 (79 + 7) / 200][Stage 66:===========================>                          (103 + 7) / 200][Stage 66:====================================>                 (136 + 6) / 200][Stage 66:==============================================>       (173 + 6) / 200]                                                                                [Stage 70:=>                                                      (7 + 6) / 200][Stage 70:===>                                                   (13 + 6) / 200][Stage 70:=====>                                                 (19 + 6) / 200][Stage 70:======>                                                (25 + 6) / 200][Stage 70:========>                                              (32 + 6) / 200][Stage 70:===========>                                           (41 + 6) / 200][Stage 70:============>                                          (47 + 6) / 200][Stage 70:==============>                                        (53 + 6) / 200][Stage 70:================>                                      (59 + 7) / 200][Stage 70:=================>                                     (65 + 6) / 200][Stage 70:===================>                                   (71 + 6) / 200][Stage 70:=====================>                                 (79 + 6) / 200][Stage 70:=======================>                               (84 + 6) / 200][Stage 70:=======================>                               (87 + 6) / 200][Stage 70:=========================>                             (94 + 6) / 200][Stage 70:===========================>                          (102 + 6) / 200][Stage 70:==============================>                       (112 + 6) / 200][Stage 70:================================>                     (119 + 6) / 200][Stage 70:==================================>                   (126 + 6) / 200][Stage 70:====================================>                 (134 + 6) / 200][Stage 70:======================================>               (141 + 6) / 200][Stage 70:=======================================>              (148 + 6) / 200][Stage 70:==========================================>           (157 + 6) / 200][Stage 70:============================================>         (163 + 6) / 200][Stage 70:==============================================>       (171 + 7) / 200][Stage 70:===============================================>      (177 + 6) / 200][Stage 70:==================================================>   (186 + 6) / 200][Stage 70:====================================================> (193 + 6) / 200]                                                                                2026-02-12 11:30:44,456 INFO: Case II Generated | Time Elapsed: 0.28 minutes
2026-02-12 11:30:44,457 INFO: ------------------------------------------------------------
2026-02-12 11:30:44,457 INFO: ------------------------------------------------------------
2026-02-12 11:30:44,458 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:30:44,485 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:30:45,161 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:30:45,162 INFO: ------------------------------------------------------------
2026-02-12 11:30:45,619 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:30:45,620 INFO: ------------------------------------------------------------
2026-02-12 11:30:45,624 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] null_update
2026-02-12 11:30:46,864 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:30:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:30:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:30:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=======================================>                   (4 + 2) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 6) / 6][Stage 3:=================================================>         (5 + 1) / 6]                                                                                [Stage 4:=================================================>         (5 + 1) / 6]                                                                                [Stage 7:=============================>                             (3 + 3) / 6]                                                                                [Stage 8:=========>                                                 (1 + 5) / 6]                                                                                2026-02-12 11:31:11,123 INFO: Cleaned case_1
2026-02-12 11:31:11,147 INFO: Cleaned case_2
2026-02-12 11:31:11,175 INFO: Cleaned case_3a
2026-02-12 11:31:11,204 INFO: Cleaned case_3b
2026-02-12 11:31:11,229 INFO: Cleaned case_4
2026-02-12 11:31:11,230 INFO: CLEANUP COMPLETED
2026-02-12 11:31:11,230 INFO: ============================================================
2026-02-12 11:31:11,231 INFO: ================================================================================
2026-02-12 11:31:11,231 INFO: SUMMARY PIPELINE - START
2026-02-12 11:31:11,232 INFO: ================================================================================
2026-02-12 11:31:11,232 INFO: ================================================================================
2026-02-12 11:31:11,232 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:31:11,233 INFO: ================================================================================
[Stage 14:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:31:17,413 INFO: Loading accounts from primary_catalog.main_null.accounts_all
2026-02-12 11:31:17,591 INFO: Reading from primary_catalog.main_null.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:31:12,094 INFO: Preparing source data...
2026-02-12 11:31:12,287 INFO: Applied 36 column mappings
2026-02-12 11:31:12,700 INFO: Applied 7 column transformations
2026-02-12 11:31:12,867 INFO: Created 2 inferred columns
2026-02-12 11:31:13,535 INFO: Validated 7 date columns
2026-02-12 11:31:13,697 INFO: Source data preparation complete
2026-02-12 11:31:13,879 INFO: Loading summary metadata from primary_catalog.main_null.latest_summary
2026-02-12 11:31:13,947 INFO: Loaded metadata for existing accounts
2026-02-12 11:31:13,948 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:=>                                                      (5 + 6) / 200]                                                                                2026-02-12 11:31:23,410 INFO: ------------------------------------------------------------
2026-02-12 11:31:23,411 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:31:23,411 INFO:   CASE_II: 1 records
2026-02-12 11:31:23,412 INFO:   CASE_I: 1 records
2026-02-12 11:31:23,412 INFO: ------------------------------------------------------------
2026-02-12 11:31:23,413 INFO: Classification | Time Elapsed: 0.20 minutes
2026-02-12 11:31:23,502 INFO: 
>>> PROCESSING NEW ACCOUNTS (1 records)
2026-02-12 11:31:23,503 INFO: ================================================================================
2026-02-12 11:31:23,503 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:31:23,504 INFO: ================================================================================
2026-02-12 11:31:23,505 INFO: Processing new accounts
2026-02-12 11:31:24,378 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 39:=======>                                               (27 + 6) / 200][Stage 39:============>                                          (44 + 6) / 200][Stage 39:===============>                                       (56 + 8) / 200][Stage 39:===================>                                   (70 + 6) / 200][Stage 39:=======================>                               (86 + 6) / 200][Stage 39:============================>                         (105 + 6) / 200][Stage 39:================================>                     (121 + 6) / 200][Stage 39:=====================================>                (140 + 6) / 200][Stage 39:==========================================>           (158 + 6) / 200][Stage 39:===============================================>      (176 + 6) / 200][Stage 39:====================================================> (195 + 5) / 200][Stage 42:==========================>                            (98 + 6) / 200][Stage 42:=================================>                    (125 + 7) / 200][Stage 42:===========================================>          (162 + 6) / 200][Stage 46:====================================>                   (33 + 6) / 50]                                                                                2026-02-12 11:31:29,825 INFO: Case I Generated | Time Elapsed: 0.11 minutes
2026-02-12 11:31:29,826 INFO: ------------------------------------------------------------
2026-02-12 11:31:29,903 INFO: ------------------------------------------------------------
2026-02-12 11:31:29,904 INFO: MERGING RECORDS:
2026-02-12 11:31:29,919 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:31:29,921 INFO: ------------------------------------------------------------
2026-02-12 11:31:29,942 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:31:29,943 INFO: ------------------------------------------------------------
2026-02-12 11:31:29,944 INFO: APPENDING RECORDS:
2026-02-12 11:31:30,029 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:31:30,823 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.01 minutes
2026-02-12 11:31:30,824 INFO: ------------------------------------------------------------
2026-02-12 11:31:30,825 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:31:32,714 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.01 minutes
2026-02-12 11:31:32,714 INFO: ------------------------------------------------------------
2026-02-12 11:31:32,741 INFO: 
>>> PROCESSING FORWARD ENTRIES (1 records)
2026-02-12 11:31:32,741 INFO: ================================================================================
2026-02-12 11:31:32,742 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:31:32,742 INFO: ================================================================================
2026-02-12 11:31:32,743 INFO: Processing forward entries
2026-02-12 11:31:32,790 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:31:33,741 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 62:>(54 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:>(73 + 7) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:>(90 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(110 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(130 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(152 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(172 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 62:(192 + 6) / 200][Stage 63:> (0 + 0) / 200][Stage 64:> (0 + 0) / 200][Stage 63:==>            (29 + 7) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:====>          (64 + 8) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:========>     (117 + 6) / 200][Stage 64:>               (0 + 0) / 200][Stage 63:===========>  (170 + 6) / 200][Stage 64:>               (0 + 0) / 200][Stage 64:====>                                                  (18 + 6) / 200][Stage 64:=================>                                     (63 + 6) / 200][Stage 64:===========================>                          (103 + 6) / 200][Stage 64:======================================>               (144 + 7) / 200][Stage 64:=====================================================>(198 + 2) / 200][Stage 65:=======>                                               (29 + 6) / 200][Stage 65:=============>                                         (50 + 9) / 200][Stage 65:=====================>                                 (77 + 8) / 200][Stage 65:============================>                         (105 + 6) / 200][Stage 65:=========>    (131 + 6) / 200][Stage 66:>               (0 + 0) / 200][Stage 65:===========>  (161 + 6) / 200][Stage 66:>               (0 + 0) / 200][Stage 65:=============>(189 + 6) / 200][Stage 66:>               (0 + 0) / 200]                                                                                [Stage 66:================>                                      (61 + 6) / 200][Stage 66:===================>                                   (72 + 7) / 200][Stage 66:===========================>                          (101 + 6) / 200][Stage 66:==================================>                   (129 + 7) / 200]                                                                                [Stage 70:===========================================>          (160 + 8) / 200][Stage 70:=============================================>        (170 + 6) / 200][Stage 70:================================================>     (180 + 6) / 200][Stage 70:==================================================>   (188 + 6) / 200][Stage 70:====================================================> (196 + 4) / 200]                                                                                2026-02-12 11:31:46,811 INFO: Case II Generated | Time Elapsed: 0.23 minutes
2026-02-12 11:31:46,812 INFO: ------------------------------------------------------------
2026-02-12 11:31:46,813 INFO: ------------------------------------------------------------
2026-02-12 11:31:46,813 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:31:46,841 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:31:47,465 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:31:47,466 INFO: ------------------------------------------------------------
2026-02-12 11:31:49,876 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.04 minutes
2026-02-12 11:31:49,877 INFO: ------------------------------------------------------------
2026-02-12 11:31:49,877 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] null_update
2026-02-12 11:31:51,718 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:31:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:31:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:32:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                2026-02-12 11:32:11,540 INFO: Cleaned case_1
2026-02-12 11:32:11,572 INFO: Cleaned case_2
2026-02-12 11:32:11,603 INFO: Cleaned case_3a
2026-02-12 11:32:11,629 INFO: Cleaned case_3b
2026-02-12 11:32:11,671 INFO: Cleaned case_4
2026-02-12 11:32:11,672 INFO: CLEANUP COMPLETED
2026-02-12 11:32:11,673 INFO: ============================================================
2026-02-12 11:32:11,674 INFO: ================================================================================
2026-02-12 11:32:11,674 INFO: SUMMARY PIPELINE - START
2026-02-12 11:32:11,675 INFO: ================================================================================
2026-02-12 11:32:11,675 INFO: ================================================================================
2026-02-12 11:32:11,676 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:32:11,676 INFO: ================================================================================
[Stage 10:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:32:13,956 INFO: Loading accounts from primary_catalog.main_recovery.accounts_all
2026-02-12 11:32:14,134 INFO: Reading from primary_catalog.main_recovery.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2020-01-02 00:00:00
2026-02-12 11:32:14,134 INFO: Preparing source data...
2026-02-12 11:32:14,314 INFO: Applied 36 column mappings
2026-02-12 11:32:14,817 INFO: Applied 7 column transformations
2026-02-12 11:32:15,046 INFO: Created 2 inferred columns
2026-02-12 11:32:15,795 INFO: Validated 7 date columns
2026-02-12 11:32:15,952 INFO: Source data preparation complete
2026-02-12 11:32:16,133 INFO: Loading summary metadata from primary_catalog.main_recovery.latest_summary
2026-02-12 11:32:16,169 INFO: Loaded metadata for existing accounts
2026-02-12 11:32:16,170 INFO: Classifying accounts into Case I/II/III/IV
[Stage 25:====>                                                  (15 + 6) / 200][Stage 25:=======>                                               (27 + 6) / 200][Stage 25:=========>                                             (36 + 6) / 200][Stage 25:=============>                                         (49 + 6) / 200][Stage 25:=================>                                     (63 + 6) / 200][Stage 25:===================>                                   (70 + 6) / 200][Stage 25:=====================>                                 (79 + 7) / 200][Stage 25:=========================>                             (92 + 6) / 200][Stage 25:===========================>                          (103 + 8) / 200][Stage 25:==============================>                       (114 + 6) / 200][Stage 25:==================================>                   (126 + 6) / 200][Stage 25:=====================================>                (140 + 6) / 200][Stage 25:=========================================>            (152 + 6) / 200][Stage 25:=============================================>        (169 + 6) / 200][Stage 25:=================================================>    (183 + 6) / 200][Stage 25:=====================================================>(197 + 3) / 200][Stage 28:=========>                                             (33 + 6) / 200][Stage 28:=============>                                         (50 + 6) / 200][Stage 28:==================>                                    (68 + 6) / 200][Stage 28:======================>                                (82 + 6) / 200][Stage 28:===========================>                          (102 + 6) / 200][Stage 28:================================>                     (119 + 7) / 200][Stage 28:=====================================>                (138 + 7) / 200][Stage 28:==========================================>           (156 + 6) / 200][Stage 28:===============================================>      (177 + 6) / 200]                                                                                2026-02-12 11:32:26,481 INFO: ------------------------------------------------------------
2026-02-12 11:32:26,482 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:32:26,483 INFO:   CASE_I: 1 records
2026-02-12 11:32:26,483 INFO: ------------------------------------------------------------
2026-02-12 11:32:26,484 INFO: Classification | Time Elapsed: 0.25 minutes
2026-02-12 11:32:26,545 INFO: 
>>> PROCESSING NEW ACCOUNTS (1 records)
2026-02-12 11:32:26,545 INFO: ================================================================================
2026-02-12 11:32:26,546 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:32:26,546 INFO: ================================================================================
2026-02-12 11:32:26,547 INFO: Processing new accounts
2026-02-12 11:32:27,365 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 35:=======>                                               (29 + 6) / 200][Stage 35:==========>                                            (39 + 6) / 200][Stage 35:===============>                                       (58 + 7) / 200][Stage 35:====================>                                  (74 + 7) / 200][Stage 35:========================>                              (90 + 6) / 200][Stage 35:============================>                         (107 + 6) / 200][Stage 35:==================================>                   (126 + 5) / 200][Stage 35:=====================================>                (140 + 6) / 200][Stage 35:========================================>             (151 + 6) / 200][Stage 35:=============================================>        (167 + 6) / 200][Stage 35:===============================================>      (175 + 7) / 200][Stage 35:==================================================>   (186 + 6) / 200][Stage 38:=====================>                                 (77 + 7) / 200][Stage 38:==============================>                       (114 + 6) / 200][Stage 38:=======================================>              (148 + 6) / 200][Stage 38:================================================>     (180 + 7) / 200][Stage 42:================================>                       (29 + 6) / 50][Stage 42:=============================================>          (41 + 6) / 50]                                                                                2026-02-12 11:32:33,456 INFO: Case I Generated | Time Elapsed: 0.12 minutes
2026-02-12 11:32:33,457 INFO: ------------------------------------------------------------
2026-02-12 11:32:33,536 INFO: ------------------------------------------------------------
2026-02-12 11:32:33,537 INFO: MERGING RECORDS:
2026-02-12 11:32:33,548 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:32:33,549 INFO: ------------------------------------------------------------
2026-02-12 11:32:33,558 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:32:33,559 INFO: ------------------------------------------------------------
2026-02-12 11:32:33,559 INFO: APPENDING RECORDS:
2026-02-12 11:32:33,597 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:32:34,415 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.01 minutes
2026-02-12 11:32:34,416 INFO: ------------------------------------------------------------
2026-02-12 11:32:34,417 INFO: UPDATING LATEST SUMMARY:
2026-02-12 11:32:34,147 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.01 minutes
2026-02-12 11:32:34,148 INFO: ------------------------------------------------------------
2026-02-12 11:32:34,179 INFO: PROCESS COMPLETED - Deleting the persisted classification results
2026-02-12 11:32:34,850 INFO: Cleaned case_1
2026-02-12 11:32:34,867 INFO: Cleaned case_2
2026-02-12 11:32:34,883 INFO: Cleaned case_3a
2026-02-12 11:32:34,900 INFO: Cleaned case_3b
2026-02-12 11:32:34,917 INFO: Cleaned case_4
2026-02-12 11:32:34,918 INFO: CLEANUP COMPLETED
2026-02-12 11:32:34,918 INFO: ============================================================
2026-02-12 11:32:34,919 INFO: ================================================================================
2026-02-12 11:32:34,920 INFO: SUMMARY PIPELINE - START
2026-02-12 11:32:34,920 INFO: ================================================================================
2026-02-12 11:32:34,921 INFO: ================================================================================
2026-02-12 11:32:34,922 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:32:34,923 INFO: ================================================================================
26/02/12 11:32:35 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$9(DataSourceV2Strategy.scala:272)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)
	org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
26/02/12 11:32:35 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$9(DataSourceV2Strategy.scala:272)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)
	org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
2026-02-12 11:32:35,923 INFO: Loading accounts from primary_catalog.main_recovery.accounts_all
2026-02-12 11:32:35,950 INFO: Reading from primary_catalog.main_recovery.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2026-02-01 00:00:00
2026-02-12 11:32:35,951 INFO: Preparing source data...
2026-02-12 11:32:36,087 INFO: Applied 36 column mappings
2026-02-12 11:32:36,268 INFO: Applied 7 column transformations
2026-02-12 11:32:36,335 INFO: Created 2 inferred columns
2026-02-12 11:32:36,735 INFO: Validated 7 date columns
2026-02-12 11:32:36,818 INFO: Source data preparation complete
2026-02-12 11:32:36,846 INFO: Loading summary metadata from primary_catalog.main_recovery.latest_summary
2026-02-12 11:32:36,864 INFO: Loaded metadata for existing accounts
2026-02-12 11:32:36,865 INFO: Classifying accounts into Case I/II/III/IV
2026-02-12 11:32:37,762 INFO: ------------------------------------------------------------
2026-02-12 11:32:37,762 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:32:37,763 INFO: ------------------------------------------------------------
2026-02-12 11:32:37,763 INFO: Classification | Time Elapsed: 0.05 minutes
2026-02-12 11:32:37,853 INFO: ------------------------------------------------------------
2026-02-12 11:32:37,854 INFO: MERGING RECORDS:
2026-02-12 11:32:37,863 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:32:37,864 INFO: ------------------------------------------------------------
2026-02-12 11:32:37,872 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:32:37,876 INFO: ------------------------------------------------------------
2026-02-12 11:32:37,877 INFO: APPENDING RECORDS:
2026-02-12 11:32:37,893 INFO: No Case I/IV records to append | Time Elapsed: 0.00 minutes
2026-02-12 11:32:37,893 INFO: ------------------------------------------------------------
2026-02-12 11:32:37,925 INFO: PROCESS COMPLETED - Deleting the persisted classification results
2026-02-12 11:32:39,525 INFO: Cleaned case_1
2026-02-12 11:32:39,541 INFO: Cleaned case_2
2026-02-12 11:32:39,556 INFO: Cleaned case_3a
2026-02-12 11:32:39,569 INFO: Cleaned case_3b
2026-02-12 11:32:39,583 INFO: Cleaned case_4
2026-02-12 11:32:39,584 INFO: CLEANUP COMPLETED
2026-02-12 11:32:39,584 INFO: ============================================================
2026-02-12 11:32:39,585 INFO: ================================================================================
2026-02-12 11:32:39,585 INFO: SUMMARY PIPELINE - START
2026-02-12 11:32:39,586 INFO: ================================================================================
2026-02-12 11:32:39,586 INFO: ================================================================================
2026-02-12 11:32:39,587 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:32:39,587 INFO: ================================================================================
26/02/12 11:32:40 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.uncacheQuery(CacheManager.scala:224)
	org.apache.spark.sql.execution.CacheManager.uncacheQuery(CacheManager.scala:165)
	org.apache.spark.sql.Dataset.unpersist(Dataset.scala:3824)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
2026-02-12 11:32:40,610 INFO: Loading accounts from primary_catalog.main_recovery.accounts_all
2026-02-12 11:32:40,637 INFO: Reading from primary_catalog.main_recovery.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2026-02-01 00:00:00
2026-02-12 11:32:40,638 INFO: Preparing source data...
2026-02-12 11:32:40,766 INFO: Applied 36 column mappings
2026-02-12 11:32:40,890 INFO: Applied 7 column transformations
2026-02-12 11:32:40,939 INFO: Created 2 inferred columns
2026-02-12 11:32:41,431 INFO: Validated 7 date columns
2026-02-12 11:32:41,521 INFO: Source data preparation complete
2026-02-12 11:32:41,548 INFO: Loading summary metadata from primary_catalog.main_recovery.latest_summary
2026-02-12 11:32:41,567 INFO: Loaded metadata for existing accounts
2026-02-12 11:32:41,567 INFO: Classifying accounts into Case I/II/III/IV
[Stage 94:====>                                                  (16 + 7) / 200]                                                                                2026-02-12 11:32:46,458 INFO: ------------------------------------------------------------
2026-02-12 11:32:46,464 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:32:46,465 INFO:   CASE_II: 1 records
2026-02-12 11:32:46,465 INFO: ------------------------------------------------------------
2026-02-12 11:32:46,466 INFO: Classification | Time Elapsed: 0.11 minutes
2026-02-12 11:32:46,526 INFO: ------------------------------------------------------------
2026-02-12 11:32:46,527 INFO: MERGING RECORDS:
2026-02-12 11:32:46,534 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:32:46,535 INFO: ------------------------------------------------------------
2026-02-12 11:32:46,540 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:32:46,544 INFO: ------------------------------------------------------------
2026-02-12 11:32:46,544 INFO: APPENDING RECORDS:
2026-02-12 11:32:46,554 INFO: No Case I/IV records to append | Time Elapsed: 0.00 minutes
2026-02-12 11:32:46,555 INFO: ------------------------------------------------------------
2026-02-12 11:32:46,571 INFO: 
>>> PROCESSING FORWARD ENTRIES (1 records)
2026-02-12 11:32:46,572 INFO: ================================================================================
2026-02-12 11:32:46,572 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:32:46,573 INFO: ================================================================================
2026-02-12 11:32:46,573 INFO: Processing forward entries
2026-02-12 11:32:46,603 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:32:47,446 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=1, scale=1.0)
[Stage 105:>                                                      (0 + 0) / 200]                                                                                [Stage 105:==================>                                   (67 + 7) / 200][Stage 105:=========================>                            (94 + 6) / 200][Stage 105:=================================>                   (126 + 7) / 200][Stage 105:=======================================>             (148 + 6) / 200][Stage 105:=============================================>       (173 + 6) / 200]                                                                                [Stage 109:=>                                                     (6 + 6) / 200][Stage 109:===>                                                  (13 + 6) / 200][Stage 109:=====>                                                (20 + 6) / 200][Stage 109:=======>                                              (27 + 6) / 200][Stage 109:=========>                                            (34 + 6) / 200][Stage 109:===========>                                          (42 + 6) / 200][Stage 109:============>                                         (47 + 6) / 200]                                                                                2026-02-12 11:32:57,629 INFO: Case II Generated | Time Elapsed: 0.18 minutes
2026-02-12 11:32:57,633 INFO: ------------------------------------------------------------
2026-02-12 11:32:57,634 INFO: ------------------------------------------------------------
2026-02-12 11:32:57,634 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:32:57,658 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=1, scale=1.0)
2026-02-12 11:32:57,750 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.00 minutes
2026-02-12 11:32:57,752 INFO: ------------------------------------------------------------
2026-02-12 11:33:01,006 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.05 minutes
2026-02-12 11:33:01,007 INFO: ------------------------------------------------------------
2026-02-12 11:33:01,007 INFO: PROCESS COMPLETED - Deleting the persisted classification results
26/02/12 11:33:01 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$19(DataSourceV2Strategy.scala:331)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.run(WriteToDataSourceV2Exec.scala:276)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	jdk.internal.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[Stage 130:>                                                        (0 + 1) / 1]                                                                                26/02/12 11:33:06 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:371)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:369)
	org.apache.iceberg.rest.RESTSessionCatalog.newFileIO(RESTSessionCatalog.java:1042)
	org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:323)
	org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:274)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:328)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:364)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$4(CacheManager.scala:277)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3(CacheManager.scala:275)
	org.apache.spark.sql.execution.CacheManager.$anonfun$recacheByCondition$3$adapted(CacheManager.scala:272)
	scala.collection.Iterator.foreach(Iterator.scala:943)
	scala.collection.Iterator.foreach$(Iterator.scala:943)
	scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	scala.collection.IterableLike.foreach(IterableLike.scala:74)
	scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:272)
	org.apache.spark.sql.execution.CacheManager.recacheByPlan(CacheManager.scala:258)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.refreshCache(DataSourceV2Strategy.scala:76)
	org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$9(DataSourceV2Strategy.scala:272)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:343)
	org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)
	org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	java.base/java.lang.reflect.Method.invoke(Method.java:569)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:282)
	py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	py4j.commands.CallCommand.execute(CallCommand.java:79)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.base/java.lang.Thread.run(Thread.java:840)
[PASS] recovery
2026-02-12 11:33:01,456 INFO: Closing down clientserver connection
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/12 11:33:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/02/12 11:33:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/02/12 11:33:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 0:>                                                          (0 + 6) / 6][Stage 0:=========>                                                 (1 + 5) / 6][Stage 2:>                                                          (0 + 1) / 1]                                                                                [Stage 8:=================================================>         (5 + 1) / 6][Stage 10:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:33:24,821 INFO: Cleaned case_1
2026-02-12 11:33:24,844 INFO: Cleaned case_2
2026-02-12 11:33:24,870 INFO: Cleaned case_3a
2026-02-12 11:33:24,900 INFO: Cleaned case_3b
2026-02-12 11:33:24,921 INFO: Cleaned case_4
2026-02-12 11:33:24,921 INFO: CLEANUP COMPLETED
2026-02-12 11:33:24,922 INFO: ============================================================
2026-02-12 11:33:24,922 INFO: ================================================================================
2026-02-12 11:33:24,923 INFO: SUMMARY PIPELINE - START
2026-02-12 11:33:24,923 INFO: ================================================================================
2026-02-12 11:33:24,924 INFO: ================================================================================
2026-02-12 11:33:24,924 INFO: STEP 1: Load and Classify Accounts
2026-02-12 11:33:24,924 INFO: ================================================================================
[Stage 14:>                                                         (0 + 1) / 1]                                                                                2026-02-12 11:33:27,835 INFO: Loading accounts from primary_catalog.main_perf.accounts_all
2026-02-12 11:33:28,020 INFO: Reading from primary_catalog.main_perf.accounts_all - (rpt_as_of_mo < 2027-01) & (base_ts > 2025-12-01 00:00:00
2026-02-12 11:33:28,021 INFO: Preparing source data...
2026-02-12 11:33:33,242 INFO: Applied 36 column mappings
2026-02-12 11:33:28,113 INFO: Applied 7 column transformations
2026-02-12 11:33:28,247 INFO: Created 2 inferred columns
2026-02-12 11:33:29,105 INFO: Validated 7 date columns
2026-02-12 11:33:29,306 INFO: Source data preparation complete
2026-02-12 11:33:29,452 INFO: Loading summary metadata from primary_catalog.main_perf.latest_summary
2026-02-12 11:33:29,500 INFO: Loaded metadata for existing accounts
2026-02-12 11:33:29,500 INFO: Classifying accounts into Case I/II/III/IV
[Stage 29:=>                                                      (6 + 6) / 200][Stage 29:===>                                                   (12 + 6) / 200][Stage 29:=====>                                                 (19 + 6) / 200][Stage 29:========>                                              (30 + 6) / 200][Stage 29:==========>                                            (39 + 6) / 200][Stage 29:============>                                          (46 + 6) / 200][Stage 29:===============>                                       (57 + 7) / 200][Stage 29:===================>                                   (71 + 6) / 200][Stage 29:=======================>                               (84 + 6) / 200][Stage 29:===========================>                           (99 + 6) / 200][Stage 29:=============================>                        (111 + 6) / 200][Stage 29:=================================>                    (123 + 6) / 200][Stage 29:=====================================>                (138 + 6) / 200][Stage 29:========================================>             (151 + 6) / 200][Stage 29:===========================================>          (162 + 7) / 200][Stage 29:================================================>     (178 + 6) / 200][Stage 29:===================================================>  (192 + 6) / 200][Stage 32:===========>                                           (40 + 6) / 200][Stage 32:==============>                                        (54 + 6) / 200][Stage 32:==================>                                    (67 + 6) / 200]                                                                                2026-02-12 11:33:39,681 INFO: ------------------------------------------------------------
2026-02-12 11:33:39,682 INFO: CLASSIFICATION RESULTS:
2026-02-12 11:33:39,683 INFO:   CASE_II: 10 records
2026-02-12 11:33:39,683 INFO:   CASE_I: 25 records
2026-02-12 11:33:39,684 INFO:   CASE_IV: 35 records
2026-02-12 11:33:39,684 INFO: ------------------------------------------------------------
2026-02-12 11:33:39,685 INFO: Classification | Time Elapsed: 0.25 minutes
2026-02-12 11:33:39,734 INFO: 
>>> PROCESSING NEW ACCOUNTS (25 records)
2026-02-12 11:33:39,735 INFO: ================================================================================
2026-02-12 11:33:39,735 INFO: STEP 2a: Process Case I (New Accounts)
2026-02-12 11:33:39,735 INFO: ================================================================================
2026-02-12 11:33:39,736 INFO: Processing new accounts
2026-02-12 11:33:40,503 INFO: Write partitions [case_1_temp]: 50 (base=200, expected_rows=25, scale=1.0)
[Stage 39:===================================>                  (133 + 8) / 200]                                                                                2026-02-12 11:33:46,156 INFO: Case I Generated | Time Elapsed: 0.11 minutes
2026-02-12 11:33:46,156 INFO: ------------------------------------------------------------
2026-02-12 11:33:46,182 INFO: 
>>> PROCESSING BULK HISTORICAL (35 records)
2026-02-12 11:33:46,183 INFO: ================================================================================
2026-02-12 11:33:46,183 INFO: STEP 2d: Process Case IV (Bulk Historical Load)
2026-02-12 11:33:46,184 INFO: ================================================================================
2026-02-12 11:33:46,184 INFO: Processing bulk historical records
2026-02-12 11:33:46,275 INFO: Building complete history using window functions
2026-02-12 11:33:51,011 INFO: Write partitions [case_4_temp]: 50 (base=200, expected_rows=35, scale=1.0)
[Stage 58:=========================================>            (153 + 6) / 200][Stage 58:===============================================>      (175 + 6) / 200][Stage 58:=============>(197 + 3) / 200][Stage 59:>               (0 + 5) / 400]                                                                                [Stage 59:======>                                                (45 + 7) / 400][Stage 59:=========>                                             (71 + 7) / 400][Stage 59:==============>                                       (108 + 6) / 400][Stage 59:===============>                                      (117 + 7) / 400][Stage 59:=======================>                              (175 + 7) / 400][Stage 59:===========================>                          (200 + 6) / 400][Stage 59:=======>      (220 + 6) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:========>     (254 + 6) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:==========>   (286 + 6) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:==========>   (307 + 6) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:===========>  (341 + 7) / 400][Stage 60:>               (0 + 0) / 400][Stage 59:=============>(374 + 6) / 400][Stage 60:>               (0 + 0) / 400]                                                                                [Stage 60:=========>                                             (70 + 7) / 400][Stage 60:=============>                                         (95 + 6) / 400][Stage 60:================>                                     (120 + 6) / 400][Stage 60:====================>                                 (155 + 6) / 400]                                                                                2026-02-12 11:34:06,331 INFO: Case IV Generated | Time Elapsed: 0.34 minutes
2026-02-12 11:34:06,332 INFO: ------------------------------------------------------------
2026-02-12 11:34:06,350 INFO: ------------------------------------------------------------
2026-02-12 11:34:06,351 INFO: MERGING RECORDS:
2026-02-12 11:34:06,361 INFO: MERGED - CASE III-A (NEW summary rows for backfill months with inherited history)| Time Elapsed: 0.00 minutes
2026-02-12 11:34:06,362 INFO: ------------------------------------------------------------
2026-02-12 11:34:06,370 INFO: Updated Summary | MERGED - CASE III-B (future summary rows with backfill data) | Time Elapsed: 0.00 minutes
2026-02-12 11:34:06,371 INFO: ------------------------------------------------------------
2026-02-12 11:34:06,372 INFO: APPENDING RECORDS:
2026-02-12 11:34:06,434 INFO: Write partitions [summary_append_case_1_4]: 50 (base=200, expected_rows=60, scale=1.0)
2026-02-12 11:34:08,403 INFO: Updated Summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.03 minutes
2026-02-12 11:34:08,403 INFO: ------------------------------------------------------------
2026-02-12 11:34:08,404 INFO: UPDATING LATEST SUMMARY:
[Stage 76:================================>                        (8 + 6) / 14]                                                                                2026-02-12 11:34:10,469 INFO: Updated latest_summary | APPENDED - CASE I & IV (New Records + Bulk Historical) | Time Elapsed: 0.03 minutes
2026-02-12 11:34:10,474 INFO: ------------------------------------------------------------
2026-02-12 11:34:10,495 INFO: 
>>> PROCESSING FORWARD ENTRIES (10 records)
2026-02-12 11:34:10,496 INFO: ================================================================================
2026-02-12 11:34:10,497 INFO: STEP 2b: Process Case II (Forward Entries)
2026-02-12 11:34:10,497 INFO: ================================================================================
2026-02-12 11:34:10,497 INFO: Processing forward entries
2026-02-12 11:34:10,531 INFO: Loading latest summary for Affected Accounts
2026-02-12 11:34:11,240 INFO: Write partitions [case_2_temp]: 50 (base=200, expected_rows=10, scale=1.0)
[Stage 85:>(24 + 7) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200][Stage 85:>(42 + 6) / 200][Stage 86:> (0 + 0) / 200][Stage 87:> (0 + 0) / 200]                                                                                [Stage 89:=================================>                    (124 + 6) / 200][Stage 89:=========================================>            (155 + 6) / 200][Stage 89:==================================================>   (187 + 7) / 200][Stage 89:====================================================> (195 + 5) / 200]                                                                                [Stage 93:=>                                                      (6 + 6) / 200][Stage 93:===>                                                   (12 + 6) / 200][Stage 93:====>                                                  (18 + 6) / 200][Stage 93:=======>                                               (27 + 6) / 200][Stage 93:=========>                                             (33 + 6) / 200][Stage 93:===========>                                           (41 + 6) / 200][Stage 93:============>                                          (46 + 6) / 200][Stage 93:=============>                                         (48 + 6) / 200][Stage 93:==============>                                        (54 + 6) / 200][Stage 93:================>                                      (60 + 6) / 200][Stage 93:==================>                                    (66 + 7) / 200][Stage 93:====================>                                  (75 + 7) / 200][Stage 93:======================>                                (81 + 7) / 200][Stage 93:========================>                              (90 + 6) / 200][Stage 93:==========================>                            (96 + 6) / 200][Stage 93:===========================>                          (103 + 7) / 200]                                                                                2026-02-12 11:34:26,277 INFO: Case II Generated | Time Elapsed: 0.26 minutes
2026-02-12 11:34:26,278 INFO: ------------------------------------------------------------
2026-02-12 11:34:26,279 INFO: ------------------------------------------------------------
2026-02-12 11:34:26,279 INFO: MERGING FORWARD RECORDS:
2026-02-12 11:34:26,311 INFO: Write partitions [summary_append_case_2]: 50 (base=200, expected_rows=10, scale=1.0)
2026-02-12 11:34:27,055 INFO: Updated Summary | APPENDED - CASE II (Forward Records) | Time Elapsed: 0.01 minutes
2026-02-12 11:34:27,056 INFO: ------------------------------------------------------------
2026-02-12 11:34:29,566 INFO: Updated latest_summary | MERGE - CASE II (Forward Records) | Time Elapsed: 0.04 minutes
2026-02-12 11:34:29,568 INFO: ------------------------------------------------------------
2026-02-12 11:34:29,569 INFO: PROCESS COMPLETED - Deleting the persisted classification results
[PASS] performance_benchmark scale=TINY rows=70 elapsed=64.78s throughput=1.1/s
2026-02-12 11:34:30,465 INFO: Closing down clientserver connection
================================================================================
RUNNING simple_test.py
================================================================================
================================================================================
RUNNING test_main_all_cases.py
================================================================================
================================================================================
RUNNING test_main_base_ts_propagation.py
================================================================================
================================================================================
RUNNING run_backfill_test.py
================================================================================
================================================================================
RUNNING test_all_scenarios.py
================================================================================
================================================================================
RUNNING test_all_scenarios_v942.py
================================================================================
================================================================================
RUNNING test_bulk_historical_load.py
================================================================================
================================================================================
RUNNING test_complex_scenarios.py
================================================================================
================================================================================
RUNNING test_comprehensive_50_cases.py
================================================================================
================================================================================
RUNNING test_comprehensive_edge_cases.py
================================================================================
================================================================================
RUNNING test_consecutive_backfill.py
================================================================================
================================================================================
RUNNING test_duplicate_records.py
================================================================================
================================================================================
RUNNING test_full_46_columns.py
================================================================================
================================================================================
RUNNING test_long_backfill_gaps.py
================================================================================
================================================================================
RUNNING test_non_continuous_backfill.py
================================================================================
================================================================================
RUNNING test_null_update_case_iii.py
================================================================================
================================================================================
RUNNING test_null_update_other_cases.py
================================================================================
================================================================================
RUNNING test_null_update.py
================================================================================
================================================================================
RUNNING test_recovery.py
================================================================================
================================================================================
RUNNING test_performance_benchmark.py --scale TINY
================================================================================
================================================================================
ALL TESTS PASSED
================================================================================
