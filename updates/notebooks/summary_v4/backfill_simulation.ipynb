{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill Simulation - Summary Pipeline v4.0\n",
    "\n",
    "This notebook demonstrates the **backfill capability** of the Summary Pipeline v4.0.\n",
    "\n",
    "## What is Backfill?\n",
    "Backfill handles **late-arriving data** - when new records arrive for a historical month that was already processed.\n",
    "\n",
    "### Scenario\n",
    "1. We have processed months 2024-01 through 2024-06\n",
    "2. New data arrives for 2024-02 (with a newer timestamp)\n",
    "3. The pipeline must:\n",
    "   - Detect the newer records\n",
    "   - Update the 2024-02 summary rows\n",
    "   - Rebuild the rolling history arrays for all affected accounts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.5\n",
      "Session started at: 2026-01-21 16:38:23.344785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 16:38:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# Create Spark session with Iceberg support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BackfillSimulation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Session started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check Current State (BEFORE Backfill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CURRENT TABLE COUNTS\n",
      "============================================================\n",
      "accounts_all:   5,975 records\n",
      "summary:        5,950 records\n",
      "latest_summary: 1,000 records\n"
     ]
    }
   ],
   "source": [
    "# Check table counts\n",
    "print(\"=\" * 60)\n",
    "print(\"CURRENT TABLE COUNTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accounts_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.default.accounts_all\").collect()[0]['cnt']\n",
    "summary_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.summary\").collect()[0]['cnt']\n",
    "latest_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.latest_summary\").collect()[0]['cnt']\n",
    "\n",
    "print(f\"accounts_all:   {accounts_count:,} records\")\n",
    "print(f\"summary:        {summary_count:,} records\")\n",
    "print(f\"latest_summary: {latest_count:,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RECORDS BY MONTH\n",
      "============================================================\n",
      "+-------+---------------+\n",
      "|  month|summary_records|\n",
      "+-------+---------------+\n",
      "|2024-01|           1000|\n",
      "|2024-02|           1000|\n",
      "|2024-03|            990|\n",
      "|2024-04|            981|\n",
      "|2024-05|            979|\n",
      "|2024-06|           1000|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check records by month\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECORDS BY MONTH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT rpt_as_of_mo as month, COUNT(*) as summary_records\n",
    "    FROM default.summary\n",
    "    GROUP BY rpt_as_of_mo\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Select Accounts for Backfill Simulation\n",
    "\n",
    "We'll pick 5 accounts to demonstrate the backfill. Let's see their current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accounts selected for backfill: [1, 2, 3, 4, 5]\n",
      "Month to backfill: 2024-03\n"
     ]
    }
   ],
   "source": [
    "# Define accounts to backfill (accounts 1-5)\n",
    "BACKFILL_ACCOUNTS = [1, 2, 3, 4, 5]\n",
    "BACKFILL_MONTH = '2024-03'  # We'll backfill March 2024\n",
    "\n",
    "print(f\"Accounts selected for backfill: {BACKFILL_ACCOUNTS}\")\n",
    "print(f\"Month to backfill: {BACKFILL_MONTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BEFORE BACKFILL - State for 2024-03\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cons_acct_key</th>\n",
       "      <th>rpt_as_of_mo</th>\n",
       "      <th>past_due_am</th>\n",
       "      <th>days_past_due</th>\n",
       "      <th>balance_am</th>\n",
       "      <th>payment_history_grid</th>\n",
       "      <th>past_due_latest</th>\n",
       "      <th>dpd_latest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>96266</td>\n",
       "      <td>118</td>\n",
       "      <td>192533</td>\n",
       "      <td>300?????????????????????????????????</td>\n",
       "      <td>96266</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>5080</td>\n",
       "      <td>3</td>\n",
       "      <td>169342</td>\n",
       "      <td>000?????????????????????????????????</td>\n",
       "      <td>5080</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>102673</td>\n",
       "      <td>61</td>\n",
       "      <td>205346</td>\n",
       "      <td>200?????????????????????????????????</td>\n",
       "      <td>102673</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>182081</td>\n",
       "      <td>161</td>\n",
       "      <td>364163</td>\n",
       "      <td>554?????????????????????????????????</td>\n",
       "      <td>182081</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>43498</td>\n",
       "      <td>37</td>\n",
       "      <td>117564</td>\n",
       "      <td>100?????????????????????????????????</td>\n",
       "      <td>43498</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cons_acct_key rpt_as_of_mo  past_due_am  days_past_due  balance_am  \\\n",
       "0              1      2024-03        96266            118      192533   \n",
       "1              2      2024-03         5080              3      169342   \n",
       "2              3      2024-03       102673             61      205346   \n",
       "3              4      2024-03       182081            161      364163   \n",
       "4              5      2024-03        43498             37      117564   \n",
       "\n",
       "                   payment_history_grid  past_due_latest  dpd_latest  \n",
       "0  300?????????????????????????????????            96266         118  \n",
       "1  000?????????????????????????????????             5080           3  \n",
       "2  200?????????????????????????????????           102673          61  \n",
       "3  554?????????????????????????????????           182081         161  \n",
       "4  100?????????????????????????????????            43498          37  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store BEFORE state for comparison\n",
    "before_state = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due,\n",
    "        balance_am,\n",
    "        payment_history_grid,\n",
    "        past_due_am_history[0] as past_due_latest,\n",
    "        days_past_due_history[0] as dpd_latest\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"BEFORE BACKFILL - State for {BACKFILL_MONTH}\")\n",
    "print(\"=\" * 80)\n",
    "before_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current source data timestamps:\n",
      "+-------------+------------+-----------+-------------+--------------------------+\n",
      "|cons_acct_key|rpt_as_of_mo|past_due_am|days_past_due|base_ts                   |\n",
      "+-------------+------------+-----------+-------------+--------------------------+\n",
      "|1            |2024-03     |96266      |118          |2025-09-23 12:41:34.009371|\n",
      "|2            |2024-03     |5080       |3            |2025-09-23 12:41:34.009371|\n",
      "|3            |2024-03     |102673     |61           |2025-09-23 12:41:34.009371|\n",
      "|4            |2024-03     |182081     |161          |2025-09-23 12:41:34.009371|\n",
      "|5            |2024-03     |43498      |37           |2025-09-23 12:41:34.009371|\n",
      "+-------------+------------+-----------+-------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Also check current source data timestamps\n",
    "print(\"\\nCurrent source data timestamps:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due_ct_4in as days_past_due,\n",
    "        base_ts\n",
    "    FROM default.default.accounts_all\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key, base_ts\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Insert Late-Arriving Data\n",
    "\n",
    "Now we'll simulate late-arriving data by inserting new records with:\n",
    "- **Newer timestamp** (current time)\n",
    "- **Modified values** (increased past_due and days_past_due)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes to apply:\n",
      "  - past_due_am: +10,000\n",
      "  - days_past_due: +30\n"
     ]
    }
   ],
   "source": [
    "# Define the changes we'll make\n",
    "PAST_DUE_INCREASE = 10000  # Add 10,000 to past_due_am\n",
    "DPD_INCREASE = 30          # Add 30 days to days_past_due\n",
    "\n",
    "print(f\"Changes to apply:\")\n",
    "print(f\"  - past_due_am: +{PAST_DUE_INCREASE:,}\")\n",
    "print(f\"  - days_past_due: +{DPD_INCREASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting late-arriving records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 16:39:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done!\n"
     ]
    }
   ],
   "source": [
    "# Insert late-arriving records\n",
    "insert_sql = f\"\"\"\n",
    "INSERT INTO default.default.accounts_all\n",
    "SELECT \n",
    "    cons_acct_key,\n",
    "    bureau_mbr_id,\n",
    "    port_type_cd,\n",
    "    acct_type_dtl_cd,\n",
    "    pymt_terms_cd,\n",
    "    pymt_terms_dtl_cd,\n",
    "    acct_open_dt,\n",
    "    acct_closed_dt,\n",
    "    acct_dt,\n",
    "    last_pymt_dt,\n",
    "    schd_pymt_dt,\n",
    "    orig_pymt_due_dt,\n",
    "    write_off_dt,\n",
    "    acct_stat_cd,\n",
    "    acct_pymt_stat_cd,\n",
    "    acct_pymt_stat_dtl_cd,\n",
    "    acct_credit_ext_am,\n",
    "    acct_bal_am,\n",
    "    past_due_am + {PAST_DUE_INCREASE} as past_due_am,\n",
    "    actual_pymt_am,\n",
    "    next_schd_pymt_am,\n",
    "    write_off_am,\n",
    "    asset_class_cd_4in,\n",
    "    days_past_due_ct_4in + {DPD_INCREASE} as days_past_due_ct_4in,\n",
    "    high_credit_am_4in,\n",
    "    cash_limit_am_4in,\n",
    "    collateral_am_4in,\n",
    "    total_write_off_am_4in,\n",
    "    principal_write_off_am_4in,\n",
    "    settled_am_4in,\n",
    "    interest_rate_4in,\n",
    "    suit_filed_wilful_def_stat_cd_4in,\n",
    "    wo_settled_stat_cd_4in,\n",
    "    collateral_cd,\n",
    "    rpt_as_of_mo,\n",
    "    current_timestamp() as base_ts\n",
    "FROM default.default.accounts_all\n",
    "WHERE rpt_as_of_mo = '{BACKFILL_MONTH}' \n",
    "  AND cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "\"\"\"\n",
    "\n",
    "print(\"Inserting late-arriving records...\")\n",
    "spark.sql(insert_sql)\n",
    "print(\"‚úÖ Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source data after insertion (showing both old and new records):\n",
      "+-------------+------------+-----------+-------------+--------------------------+---------------------+\n",
      "|cons_acct_key|rpt_as_of_mo|past_due_am|days_past_due|base_ts                   |record_type          |\n",
      "+-------------+------------+-----------+-------------+--------------------------+---------------------+\n",
      "|1            |2024-03     |96266      |118          |2025-09-23 12:41:34.009371|(original)           |\n",
      "|1            |2024-03     |106266     |148          |2026-01-21 16:39:49.090413|‚Üê NEW (late-arriving)|\n",
      "|2            |2024-03     |5080       |3            |2025-09-23 12:41:34.009371|(original)           |\n",
      "|2            |2024-03     |15080      |33           |2026-01-21 16:39:49.090413|‚Üê NEW (late-arriving)|\n",
      "|3            |2024-03     |102673     |61           |2025-09-23 12:41:34.009371|(original)           |\n",
      "|3            |2024-03     |112673     |91           |2026-01-21 16:39:49.090413|‚Üê NEW (late-arriving)|\n",
      "|4            |2024-03     |182081     |161          |2025-09-23 12:41:34.009371|(original)           |\n",
      "|4            |2024-03     |192081     |191          |2026-01-21 16:39:49.090413|‚Üê NEW (late-arriving)|\n",
      "|5            |2024-03     |43498      |37           |2025-09-23 12:41:34.009371|(original)           |\n",
      "|5            |2024-03     |53498      |67           |2026-01-21 16:39:49.090413|‚Üê NEW (late-arriving)|\n",
      "+-------------+------------+-----------+-------------+--------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the new records were inserted\n",
    "print(\"\\nSource data after insertion (showing both old and new records):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due_ct_4in as days_past_due,\n",
    "        base_ts,\n",
    "        CASE \n",
    "            WHEN base_ts > timestamp'2026-01-21 15:00:00' THEN '‚Üê NEW (late-arriving)'\n",
    "            ELSE '(original)'\n",
    "        END as record_type\n",
    "    FROM default.default.accounts_all\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key, base_ts\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Backfill Pipeline\n",
    "\n",
    "Now we'll run the backfill to process the late-arriving data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     13\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 15\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mSummaryConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig loaded from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39msource_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/iceberg/notebooks/notebooks/summary_v4/summary_pipeline.py:71\u001b[0m, in \u001b[0;36mSummaryConfig.__init__\u001b[0;34m(self, config_path)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Core tables\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not dict"
     ]
    }
   ],
   "source": [
    "# Import and run the pipeline\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add the summary_v4 directory to path\n",
    "sys.path.insert(0, '/home/iceberg/notebooks/notebooks/summary_v4')\n",
    "\n",
    "from summary_pipeline import SummaryConfig, SummaryPipeline\n",
    "\n",
    "# Load config\n",
    "config_path = '/home/iceberg/notebooks/notebooks/summary_v4/summary_config.json'\n",
    "with open(config_path) as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "config = SummaryConfig(config_dict)\n",
    "print(f\"Config loaded from: {config_path}\")\n",
    "print(f\"Source table: {config.source_table}\")\n",
    "print(f\"Destination table: {config.destination_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline instance\n",
    "pipeline = SummaryPipeline(spark, config)\n",
    "\n",
    "print(f\"\\nRunning backfill for month: {BACKFILL_MONTH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the backfill\n",
    "from datetime import date\n",
    "\n",
    "# Parse month\n",
    "year, month = map(int, BACKFILL_MONTH.split('-'))\n",
    "start_date = date(year, month, 1)\n",
    "end_date = date(year, month, 1)\n",
    "\n",
    "print(f\"Starting backfill processing...\")\n",
    "print(f\"Date range: {start_date} to {end_date}\")\n",
    "print()\n",
    "\n",
    "# Run backfill\n",
    "pipeline.run_backfill(start_date, end_date)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ BACKFILL COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Check State AFTER Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AFTER state\n",
    "after_state = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due,\n",
    "        balance_am,\n",
    "        payment_history_grid,\n",
    "        past_due_am_history[0] as past_due_latest,\n",
    "        days_past_due_history[0] as dpd_latest\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"AFTER BACKFILL - State for {BACKFILL_MONTH}\")\n",
    "print(\"=\" * 80)\n",
    "after_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare BEFORE vs AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.merge(\n",
    "    before_state[['cons_acct_key', 'past_due_am', 'days_past_due', 'payment_history_grid']],\n",
    "    after_state[['cons_acct_key', 'past_due_am', 'days_past_due', 'payment_history_grid']],\n",
    "    on='cons_acct_key',\n",
    "    suffixes=('_BEFORE', '_AFTER')\n",
    ")\n",
    "\n",
    "# Calculate differences\n",
    "comparison['past_due_DIFF'] = comparison['past_due_am_AFTER'] - comparison['past_due_am_BEFORE']\n",
    "comparison['dpd_DIFF'] = comparison['days_past_due_AFTER'] - comparison['days_past_due_BEFORE']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPARISON: BEFORE vs AFTER BACKFILL\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nExpected changes: past_due +{PAST_DUE_INCREASE:,}, days_past_due +{DPD_INCREASE}\")\n",
    "print()\n",
    "\n",
    "comparison[['cons_acct_key', 'past_due_am_BEFORE', 'past_due_am_AFTER', 'past_due_DIFF', \n",
    "            'days_past_due_BEFORE', 'days_past_due_AFTER', 'dpd_DIFF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes match expected\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_past_due_correct = (comparison['past_due_DIFF'] == PAST_DUE_INCREASE).all()\n",
    "all_dpd_correct = (comparison['dpd_DIFF'] == DPD_INCREASE).all()\n",
    "\n",
    "print(f\"\\npast_due_am increased by {PAST_DUE_INCREASE:,} for all accounts: \", end=\"\")\n",
    "print(\"‚úÖ YES\" if all_past_due_correct else \"‚ùå NO\")\n",
    "\n",
    "print(f\"days_past_due increased by {DPD_INCREASE} for all accounts: \", end=\"\")\n",
    "print(\"‚úÖ YES\" if all_dpd_correct else \"‚ùå NO\")\n",
    "\n",
    "if all_past_due_correct and all_dpd_correct:\n",
    "    print(\"\\n\" + \"üéâ\" * 20)\n",
    "    print(\"BACKFILL SIMULATION SUCCESSFUL!\")\n",
    "    print(\"üéâ\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Check Impact on Future Months\n",
    "\n",
    "The backfill should also update the rolling history arrays for months AFTER the backfilled month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the history arrays look for later months\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ROLLING HISTORY ARRAYS - Account 1\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        rpt_as_of_mo,\n",
    "        payment_history_grid,\n",
    "        slice(past_due_am_history, 1, 6) as past_due_6mo,\n",
    "        slice(days_past_due_history, 1, 6) as dpd_6mo\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key = 1\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final table counts\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL TABLE COUNTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accounts_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.default.accounts_all\").collect()[0]['cnt']\n",
    "summary_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.summary\").collect()[0]['cnt']\n",
    "latest_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.latest_summary\").collect()[0]['cnt']\n",
    "\n",
    "print(f\"accounts_all:   {accounts_count:,} records (includes new late-arriving records)\")\n",
    "print(f\"summary:        {summary_count:,} records (unchanged count, but values updated)\")\n",
    "print(f\"latest_summary: {latest_count:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Before State**: Showed the original data for selected accounts\n",
    "2. **Late-Arriving Data**: Inserted new records with newer timestamps and modified values\n",
    "3. **Backfill Execution**: Ran the pipeline in backfill mode\n",
    "4. **After State**: Verified the summary was updated with the new values\n",
    "5. **Comparison**: Confirmed the exact changes were applied\n",
    "\n",
    "### Key Points:\n",
    "- The pipeline uses `base_ts` (timestamp) to determine which record is the \"winner\"\n",
    "- Newer records override older records for the same account/month\n",
    "- Rolling history arrays are rebuilt to reflect the updated values\n",
    "- The summary table record count stays the same (update, not insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional - uncomment to run)\n",
    "# spark.stop()\n",
    "# print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
