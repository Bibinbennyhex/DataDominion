{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Pipeline v4.0 - Data Validation Notebook\n",
    "\n",
    "This notebook allows you to explore and validate the summary pipeline results.\n",
    "\n",
    "**Tables:**\n",
    "- `default.default.accounts_all` - Source data\n",
    "- `default.summary` - Summary with rolling history arrays\n",
    "- `default.latest_summary` - Latest state per account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 13:03:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SummaryValidation\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Table Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TABLE OVERVIEW\n",
      "============================================================\n",
      "Source (accounts_all):  5,950 rows\n",
      "Summary:                5,950 rows\n",
      "Latest Summary:         1,000 rows\n"
     ]
    }
   ],
   "source": [
    "# Check table counts\n",
    "print(\"=\" * 60)\n",
    "print(\"TABLE OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accounts_count = spark.table(\"default.default.accounts_all\").count()\n",
    "summary_count = spark.table(\"default.summary\").count()\n",
    "latest_count = spark.table(\"default.latest_summary\").count()\n",
    "\n",
    "print(f\"Source (accounts_all):  {accounts_count:,} rows\")\n",
    "print(f\"Summary:                {summary_count:,} rows\")\n",
    "print(f\"Latest Summary:         {latest_count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check months in data\n",
    "print(\"\\nMonths in accounts_all:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT rpt_as_of_mo, COUNT(*) as record_count \n",
    "    FROM default.default.accounts_all \n",
    "    GROUP BY rpt_as_of_mo \n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check months in summary\n",
    "print(\"\\nMonths in summary:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT rpt_as_of_mo, COUNT(*) as record_count \n",
    "    FROM default.summary \n",
    "    GROUP BY rpt_as_of_mo \n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table schema\n",
    "print(\"Summary Table Schema:\")\n",
    "print(\"=\" * 60)\n",
    "spark.table(\"default.summary\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest summary schema\n",
    "print(\"Latest Summary Schema:\")\n",
    "print(\"=\" * 60)\n",
    "spark.table(\"default.latest_summary\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample account to explore\n",
    "sample_account = spark.sql(\"\"\"\n",
    "    SELECT cons_acct_key \n",
    "    FROM default.summary \n",
    "    GROUP BY cons_acct_key \n",
    "    HAVING COUNT(*) = 6 \n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0][0]\n",
    "\n",
    "print(f\"Sample account with 6 months of data: {sample_account}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View source data for this account\n",
    "print(f\"\\nSource data for account {sample_account}:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        acct_bal_am as balance,\n",
    "        days_past_due_ct_4in as dpd,\n",
    "        actual_pymt_am as payment,\n",
    "        asset_class_cd_4in as asset_class,\n",
    "        base_ts\n",
    "    FROM default.default.accounts_all\n",
    "    WHERE cons_acct_key = {sample_account}\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View summary data for this account\n",
    "print(f\"\\nSummary data for account {sample_account}:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        balance_am,\n",
    "        days_past_due,\n",
    "        payment_history_grid\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key = {sample_account}\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View rolling history arrays for this account\n",
    "print(f\"\\nRolling history arrays for account {sample_account} (latest month):\")\n",
    "df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        balance_am_history,\n",
    "        days_past_due_history,\n",
    "        payment_rating_cd_history\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key = {sample_account}\n",
    "    ORDER BY rpt_as_of_mo DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "row = df.collect()[0]\n",
    "print(f\"Account: {row['cons_acct_key']}\")\n",
    "print(f\"Month: {row['rpt_as_of_mo']}\")\n",
    "print(f\"\\nBalance History (first 12): {row['balance_am_history'][:12]}\")\n",
    "print(f\"DPD History (first 12):     {row['days_past_due_history'][:12]}\")\n",
    "print(f\"Rating History (first 12):  {row['payment_rating_cd_history'][:12]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1: No duplicates\n",
    "print(\"CHECK 1: Duplicate Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "duplicates = spark.sql(\"\"\"\n",
    "    SELECT cons_acct_key, rpt_as_of_mo, COUNT(*) as cnt\n",
    "    FROM default.summary\n",
    "    GROUP BY cons_acct_key, rpt_as_of_mo\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "\n",
    "dup_count = duplicates.count()\n",
    "if dup_count == 0:\n",
    "    print(\"PASSED: No duplicate (account, month) combinations\")\n",
    "else:\n",
    "    print(f\"FAILED: Found {dup_count} duplicates\")\n",
    "    duplicates.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2: Array lengths\n",
    "print(\"\\nCHECK 2: Array Length Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "array_check = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'balance_am_history' as column_name,\n",
    "        COUNT(*) as total_rows,\n",
    "        SUM(CASE WHEN SIZE(balance_am_history) = 36 THEN 1 ELSE 0 END) as correct_length,\n",
    "        SUM(CASE WHEN SIZE(balance_am_history) != 36 THEN 1 ELSE 0 END) as wrong_length\n",
    "    FROM default.summary\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'days_past_due_history',\n",
    "        COUNT(*),\n",
    "        SUM(CASE WHEN SIZE(days_past_due_history) = 36 THEN 1 ELSE 0 END),\n",
    "        SUM(CASE WHEN SIZE(days_past_due_history) != 36 THEN 1 ELSE 0 END)\n",
    "    FROM default.summary\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'payment_rating_cd_history',\n",
    "        COUNT(*),\n",
    "        SUM(CASE WHEN SIZE(payment_rating_cd_history) = 36 THEN 1 ELSE 0 END),\n",
    "        SUM(CASE WHEN SIZE(payment_rating_cd_history) != 36 THEN 1 ELSE 0 END)\n",
    "    FROM default.summary\n",
    "\"\"\")\n",
    "\n",
    "array_check.show(truncate=False)\n",
    "\n",
    "wrong = array_check.agg(F.sum(\"wrong_length\")).collect()[0][0]\n",
    "if wrong == 0:\n",
    "    print(\"PASSED: All arrays have correct length (36)\")\n",
    "else:\n",
    "    print(f\"FAILED: {wrong} rows have incorrect array lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 3: Latest summary consistency\n",
    "print(\"\\nCHECK 3: Latest Summary Consistency\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "consistency = spark.sql(\"\"\"\n",
    "    WITH max_months AS (\n",
    "        SELECT cons_acct_key, MAX(rpt_as_of_mo) as max_month\n",
    "        FROM default.summary\n",
    "        GROUP BY cons_acct_key\n",
    "    )\n",
    "    SELECT \n",
    "        COUNT(*) as total_accounts,\n",
    "        SUM(CASE WHEN l.cons_acct_key IS NOT NULL THEN 1 ELSE 0 END) as in_latest,\n",
    "        SUM(CASE WHEN l.cons_acct_key IS NULL THEN 1 ELSE 0 END) as missing_from_latest\n",
    "    FROM max_months m\n",
    "    LEFT JOIN default.latest_summary l ON m.cons_acct_key = l.cons_acct_key\n",
    "\"\"\")\n",
    "\n",
    "consistency.show()\n",
    "\n",
    "missing = consistency.collect()[0]['missing_from_latest']\n",
    "if missing == 0:\n",
    "    print(\"PASSED: All accounts have entries in latest_summary\")\n",
    "else:\n",
    "    print(f\"FAILED: {missing} accounts missing from latest_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 4: Grid length\n",
    "print(\"\\nCHECK 4: Payment History Grid Length\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "grid_check = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        LENGTH(payment_history_grid) as grid_length,\n",
    "        COUNT(*) as row_count\n",
    "    FROM default.summary\n",
    "    GROUP BY LENGTH(payment_history_grid)\n",
    "    ORDER BY grid_length\n",
    "\"\"\")\n",
    "\n",
    "grid_check.show()\n",
    "\n",
    "all_36 = grid_check.filter(\"grid_length = 36\").count() == grid_check.count()\n",
    "if all_36:\n",
    "    print(\"PASSED: All payment_history_grid values have length 36\")\n",
    "else:\n",
    "    print(\"WARNING: Some grids have unexpected lengths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. History Array Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify history shifts correctly over months\n",
    "print(\"History Array Shift Verification\")\n",
    "print(\"=\" * 60)\n",
    "print(\"For each month, index 0 should be current value, index 1 should be previous month, etc.\")\n",
    "print()\n",
    "\n",
    "# Get an account with multiple months\n",
    "history_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        rpt_as_of_mo,\n",
    "        balance_am_history[0] as current_balance,\n",
    "        balance_am_history[1] as prev_1_month,\n",
    "        balance_am_history[2] as prev_2_month,\n",
    "        balance_am_history[3] as prev_3_month,\n",
    "        balance_am_history[4] as prev_4_month,\n",
    "        balance_am_history[5] as prev_5_month\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key = {sample_account}\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Balance history for account {sample_account}:\")\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with source values\n",
    "print(\"\\nComparison with source values:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        a.rpt_as_of_mo,\n",
    "        a.acct_bal_am as source_balance,\n",
    "        s.balance_am_history[0] as history_current\n",
    "    FROM default.default.accounts_all a\n",
    "    JOIN default.summary s \n",
    "        ON a.cons_acct_key = s.cons_acct_key \n",
    "        AND a.rpt_as_of_mo = s.rpt_as_of_mo\n",
    "    WHERE a.cons_acct_key = {sample_account}\n",
    "    ORDER BY a.rpt_as_of_mo\n",
    "\"\"\")\n",
    "\n",
    "comparison.show(truncate=False)\n",
    "\n",
    "# Verify match\n",
    "mismatches = comparison.filter(\"source_balance != history_current\").count()\n",
    "if mismatches == 0:\n",
    "    print(\"PASSED: Source values match history[0] values\")\n",
    "else:\n",
    "    print(f\"FAILED: {mismatches} mismatches found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DPD and Payment Rating Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPD distribution\n",
    "print(\"DPD Distribution (current month values):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN days_past_due = 0 THEN '0 (Current)'\n",
    "            WHEN days_past_due BETWEEN 1 AND 29 THEN '1-29 DPD'\n",
    "            WHEN days_past_due BETWEEN 30 AND 59 THEN '30-59 DPD'\n",
    "            WHEN days_past_due BETWEEN 60 AND 89 THEN '60-89 DPD'\n",
    "            WHEN days_past_due BETWEEN 90 AND 119 THEN '90-119 DPD'\n",
    "            WHEN days_past_due BETWEEN 120 AND 179 THEN '120-179 DPD'\n",
    "            WHEN days_past_due >= 180 THEN '180+ DPD'\n",
    "            ELSE 'Unknown'\n",
    "        END as dpd_bucket,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pct\n",
    "    FROM default.summary\n",
    "    WHERE rpt_as_of_mo = (SELECT MAX(rpt_as_of_mo) FROM default.summary)\n",
    "    GROUP BY 1\n",
    "    ORDER BY \n",
    "        CASE \n",
    "            WHEN days_past_due = 0 THEN 0\n",
    "            WHEN days_past_due BETWEEN 1 AND 29 THEN 1\n",
    "            WHEN days_past_due BETWEEN 30 AND 59 THEN 2\n",
    "            WHEN days_past_due BETWEEN 60 AND 89 THEN 3\n",
    "            WHEN days_past_due BETWEEN 90 AND 119 THEN 4\n",
    "            WHEN days_past_due BETWEEN 120 AND 179 THEN 5\n",
    "            ELSE 6\n",
    "        END\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment rating distribution\n",
    "print(\"\\nPayment Rating Distribution (from grid, first character):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUBSTRING(payment_history_grid, 1, 1) as rating,\n",
    "        CASE SUBSTRING(payment_history_grid, 1, 1)\n",
    "            WHEN '0' THEN 'Current (0-29 DPD)'\n",
    "            WHEN '1' THEN '30-59 DPD'\n",
    "            WHEN '2' THEN '60-89 DPD'\n",
    "            WHEN '3' THEN '90-119 DPD'\n",
    "            WHEN '4' THEN '120-149 DPD'\n",
    "            WHEN '5' THEN '150-179 DPD'\n",
    "            WHEN '6' THEN '180+ DPD'\n",
    "            WHEN 'S' THEN 'Standard'\n",
    "            WHEN 'B' THEN 'Sub-Standard'\n",
    "            WHEN 'D' THEN 'Doubtful'\n",
    "            WHEN 'L' THEN 'Loss'\n",
    "            WHEN '?' THEN 'Unknown'\n",
    "            ELSE 'Other'\n",
    "        END as description,\n",
    "        COUNT(*) as count\n",
    "    FROM default.summary\n",
    "    WHERE rpt_as_of_mo = (SELECT MAX(rpt_as_of_mo) FROM default.summary)\n",
    "    GROUP BY 1, 2\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up any specific account\n",
    "# Change this value to explore different accounts\n",
    "ACCOUNT_TO_EXPLORE = 100\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"Account {ACCOUNT_TO_EXPLORE} Details\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Source data\n",
    "print(\"\\nSource Data:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        rpt_as_of_mo, \n",
    "        acct_bal_am as balance,\n",
    "        days_past_due_ct_4in as dpd,\n",
    "        actual_pymt_am as payment,\n",
    "        acct_stat_cd as status\n",
    "    FROM default.default.accounts_all \n",
    "    WHERE cons_acct_key = {ACCOUNT_TO_EXPLORE}\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary data for same account\n",
    "print(f\"\\nSummary Data for Account {ACCOUNT_TO_EXPLORE}:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        rpt_as_of_mo,\n",
    "        balance_am,\n",
    "        days_past_due,\n",
    "        payment_history_grid,\n",
    "        SLICE(balance_am_history, 1, 6) as balance_hist_first6,\n",
    "        SLICE(days_past_due_history, 1, 6) as dpd_hist_first6\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key = {ACCOUNT_TO_EXPLORE}\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest summary for same account\n",
    "print(f\"\\nLatest Summary for Account {ACCOUNT_TO_EXPLORE}:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM default.latest_summary\n",
    "    WHERE cons_acct_key = {ACCOUNT_TO_EXPLORE}\n",
    "\"\"\").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary stats\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stats = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(DISTINCT cons_acct_key) as unique_accounts,\n",
    "        COUNT(DISTINCT rpt_as_of_mo) as months,\n",
    "        MIN(rpt_as_of_mo) as first_month,\n",
    "        MAX(rpt_as_of_mo) as last_month,\n",
    "        AVG(balance_am) as avg_balance,\n",
    "        AVG(days_past_due) as avg_dpd\n",
    "    FROM default.summary\n",
    "\"\"\")\n",
    "\n",
    "stats.show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accounts with gaps\n",
    "print(\"\\nAccounts with Missing Months (gaps):\")\n",
    "spark.sql(\"\"\"\n",
    "    WITH account_months AS (\n",
    "        SELECT \n",
    "            cons_acct_key,\n",
    "            COUNT(DISTINCT rpt_as_of_mo) as month_count,\n",
    "            MIN(rpt_as_of_mo) as first_month,\n",
    "            MAX(rpt_as_of_mo) as last_month\n",
    "        FROM default.summary\n",
    "        GROUP BY cons_acct_key\n",
    "    )\n",
    "    SELECT \n",
    "        month_count,\n",
    "        COUNT(*) as account_count\n",
    "    FROM account_months\n",
    "    GROUP BY month_count\n",
    "    ORDER BY month_count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Query Cell\n",
    "\n",
    "Use this cell to run your own queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your own query here\n",
    "# Example: Find accounts with DPD > 90\n",
    "\n",
    "your_query = \"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        days_past_due,\n",
    "        balance_am,\n",
    "        payment_history_grid\n",
    "    FROM default.summary\n",
    "    WHERE days_past_due > 90\n",
    "    ORDER BY days_past_due DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(your_query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session when done\n",
    "# spark.stop()\n",
    "print(\"Notebook complete! Uncomment spark.stop() to close the session.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
