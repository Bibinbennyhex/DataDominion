{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill Simulation - Summary Pipeline v4.0\n",
    "\n",
    "This notebook demonstrates the **backfill capability** of the Summary Pipeline v4.0.\n",
    "\n",
    "## What is Backfill?\n",
    "Backfill handles **late-arriving data** - when new records arrive for a historical month that was already processed.\n",
    "\n",
    "### Scenario\n",
    "1. We have processed months 2024-01 through 2024-06\n",
    "2. New data arrives for 2024-02 (with a newer timestamp)\n",
    "3. The pipeline must:\n",
    "   - Detect the newer records\n",
    "   - Update the 2024-02 summary rows\n",
    "   - Rebuild the rolling history arrays for all affected accounts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# Create Spark session with Iceberg support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BackfillSimulation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Session started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check Current State (BEFORE Backfill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table counts\n",
    "print(\"=\" * 60)\n",
    "print(\"CURRENT TABLE COUNTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accounts_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.default.accounts_all\").collect()[0]['cnt']\n",
    "summary_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.summary\").collect()[0]['cnt']\n",
    "latest_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.latest_summary\").collect()[0]['cnt']\n",
    "\n",
    "print(f\"accounts_all:   {accounts_count:,} records\")\n",
    "print(f\"summary:        {summary_count:,} records\")\n",
    "print(f\"latest_summary: {latest_count:,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check records by month\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECORDS BY MONTH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT rpt_as_of_mo as month, COUNT(*) as summary_records\n",
    "    FROM default.summary\n",
    "    GROUP BY rpt_as_of_mo\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Select Accounts for Backfill Simulation\n",
    "\n",
    "We'll pick 5 accounts to demonstrate the backfill. Let's see their current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define accounts to backfill (accounts 1-5)\n",
    "BACKFILL_ACCOUNTS = [1, 2, 3, 4, 5]\n",
    "BACKFILL_MONTH = '2024-03'  # We'll backfill March 2024\n",
    "\n",
    "print(f\"Accounts selected for backfill: {BACKFILL_ACCOUNTS}\")\n",
    "print(f\"Month to backfill: {BACKFILL_MONTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store BEFORE state for comparison\n",
    "before_state = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due,\n",
    "        balance_am,\n",
    "        payment_history_grid,\n",
    "        past_due_am_history[0] as past_due_latest,\n",
    "        days_past_due_history[0] as dpd_latest\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"BEFORE BACKFILL - State for {BACKFILL_MONTH}\")\n",
    "print(\"=\" * 80)\n",
    "before_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check current source data timestamps\n",
    "print(\"\\nCurrent source data timestamps:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due_ct_4in as days_past_due,\n",
    "        base_ts\n",
    "    FROM default.default.accounts_all\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key, base_ts\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Insert Late-Arriving Data\n",
    "\n",
    "Now we'll simulate late-arriving data by inserting new records with:\n",
    "- **Newer timestamp** (current time)\n",
    "- **Modified values** (increased past_due and days_past_due)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the changes we'll make\n",
    "PAST_DUE_INCREASE = 10000  # Add 10,000 to past_due_am\n",
    "DPD_INCREASE = 30          # Add 30 days to days_past_due\n",
    "\n",
    "print(f\"Changes to apply:\")\n",
    "print(f\"  - past_due_am: +{PAST_DUE_INCREASE:,}\")\n",
    "print(f\"  - days_past_due: +{DPD_INCREASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert late-arriving records\n",
    "insert_sql = f\"\"\"\n",
    "INSERT INTO default.default.accounts_all\n",
    "SELECT \n",
    "    cons_acct_key,\n",
    "    bureau_mbr_id,\n",
    "    port_type_cd,\n",
    "    acct_type_dtl_cd,\n",
    "    pymt_terms_cd,\n",
    "    pymt_terms_dtl_cd,\n",
    "    acct_open_dt,\n",
    "    acct_closed_dt,\n",
    "    acct_dt,\n",
    "    last_pymt_dt,\n",
    "    schd_pymt_dt,\n",
    "    orig_pymt_due_dt,\n",
    "    write_off_dt,\n",
    "    acct_stat_cd,\n",
    "    acct_pymt_stat_cd,\n",
    "    acct_pymt_stat_dtl_cd,\n",
    "    acct_credit_ext_am,\n",
    "    acct_bal_am,\n",
    "    past_due_am + {PAST_DUE_INCREASE} as past_due_am,\n",
    "    actual_pymt_am,\n",
    "    next_schd_pymt_am,\n",
    "    write_off_am,\n",
    "    asset_class_cd_4in,\n",
    "    days_past_due_ct_4in + {DPD_INCREASE} as days_past_due_ct_4in,\n",
    "    high_credit_am_4in,\n",
    "    cash_limit_am_4in,\n",
    "    collateral_am_4in,\n",
    "    total_write_off_am_4in,\n",
    "    principal_write_off_am_4in,\n",
    "    settled_am_4in,\n",
    "    interest_rate_4in,\n",
    "    suit_filed_wilful_def_stat_cd_4in,\n",
    "    wo_settled_stat_cd_4in,\n",
    "    collateral_cd,\n",
    "    rpt_as_of_mo,\n",
    "    current_timestamp() as base_ts\n",
    "FROM default.default.accounts_all\n",
    "WHERE rpt_as_of_mo = '{BACKFILL_MONTH}' \n",
    "  AND cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "\"\"\"\n",
    "\n",
    "print(\"Inserting late-arriving records...\")\n",
    "spark.sql(insert_sql)\n",
    "print(\"‚úÖ Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the new records were inserted\n",
    "print(\"\\nSource data after insertion (showing both old and new records):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due_ct_4in as days_past_due,\n",
    "        base_ts,\n",
    "        CASE \n",
    "            WHEN base_ts > timestamp'2026-01-21 15:00:00' THEN '‚Üê NEW (late-arriving)'\n",
    "            ELSE '(original)'\n",
    "        END as record_type\n",
    "    FROM default.default.accounts_all\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key, base_ts\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Backfill Pipeline\n",
    "\n",
    "Now we'll run the backfill to process the late-arriving data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and run the pipeline\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add the summary_v4 directory to path\n",
    "sys.path.insert(0, '/home/iceberg/notebooks/notebooks/summary_v4')\n",
    "\n",
    "from summary_pipeline import SummaryConfig, SummaryPipeline\n",
    "\n",
    "# Load config\n",
    "config_path = '/home/iceberg/notebooks/notebooks/summary_v4/summary_config.json'\n",
    "with open(config_path) as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "config = SummaryConfig(config_dict)\n",
    "print(f\"Config loaded from: {config_path}\")\n",
    "print(f\"Source table: {config.source_table}\")\n",
    "print(f\"Destination table: {config.destination_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline instance\n",
    "pipeline = SummaryPipeline(spark, config)\n",
    "\n",
    "print(f\"\\nRunning backfill for month: {BACKFILL_MONTH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the backfill\n",
    "from datetime import date\n",
    "\n",
    "# Parse month\n",
    "year, month = map(int, BACKFILL_MONTH.split('-'))\n",
    "start_date = date(year, month, 1)\n",
    "end_date = date(year, month, 1)\n",
    "\n",
    "print(f\"Starting backfill processing...\")\n",
    "print(f\"Date range: {start_date} to {end_date}\")\n",
    "print()\n",
    "\n",
    "# Run backfill\n",
    "pipeline.run_backfill(start_date, end_date)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ BACKFILL COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Check State AFTER Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AFTER state\n",
    "after_state = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due,\n",
    "        balance_am,\n",
    "        payment_history_grid,\n",
    "        past_due_am_history[0] as past_due_latest,\n",
    "        days_past_due_history[0] as dpd_latest\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"AFTER BACKFILL - State for {BACKFILL_MONTH}\")\n",
    "print(\"=\" * 80)\n",
    "after_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare BEFORE vs AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.merge(\n",
    "    before_state[['cons_acct_key', 'past_due_am', 'days_past_due', 'payment_history_grid']],\n",
    "    after_state[['cons_acct_key', 'past_due_am', 'days_past_due', 'payment_history_grid']],\n",
    "    on='cons_acct_key',\n",
    "    suffixes=('_BEFORE', '_AFTER')\n",
    ")\n",
    "\n",
    "# Calculate differences\n",
    "comparison['past_due_DIFF'] = comparison['past_due_am_AFTER'] - comparison['past_due_am_BEFORE']\n",
    "comparison['dpd_DIFF'] = comparison['days_past_due_AFTER'] - comparison['days_past_due_BEFORE']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPARISON: BEFORE vs AFTER BACKFILL\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nExpected changes: past_due +{PAST_DUE_INCREASE:,}, days_past_due +{DPD_INCREASE}\")\n",
    "print()\n",
    "\n",
    "comparison[['cons_acct_key', 'past_due_am_BEFORE', 'past_due_am_AFTER', 'past_due_DIFF', \n",
    "            'days_past_due_BEFORE', 'days_past_due_AFTER', 'dpd_DIFF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes match expected\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_past_due_correct = (comparison['past_due_DIFF'] == PAST_DUE_INCREASE).all()\n",
    "all_dpd_correct = (comparison['dpd_DIFF'] == DPD_INCREASE).all()\n",
    "\n",
    "print(f\"\\npast_due_am increased by {PAST_DUE_INCREASE:,} for all accounts: \", end=\"\")\n",
    "print(\"‚úÖ YES\" if all_past_due_correct else \"‚ùå NO\")\n",
    "\n",
    "print(f\"days_past_due increased by {DPD_INCREASE} for all accounts: \", end=\"\")\n",
    "print(\"‚úÖ YES\" if all_dpd_correct else \"‚ùå NO\")\n",
    "\n",
    "if all_past_due_correct and all_dpd_correct:\n",
    "    print(\"\\n\" + \"üéâ\" * 20)\n",
    "    print(\"BACKFILL SIMULATION SUCCESSFUL!\")\n",
    "    print(\"üéâ\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Check Impact on Future Months\n",
    "\n",
    "The backfill should also update the rolling history arrays for months AFTER the backfilled month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the history arrays look for later months\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ROLLING HISTORY ARRAYS - Account 1\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        rpt_as_of_mo,\n",
    "        payment_history_grid,\n",
    "        slice(past_due_am_history, 1, 6) as past_due_6mo,\n",
    "        slice(days_past_due_history, 1, 6) as dpd_6mo\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key = 1\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final table counts\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL TABLE COUNTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accounts_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.default.accounts_all\").collect()[0]['cnt']\n",
    "summary_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.summary\").collect()[0]['cnt']\n",
    "latest_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.latest_summary\").collect()[0]['cnt']\n",
    "\n",
    "print(f\"accounts_all:   {accounts_count:,} records (includes new late-arriving records)\")\n",
    "print(f\"summary:        {summary_count:,} records (unchanged count, but values updated)\")\n",
    "print(f\"latest_summary: {latest_count:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Before State**: Showed the original data for selected accounts\n",
    "2. **Late-Arriving Data**: Inserted new records with newer timestamps and modified values\n",
    "3. **Backfill Execution**: Ran the pipeline in backfill mode\n",
    "4. **After State**: Verified the summary was updated with the new values\n",
    "5. **Comparison**: Confirmed the exact changes were applied\n",
    "\n",
    "### Key Points:\n",
    "- The pipeline uses `base_ts` (timestamp) to determine which record is the \"winner\"\n",
    "- Newer records override older records for the same account/month\n",
    "- Rolling history arrays are rebuilt to reflect the updated values\n",
    "- The summary table record count stays the same (update, not insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional - uncomment to run)\n",
    "# spark.stop()\n",
    "# print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
