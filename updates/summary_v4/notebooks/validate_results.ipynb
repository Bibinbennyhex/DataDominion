{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Pipeline v4.0 - Complete Validation Notebook\n",
    "\n",
    "This notebook allows you to explore and validate the summary pipeline results, covering:\n",
    "1. **Basic data integrity** - row counts, duplicates, array lengths\n",
    "2. **Gap scenarios** - accounts with missing months\n",
    "3. **Backfill scenarios** - late-arriving data handling\n",
    "4. **Rolling history correctness** - values shift properly\n",
    "\n",
    "**Tables:**\n",
    "- `default.default.accounts_all` - Source data\n",
    "- `default.summary` - Summary with rolling history arrays\n",
    "- `default.latest_summary` - Latest state per account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SummaryValidation\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Table Overview & Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table counts\n",
    "print(\"=\" * 70)\n",
    "print(\"TABLE OVERVIEW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "accounts_count = spark.table(\"default.default.accounts_all\").count()\n",
    "summary_count = spark.table(\"default.summary\").count()\n",
    "latest_count = spark.table(\"default.latest_summary\").count()\n",
    "\n",
    "print(f\"Source (accounts_all):  {accounts_count:,} rows\")\n",
    "print(f\"Summary:                {summary_count:,} rows\")\n",
    "print(f\"Latest Summary:         {latest_count:,} rows\")\n",
    "print(f\"\\nUnique accounts in summary: {spark.table('default.summary').select('cons_acct_key').distinct().count():,}\")\n",
    "print(f\"Unique months in summary:   {spark.table('default.summary').select('rpt_as_of_mo').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full schema check - verify all expected columns exist\n",
    "print(\"\\nSUMMARY TABLE SCHEMA:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_cols = [f.name for f in spark.table(\"default.summary\").schema.fields]\n",
    "\n",
    "# Expected columns\n",
    "expected_cols = [\n",
    "    # Key columns\n",
    "    'cons_acct_key', 'rpt_as_of_mo', 'base_ts',\n",
    "    # Scalar columns\n",
    "    'balance_am', 'days_past_due', 'actual_payment_am', 'credit_limit_am',\n",
    "    'past_due_am', 'asset_class_cd', 'emi_amt',\n",
    "    # History arrays\n",
    "    'balance_am_history', 'days_past_due_history', 'actual_payment_am_history',\n",
    "    'credit_limit_am_history', 'past_due_am_history', 'payment_rating_cd_history',\n",
    "    'asset_class_cd_history',\n",
    "    # Grid\n",
    "    'payment_history_grid'\n",
    "]\n",
    "\n",
    "print(f\"{'Column':<35} {'Present':<10} {'Type'}\")\n",
    "print(\"-\" * 70)\n",
    "for col in expected_cols:\n",
    "    present = \"YES\" if col in summary_cols else \"MISSING\"\n",
    "    col_type = str([f.dataType for f in spark.table(\"default.summary\").schema.fields if f.name == col][0]) if col in summary_cols else \"N/A\"\n",
    "    status = \"\" if present == \"YES\" else \" <-- FIX NEEDED\"\n",
    "    print(f\"{col:<35} {present:<10} {col_type}{status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data with key columns\n",
    "print(\"\\nSAMPLE DATA (key columns):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        balance_am,\n",
    "        days_past_due,\n",
    "        actual_payment_am,\n",
    "        asset_class_cd,\n",
    "        SUBSTRING(payment_history_grid, 1, 12) as grid_first12\n",
    "    FROM default.summary\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Gap Scenario Validation\n",
    "\n",
    "Test accounts that have **missing months** (gaps) in their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find accounts with gaps in source data\n",
    "print(\"=\" * 70)\n",
    "print(\"GAP SCENARIO ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get accounts with fewer months than expected\n",
    "gap_accounts = spark.sql(\"\"\"\n",
    "    WITH all_months AS (\n",
    "        SELECT DISTINCT rpt_as_of_mo FROM default.default.accounts_all ORDER BY rpt_as_of_mo\n",
    "    ),\n",
    "    account_months AS (\n",
    "        SELECT \n",
    "            cons_acct_key,\n",
    "            COUNT(DISTINCT rpt_as_of_mo) as month_count,\n",
    "            COLLECT_SET(rpt_as_of_mo) as months_present\n",
    "        FROM default.default.accounts_all\n",
    "        GROUP BY cons_acct_key\n",
    "    )\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        month_count,\n",
    "        (SELECT COUNT(*) FROM all_months) as total_months,\n",
    "        (SELECT COUNT(*) FROM all_months) - month_count as missing_months\n",
    "    FROM account_months\n",
    "    WHERE month_count < (SELECT COUNT(*) FROM all_months)\n",
    "    ORDER BY missing_months DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nAccounts with gaps (missing months in source):\")\n",
    "gap_accounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a gap account and analyze it\n",
    "gap_account_row = gap_accounts.first()\n",
    "if gap_account_row:\n",
    "    gap_account = gap_account_row['cons_acct_key']\n",
    "    print(f\"\\nAnalyzing gap account: {gap_account}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Source data for this account\n",
    "    print(\"\\nSource data (accounts_all):\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT rpt_as_of_mo, acct_bal_am as balance, days_past_due_ct_4in as dpd\n",
    "        FROM default.default.accounts_all\n",
    "        WHERE cons_acct_key = {gap_account}\n",
    "        ORDER BY rpt_as_of_mo\n",
    "    \"\"\").show()\n",
    "    \n",
    "    # Summary data for this account\n",
    "    print(\"\\nSummary data (with history arrays):\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            rpt_as_of_mo,\n",
    "            balance_am,\n",
    "            days_past_due,\n",
    "            SLICE(balance_am_history, 1, 8) as bal_hist_first8,\n",
    "            payment_history_grid\n",
    "        FROM default.summary\n",
    "        WHERE cons_acct_key = {gap_account}\n",
    "        ORDER BY rpt_as_of_mo\n",
    "    \"\"\").show(truncate=False)\n",
    "else:\n",
    "    print(\"No gap accounts found in source data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify gap handling - NULL values should appear in history for missing months\n",
    "print(\"\\nGAP HANDLING VERIFICATION:\")\n",
    "print(\"When a month is missing, the history array should have NULL at that position.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if gap_account_row:\n",
    "    # Get the history for the latest month\n",
    "    latest_hist = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            rpt_as_of_mo,\n",
    "            balance_am_history,\n",
    "            days_past_due_history\n",
    "        FROM default.summary\n",
    "        WHERE cons_acct_key = {gap_account}\n",
    "        ORDER BY rpt_as_of_mo DESC\n",
    "        LIMIT 1\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    bal_hist = latest_hist['balance_am_history']\n",
    "    dpd_hist = latest_hist['days_past_due_history']\n",
    "    \n",
    "    print(f\"\\nAccount {gap_account} - Latest month: {latest_hist['rpt_as_of_mo']}\")\n",
    "    print(f\"\\nBalance history (first 12 positions):\")\n",
    "    for i, val in enumerate(bal_hist[:12]):\n",
    "        marker = \"<-- NULL (gap)\" if val is None else \"\"\n",
    "        print(f\"  [{i}] {val} {marker}\")\n",
    "    \n",
    "    null_count = sum(1 for v in bal_hist if v is None)\n",
    "    print(f\"\\nTotal NULLs in balance history: {null_count} out of {len(bal_hist)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Backfill Scenario Test\n",
    "\n",
    "Simulate late-arriving data and test backfill processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's any backfill data (records with later base_ts for earlier months)\n",
    "print(\"=\" * 70)\n",
    "print(\"BACKFILL SCENARIO ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "backfill_candidates = spark.sql(\"\"\"\n",
    "    WITH ranked AS (\n",
    "        SELECT \n",
    "            cons_acct_key,\n",
    "            rpt_as_of_mo,\n",
    "            base_ts,\n",
    "            ROW_NUMBER() OVER (PARTITION BY cons_acct_key ORDER BY rpt_as_of_mo) as month_rank,\n",
    "            ROW_NUMBER() OVER (PARTITION BY cons_acct_key ORDER BY base_ts) as ts_rank\n",
    "        FROM default.default.accounts_all\n",
    "    )\n",
    "    SELECT cons_acct_key, COUNT(*) as out_of_order_count\n",
    "    FROM ranked\n",
    "    WHERE month_rank != ts_rank\n",
    "    GROUP BY cons_acct_key\n",
    "    ORDER BY out_of_order_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "backfill_count = backfill_candidates.count()\n",
    "print(f\"\\nAccounts with out-of-order timestamps (potential backfill): {backfill_count}\")\n",
    "if backfill_count > 0:\n",
    "    backfill_candidates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulated backfill scenario for testing\n",
    "print(\"\\nSIMULATED BACKFILL TEST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"We'll insert a new record for an old month and run backfill processing.\")\n",
    "print(\"\\nCurrent state before backfill:\")\n",
    "\n",
    "# Pick an account to test backfill\n",
    "test_account = spark.sql(\"\"\"\n",
    "    SELECT cons_acct_key \n",
    "    FROM default.summary \n",
    "    GROUP BY cons_acct_key \n",
    "    HAVING COUNT(*) >= 3\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0][0]\n",
    "\n",
    "print(f\"\\nTest account: {test_account}\")\n",
    "print(\"\\nCurrent summary history:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        rpt_as_of_mo,\n",
    "        balance_am,\n",
    "        SLICE(balance_am_history, 1, 6) as bal_hist\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key = {test_account}\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert backfill test data\n",
    "print(\"\\nInserting backfill test record...\")\n",
    "\n",
    "# Get an existing record to use as template\n",
    "template = spark.sql(f\"\"\"\n",
    "    SELECT * FROM default.default.accounts_all\n",
    "    WHERE cons_acct_key = {test_account}\n",
    "    ORDER BY rpt_as_of_mo\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "# Create backfill record for month 2024-02 with updated balance\n",
    "backfill_month = \"2024-02\"\n",
    "new_balance = 999999  # Distinctive value to identify\n",
    "\n",
    "print(f\"Creating backfill record for month {backfill_month} with balance {new_balance}\")\n",
    "print(\"(This would be inserted with a newer base_ts to simulate late arrival)\")\n",
    "\n",
    "# Show what the backfill record would look like\n",
    "print(f\"\"\"\n",
    "Backfill record:\n",
    "  cons_acct_key: {test_account}\n",
    "  rpt_as_of_mo:  {backfill_month}\n",
    "  balance:       {new_balance}\n",
    "  base_ts:       {datetime.now()} (newer than existing)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain backfill logic\n",
    "print(\"\\nBACKFILL PROCESSING LOGIC:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "When backfill mode runs, it:\n",
    "\n",
    "1. CASE I (New Account):\n",
    "   - Account doesn't exist in summary\n",
    "   - Creates new row with [value, NULL, NULL, ...]\n",
    "\n",
    "2. CASE II (Forward Entry):\n",
    "   - New month > max existing month\n",
    "   - Shifts history: [new_value, gap_nulls, old_history...]\n",
    "\n",
    "3. CASE III (Backfill - Historical Update):\n",
    "   - New month <= max existing month AND new.base_ts > existing.base_ts\n",
    "   - Rebuilds entire history for affected account\n",
    "   - Updates all future months' history arrays\n",
    "\n",
    "To test backfill manually:\n",
    "  1. Insert a record with an old month but new timestamp\n",
    "  2. Run: spark-submit summary_pipeline.py --config summary_config.json --mode backfill\n",
    "  3. Verify the history arrays are rebuilt correctly\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Rolling History Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify history shift is correct\n",
    "print(\"=\" * 70)\n",
    "print(\"ROLLING HISTORY SHIFT VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pick an account with full history\n",
    "full_account = spark.sql(\"\"\"\n",
    "    SELECT cons_acct_key \n",
    "    FROM default.summary \n",
    "    GROUP BY cons_acct_key \n",
    "    HAVING COUNT(*) = (SELECT COUNT(DISTINCT rpt_as_of_mo) FROM default.summary)\n",
    "    LIMIT 1\n",
    "\"\"\").collect()\n",
    "\n",
    "if full_account:\n",
    "    full_account = full_account[0][0]\n",
    "    print(f\"\\nAccount with all months: {full_account}\")\n",
    "    \n",
    "    # Get source values\n",
    "    source_df = spark.sql(f\"\"\"\n",
    "        SELECT rpt_as_of_mo, acct_bal_am as balance\n",
    "        FROM default.default.accounts_all\n",
    "        WHERE cons_acct_key = {full_account}\n",
    "        ORDER BY rpt_as_of_mo\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nSource values:\")\n",
    "    source_df.show()\n",
    "    \n",
    "    # Get summary history\n",
    "    print(\"\\nHistory arrays over time (showing how values shift):\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            rpt_as_of_mo,\n",
    "            balance_am_history[0] as h0_current,\n",
    "            balance_am_history[1] as h1_prev1,\n",
    "            balance_am_history[2] as h2_prev2,\n",
    "            balance_am_history[3] as h3_prev3,\n",
    "            balance_am_history[4] as h4_prev4,\n",
    "            balance_am_history[5] as h5_prev5\n",
    "        FROM default.summary\n",
    "        WHERE cons_acct_key = {full_account}\n",
    "        ORDER BY rpt_as_of_mo\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"No account found with all months. Using first available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify source matches history[0]\n",
    "print(\"\\nVALIDATION: Source value = history[0]\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "mismatch_check = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        a.cons_acct_key,\n",
    "        a.rpt_as_of_mo,\n",
    "        a.acct_bal_am as source_balance,\n",
    "        s.balance_am_history[0] as history_0,\n",
    "        CASE WHEN a.acct_bal_am = s.balance_am_history[0] THEN 'MATCH' ELSE 'MISMATCH' END as status\n",
    "    FROM default.default.accounts_all a\n",
    "    JOIN default.summary s \n",
    "        ON a.cons_acct_key = s.cons_acct_key \n",
    "        AND a.rpt_as_of_mo = s.rpt_as_of_mo\n",
    "    WHERE a.acct_bal_am != s.balance_am_history[0]\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "mismatch_count = mismatch_check.count()\n",
    "if mismatch_count == 0:\n",
    "    print(\"PASSED: All source values match history[0] values\")\n",
    "else:\n",
    "    print(f\"FAILED: {mismatch_count} mismatches found\")\n",
    "    mismatch_check.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify history[1] = previous month's history[0]\n",
    "print(\"\\nVALIDATION: history[1] should equal previous month's history[0]\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "shift_check = spark.sql(\"\"\"\n",
    "    WITH ordered AS (\n",
    "        SELECT \n",
    "            cons_acct_key,\n",
    "            rpt_as_of_mo,\n",
    "            balance_am_history[0] as current_h0,\n",
    "            balance_am_history[1] as current_h1,\n",
    "            LAG(balance_am_history[0]) OVER (PARTITION BY cons_acct_key ORDER BY rpt_as_of_mo) as prev_h0\n",
    "        FROM default.summary\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM ordered\n",
    "    WHERE prev_h0 IS NOT NULL  -- Skip first month\n",
    "      AND current_h1 != prev_h0\n",
    "      AND current_h1 IS NOT NULL  -- Allow for gaps\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "shift_errors = shift_check.count()\n",
    "if shift_errors == 0:\n",
    "    print(\"PASSED: History shift is correct (h[1] = prev month's h[0])\")\n",
    "else:\n",
    "    print(f\"FAILED: {shift_errors} shift errors found\")\n",
    "    shift_check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Array Length & Data Integrity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all array lengths\n",
    "print(\"=\" * 70)\n",
    "print(\"ARRAY LENGTH VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "array_columns = [\n",
    "    'balance_am_history',\n",
    "    'days_past_due_history', \n",
    "    'actual_payment_am_history',\n",
    "    'credit_limit_am_history',\n",
    "    'past_due_am_history',\n",
    "    'payment_rating_cd_history',\n",
    "    'asset_class_cd_history'\n",
    "]\n",
    "\n",
    "for col in array_columns:\n",
    "    result = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            '{col}' as column_name,\n",
    "            SUM(CASE WHEN SIZE({col}) = 36 THEN 1 ELSE 0 END) as correct,\n",
    "            SUM(CASE WHEN SIZE({col}) != 36 THEN 1 ELSE 0 END) as incorrect\n",
    "        FROM default.summary\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    status = \"PASS\" if result['incorrect'] == 0 else \"FAIL\"\n",
    "    print(f\"{col:<35} {status} (correct: {result['correct']}, incorrect: {result['incorrect']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"\\nDUPLICATE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "duplicates = spark.sql(\"\"\"\n",
    "    SELECT cons_acct_key, rpt_as_of_mo, COUNT(*) as cnt\n",
    "    FROM default.summary\n",
    "    GROUP BY cons_acct_key, rpt_as_of_mo\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\").count()\n",
    "\n",
    "if duplicates == 0:\n",
    "    print(\"PASS: No duplicate (account, month) combinations\")\n",
    "else:\n",
    "    print(f\"FAIL: {duplicates} duplicate combinations found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check payment history grid\n",
    "print(\"\\nPAYMENT HISTORY GRID CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "grid_check = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        LENGTH(payment_history_grid) as grid_length,\n",
    "        COUNT(*) as count\n",
    "    FROM default.summary\n",
    "    GROUP BY LENGTH(payment_history_grid)\n",
    "\"\"\")\n",
    "\n",
    "grid_check.show()\n",
    "\n",
    "# Validate grid content\n",
    "print(\"\\nGrid character distribution (first position):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUBSTRING(payment_history_grid, 1, 1) as first_char,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 1) as pct\n",
    "    FROM default.summary\n",
    "    GROUP BY 1\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. DPD and Payment Rating Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPD distribution\n",
    "print(\"=\" * 70)\n",
    "print(\"DAYS PAST DUE DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN days_past_due IS NULL THEN 'NULL'\n",
    "            WHEN days_past_due = 0 THEN '0 (Current)'\n",
    "            WHEN days_past_due BETWEEN 1 AND 29 THEN '1-29 DPD'\n",
    "            WHEN days_past_due BETWEEN 30 AND 59 THEN '30-59 DPD'\n",
    "            WHEN days_past_due BETWEEN 60 AND 89 THEN '60-89 DPD'\n",
    "            WHEN days_past_due BETWEEN 90 AND 119 THEN '90-119 DPD'\n",
    "            WHEN days_past_due BETWEEN 120 AND 179 THEN '120-179 DPD'\n",
    "            WHEN days_past_due >= 180 THEN '180+ DPD'\n",
    "            ELSE 'Other'\n",
    "        END as dpd_bucket,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pct\n",
    "    FROM default.summary\n",
    "    GROUP BY 1\n",
    "    ORDER BY \n",
    "        CASE \n",
    "            WHEN days_past_due IS NULL THEN -1\n",
    "            WHEN days_past_due = 0 THEN 0\n",
    "            WHEN days_past_due BETWEEN 1 AND 29 THEN 1\n",
    "            WHEN days_past_due BETWEEN 30 AND 59 THEN 2\n",
    "            WHEN days_past_due BETWEEN 60 AND 89 THEN 3\n",
    "            WHEN days_past_due BETWEEN 90 AND 119 THEN 4\n",
    "            WHEN days_past_due BETWEEN 120 AND 179 THEN 5\n",
    "            ELSE 6\n",
    "        END\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify DPD matches source\n",
    "print(\"\\nDPD SOURCE MATCH VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dpd_mismatch = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as mismatch_count\n",
    "    FROM default.default.accounts_all a\n",
    "    JOIN default.summary s \n",
    "        ON a.cons_acct_key = s.cons_acct_key \n",
    "        AND a.rpt_as_of_mo = s.rpt_as_of_mo\n",
    "    WHERE a.days_past_due_ct_4in != s.days_past_due\n",
    "\"\"\").collect()[0]['mismatch_count']\n",
    "\n",
    "if dpd_mismatch == 0:\n",
    "    print(\"PASS: All DPD values match between source and summary\")\n",
    "else:\n",
    "    print(f\"FAIL: {dpd_mismatch} DPD mismatches found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Interactive Account Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to explore any account\n",
    "EXPLORE_ACCOUNT = 100\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"ACCOUNT {EXPLORE_ACCOUNT} DEEP DIVE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if account exists\n",
    "exists = spark.sql(f\"SELECT COUNT(*) as cnt FROM default.summary WHERE cons_acct_key = {EXPLORE_ACCOUNT}\").collect()[0]['cnt']\n",
    "\n",
    "if exists > 0:\n",
    "    print(f\"\\nAccount found with {exists} month(s) of data\")\n",
    "    \n",
    "    # Source data\n",
    "    print(\"\\n--- Source Data ---\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            rpt_as_of_mo, \n",
    "            acct_bal_am as balance,\n",
    "            days_past_due_ct_4in as dpd,\n",
    "            actual_pymt_am as payment,\n",
    "            asset_class_cd_4in as asset_class\n",
    "        FROM default.default.accounts_all \n",
    "        WHERE cons_acct_key = {EXPLORE_ACCOUNT}\n",
    "        ORDER BY rpt_as_of_mo\n",
    "    \"\"\").show(truncate=False)\n",
    "    \n",
    "    # Summary data\n",
    "    print(\"\\n--- Summary Data ---\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            rpt_as_of_mo,\n",
    "            balance_am,\n",
    "            days_past_due,\n",
    "            actual_payment_am,\n",
    "            asset_class_cd,\n",
    "            payment_history_grid\n",
    "        FROM default.summary\n",
    "        WHERE cons_acct_key = {EXPLORE_ACCOUNT}\n",
    "        ORDER BY rpt_as_of_mo\n",
    "    \"\"\").show(truncate=False)\n",
    "    \n",
    "    # Latest history arrays\n",
    "    print(\"\\n--- History Arrays (latest month, first 8 positions) ---\")\n",
    "    latest = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            rpt_as_of_mo,\n",
    "            SLICE(balance_am_history, 1, 8) as balance_hist,\n",
    "            SLICE(days_past_due_history, 1, 8) as dpd_hist,\n",
    "            SLICE(payment_rating_cd_history, 1, 8) as rating_hist\n",
    "        FROM default.summary\n",
    "        WHERE cons_acct_key = {EXPLORE_ACCOUNT}\n",
    "        ORDER BY rpt_as_of_mo DESC\n",
    "        LIMIT 1\n",
    "    \"\"\").show(truncate=False)\n",
    "else:\n",
    "    print(f\"Account {EXPLORE_ACCOUNT} not found. Try another account ID.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation summary\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "checks = []\n",
    "\n",
    "# Check 1: Row counts match\n",
    "source_count = spark.table(\"default.default.accounts_all\").count()\n",
    "summary_count = spark.table(\"default.summary\").count()\n",
    "checks.append((\"Row counts (source = summary)\", source_count == summary_count, f\"{source_count} vs {summary_count}\"))\n",
    "\n",
    "# Check 2: No duplicates\n",
    "dup_count = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) FROM (\n",
    "        SELECT cons_acct_key, rpt_as_of_mo FROM default.summary\n",
    "        GROUP BY 1, 2 HAVING COUNT(*) > 1\n",
    "    )\n",
    "\"\"\").collect()[0][0]\n",
    "checks.append((\"No duplicates\", dup_count == 0, f\"{dup_count} duplicates\"))\n",
    "\n",
    "# Check 3: Array lengths\n",
    "bad_arrays = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) FROM default.summary\n",
    "    WHERE SIZE(balance_am_history) != 36 OR SIZE(days_past_due_history) != 36\n",
    "\"\"\").collect()[0][0]\n",
    "checks.append((\"Array lengths = 36\", bad_arrays == 0, f\"{bad_arrays} bad arrays\"))\n",
    "\n",
    "# Check 4: Grid length\n",
    "bad_grids = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) FROM default.summary WHERE LENGTH(payment_history_grid) != 36\n",
    "\"\"\").collect()[0][0]\n",
    "checks.append((\"Grid length = 36\", bad_grids == 0, f\"{bad_grids} bad grids\"))\n",
    "\n",
    "# Check 5: Balance match\n",
    "bal_mismatch = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) FROM default.default.accounts_all a\n",
    "    JOIN default.summary s ON a.cons_acct_key = s.cons_acct_key AND a.rpt_as_of_mo = s.rpt_as_of_mo\n",
    "    WHERE a.acct_bal_am != s.balance_am_history[0]\n",
    "\"\"\").collect()[0][0]\n",
    "checks.append((\"Balance source = history[0]\", bal_mismatch == 0, f\"{bal_mismatch} mismatches\"))\n",
    "\n",
    "# Check 6: DPD match\n",
    "dpd_mismatch = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) FROM default.default.accounts_all a\n",
    "    JOIN default.summary s ON a.cons_acct_key = s.cons_acct_key AND a.rpt_as_of_mo = s.rpt_as_of_mo\n",
    "    WHERE a.days_past_due_ct_4in != s.days_past_due\n",
    "\"\"\").collect()[0][0]\n",
    "checks.append((\"DPD source = summary\", dpd_mismatch == 0, f\"{dpd_mismatch} mismatches\"))\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'Check':<40} {'Status':<10} {'Details'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "passed = 0\n",
    "for check_name, check_passed, details in checks:\n",
    "    status = \"PASS\" if check_passed else \"FAIL\"\n",
    "    passed += 1 if check_passed else 0\n",
    "    print(f\"{check_name:<40} {status:<10} {details}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nTotal: {passed}/{len(checks)} checks passed\")\n",
    "\n",
    "if passed == len(checks):\n",
    "    print(\"\\n ALL VALIDATIONS PASSED! \")\n",
    "else:\n",
    "    print(\"\\n SOME VALIDATIONS FAILED - Review above \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom query cell\n",
    "print(\"\\nRun your own queries below:\")\n",
    "\n",
    "# Example: Find accounts with high DPD\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        days_past_due,\n",
    "        balance_am,\n",
    "        payment_history_grid\n",
    "    FROM default.summary\n",
    "    WHERE days_past_due > 90\n",
    "    ORDER BY days_past_due DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
