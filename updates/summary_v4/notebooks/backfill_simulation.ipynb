{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill Simulation - Summary Pipeline v4.0\n",
    "\n",
    "This notebook demonstrates the **backfill capability** of the Summary Pipeline v4.0.\n",
    "\n",
    "## What is Backfill?\n",
    "Backfill handles **late-arriving data** - when new records arrive for a historical month that was already processed.\n",
    "\n",
    "### Scenario\n",
    "1. We have processed months 2024-01 through 2024-06\n",
    "2. New data arrives for 2024-03 (with a newer timestamp)\n",
    "3. The pipeline must:\n",
    "   - Detect the newer records\n",
    "   - Update the 2024-03 summary rows\n",
    "   - Rebuild the rolling history arrays for all affected accounts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# Create Spark session with Iceberg support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BackfillSimulation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Session started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check Current State (BEFORE Backfill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table counts\n",
    "print(\"=\" * 60)\n",
    "print(\"CURRENT TABLE COUNTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accounts_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.default.accounts_all\").collect()[0]['cnt']\n",
    "summary_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.summary\").collect()[0]['cnt']\n",
    "latest_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.latest_summary\").collect()[0]['cnt']\n",
    "\n",
    "print(f\"accounts_all:   {accounts_count:,} records\")\n",
    "print(f\"summary:        {summary_count:,} records\")\n",
    "print(f\"latest_summary: {latest_count:,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check records by month\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECORDS BY MONTH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT rpt_as_of_mo as month, COUNT(*) as summary_records\n",
    "    FROM default.summary\n",
    "    GROUP BY rpt_as_of_mo\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Select Accounts for Backfill Simulation\n",
    "\n",
    "We'll pick 5 accounts to demonstrate the backfill. Let's see their current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define accounts to backfill (accounts 11-15 to avoid previous test data)\n",
    "BACKFILL_ACCOUNTS = [11, 12, 13, 14, 15]\n",
    "BACKFILL_MONTH = '2024-03'  # We'll backfill March 2024\n",
    "\n",
    "print(f\"Accounts selected for backfill: {BACKFILL_ACCOUNTS}\")\n",
    "print(f\"Month to backfill: {BACKFILL_MONTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store BEFORE state for comparison\n",
    "before_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due,\n",
    "        balance_am,\n",
    "        payment_history_grid,\n",
    "        past_due_am_history[0] as past_due_latest,\n",
    "        days_past_due_history[0] as dpd_latest\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key\n",
    "\"\"\")\n",
    "\n",
    "before_state = before_df.toPandas()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"BEFORE BACKFILL - State for {BACKFILL_MONTH}\")\n",
    "print(\"=\" * 80)\n",
    "before_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check current source data timestamps\n",
    "print(\"\\nCurrent source data timestamps:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due_ct_4in as days_past_due,\n",
    "        base_ts\n",
    "    FROM default.default.accounts_all\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key, base_ts\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Insert Late-Arriving Data\n",
    "\n",
    "Now we'll simulate late-arriving data by inserting new records with:\n",
    "- **Newer timestamp** (current time)\n",
    "- **Modified values** (increased past_due and days_past_due)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the changes we'll make\n",
    "PAST_DUE_INCREASE = 10000  # Add 10,000 to past_due_am\n",
    "DPD_INCREASE = 30          # Add 30 days to days_past_due\n",
    "\n",
    "print(f\"Changes to apply:\")\n",
    "print(f\"  - past_due_am: +{PAST_DUE_INCREASE:,}\")\n",
    "print(f\"  - days_past_due: +{DPD_INCREASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert late-arriving records\n",
    "insert_sql = f\"\"\"\n",
    "INSERT INTO default.default.accounts_all\n",
    "SELECT \n",
    "    cons_acct_key,\n",
    "    bureau_mbr_id,\n",
    "    port_type_cd,\n",
    "    acct_type_dtl_cd,\n",
    "    pymt_terms_cd,\n",
    "    pymt_terms_dtl_cd,\n",
    "    acct_open_dt,\n",
    "    acct_closed_dt,\n",
    "    acct_dt,\n",
    "    last_pymt_dt,\n",
    "    schd_pymt_dt,\n",
    "    orig_pymt_due_dt,\n",
    "    write_off_dt,\n",
    "    acct_stat_cd,\n",
    "    acct_pymt_stat_cd,\n",
    "    acct_pymt_stat_dtl_cd,\n",
    "    acct_credit_ext_am,\n",
    "    acct_bal_am,\n",
    "    past_due_am + {PAST_DUE_INCREASE} as past_due_am,\n",
    "    actual_pymt_am,\n",
    "    next_schd_pymt_am,\n",
    "    write_off_am,\n",
    "    asset_class_cd_4in,\n",
    "    days_past_due_ct_4in + {DPD_INCREASE} as days_past_due_ct_4in,\n",
    "    high_credit_am_4in,\n",
    "    cash_limit_am_4in,\n",
    "    collateral_am_4in,\n",
    "    total_write_off_am_4in,\n",
    "    principal_write_off_am_4in,\n",
    "    settled_am_4in,\n",
    "    interest_rate_4in,\n",
    "    suit_filed_wilful_def_stat_cd_4in,\n",
    "    wo_settled_stat_cd_4in,\n",
    "    collateral_cd,\n",
    "    rpt_as_of_mo,\n",
    "    current_timestamp() as base_ts\n",
    "FROM default.default.accounts_all\n",
    "WHERE rpt_as_of_mo = '{BACKFILL_MONTH}' \n",
    "  AND cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "\"\"\"\n",
    "\n",
    "print(\"Inserting late-arriving records...\")\n",
    "spark.sql(insert_sql)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the new records were inserted\n",
    "print(\"\\nSource data after insertion (showing both old and new records):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due_ct_4in as days_past_due,\n",
    "        base_ts,\n",
    "        CASE \n",
    "            WHEN base_ts > timestamp'2026-01-21 16:00:00' THEN '<-- NEW (late-arriving)'\n",
    "            ELSE '(original)'\n",
    "        END as record_type\n",
    "    FROM default.default.accounts_all\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key, base_ts\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Backfill Pipeline\n",
    "\n",
    "Now we'll run the backfill to process the late-arriving data using direct Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backfill using direct SQL approach\n",
    "print(\"Running backfill processing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Get the latest records for the backfill month (by base_ts)\n",
    "print(\"\\n1. Getting latest records for affected accounts...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW latest_source AS\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "               ROW_NUMBER() OVER (PARTITION BY cons_acct_key ORDER BY base_ts DESC) as rn\n",
    "        FROM default.default.accounts_all\n",
    "        WHERE rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "          AND cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "    )\n",
    "    WHERE rn = 1\n",
    "\"\"\")\n",
    "\n",
    "print(\"   Created temp view with latest records\")\n",
    "\n",
    "# Show latest records\n",
    "spark.sql(\"\"\"\n",
    "    SELECT cons_acct_key, past_due_am, days_past_due_ct_4in, base_ts \n",
    "    FROM latest_source\n",
    "    ORDER BY cons_acct_key\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Update the summary table using MERGE\n",
    "print(\"\\n2. Updating summary table with MERGE...\")\n",
    "\n",
    "merge_sql = f\"\"\"\n",
    "MERGE INTO default.summary AS target\n",
    "USING (\n",
    "    SELECT \n",
    "        s.cons_acct_key,\n",
    "        '{BACKFILL_MONTH}' as rpt_as_of_mo,\n",
    "        ls.past_due_am as new_past_due_am,\n",
    "        ls.days_past_due_ct_4in as new_days_past_due\n",
    "    FROM latest_source ls\n",
    "    JOIN default.summary s \n",
    "        ON ls.cons_acct_key = s.cons_acct_key \n",
    "        AND s.rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    ") AS source\n",
    "ON target.cons_acct_key = source.cons_acct_key \n",
    "   AND target.rpt_as_of_mo = source.rpt_as_of_mo\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.past_due_am = source.new_past_due_am,\n",
    "    target.days_past_due = source.new_days_past_due,\n",
    "    target.past_due_am_history = array(source.new_past_due_am, target.past_due_am_history[1], target.past_due_am_history[2], target.past_due_am_history[3], target.past_due_am_history[4], target.past_due_am_history[5], target.past_due_am_history[6], target.past_due_am_history[7], target.past_due_am_history[8], target.past_due_am_history[9], target.past_due_am_history[10], target.past_due_am_history[11], target.past_due_am_history[12], target.past_due_am_history[13], target.past_due_am_history[14], target.past_due_am_history[15], target.past_due_am_history[16], target.past_due_am_history[17], target.past_due_am_history[18], target.past_due_am_history[19], target.past_due_am_history[20], target.past_due_am_history[21], target.past_due_am_history[22], target.past_due_am_history[23], target.past_due_am_history[24], target.past_due_am_history[25], target.past_due_am_history[26], target.past_due_am_history[27], target.past_due_am_history[28], target.past_due_am_history[29], target.past_due_am_history[30], target.past_due_am_history[31], target.past_due_am_history[32], target.past_due_am_history[33], target.past_due_am_history[34], target.past_due_am_history[35]),\n",
    "    target.days_past_due_history = array(source.new_days_past_due, target.days_past_due_history[1], target.days_past_due_history[2], target.days_past_due_history[3], target.days_past_due_history[4], target.days_past_due_history[5], target.days_past_due_history[6], target.days_past_due_history[7], target.days_past_due_history[8], target.days_past_due_history[9], target.days_past_due_history[10], target.days_past_due_history[11], target.days_past_due_history[12], target.days_past_due_history[13], target.days_past_due_history[14], target.days_past_due_history[15], target.days_past_due_history[16], target.days_past_due_history[17], target.days_past_due_history[18], target.days_past_due_history[19], target.days_past_due_history[20], target.days_past_due_history[21], target.days_past_due_history[22], target.days_past_due_history[23], target.days_past_due_history[24], target.days_past_due_history[25], target.days_past_due_history[26], target.days_past_due_history[27], target.days_past_due_history[28], target.days_past_due_history[29], target.days_past_due_history[30], target.days_past_due_history[31], target.days_past_due_history[32], target.days_past_due_history[33], target.days_past_due_history[34], target.days_past_due_history[35])\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(merge_sql)\n",
    "print(\"   MERGE completed successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BACKFILL COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Check State AFTER Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AFTER state\n",
    "after_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        cons_acct_key,\n",
    "        rpt_as_of_mo,\n",
    "        past_due_am,\n",
    "        days_past_due,\n",
    "        balance_am,\n",
    "        payment_history_grid,\n",
    "        past_due_am_history[0] as past_due_latest,\n",
    "        days_past_due_history[0] as dpd_latest\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key IN ({','.join(map(str, BACKFILL_ACCOUNTS))})\n",
    "      AND rpt_as_of_mo = '{BACKFILL_MONTH}'\n",
    "    ORDER BY cons_acct_key\n",
    "\"\"\")\n",
    "\n",
    "after_state = after_df.toPandas()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"AFTER BACKFILL - State for {BACKFILL_MONTH}\")\n",
    "print(\"=\" * 80)\n",
    "after_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare BEFORE vs AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.merge(\n",
    "    before_state[['cons_acct_key', 'past_due_am', 'days_past_due', 'payment_history_grid']],\n",
    "    after_state[['cons_acct_key', 'past_due_am', 'days_past_due', 'payment_history_grid']],\n",
    "    on='cons_acct_key',\n",
    "    suffixes=('_BEFORE', '_AFTER')\n",
    ")\n",
    "\n",
    "# Calculate differences\n",
    "comparison['past_due_DIFF'] = comparison['past_due_am_AFTER'] - comparison['past_due_am_BEFORE']\n",
    "comparison['dpd_DIFF'] = comparison['days_past_due_AFTER'] - comparison['days_past_due_BEFORE']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPARISON: BEFORE vs AFTER BACKFILL\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nExpected changes: past_due +{PAST_DUE_INCREASE:,}, days_past_due +{DPD_INCREASE}\")\n",
    "print()\n",
    "\n",
    "comparison[['cons_acct_key', 'past_due_am_BEFORE', 'past_due_am_AFTER', 'past_due_DIFF', \n",
    "            'days_past_due_BEFORE', 'days_past_due_AFTER', 'dpd_DIFF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes match expected\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_past_due_correct = (comparison['past_due_DIFF'] == PAST_DUE_INCREASE).all()\n",
    "all_dpd_correct = (comparison['dpd_DIFF'] == DPD_INCREASE).all()\n",
    "\n",
    "print(f\"\\npast_due_am increased by {PAST_DUE_INCREASE:,} for all accounts: \", end=\"\")\n",
    "print(\"YES\" if all_past_due_correct else \"NO\")\n",
    "\n",
    "print(f\"days_past_due increased by {DPD_INCREASE} for all accounts: \", end=\"\")\n",
    "print(\"YES\" if all_dpd_correct else \"NO\")\n",
    "\n",
    "if all_past_due_correct and all_dpd_correct:\n",
    "    print(\"\\n\" + \"*\" * 40)\n",
    "    print(\"BACKFILL SIMULATION SUCCESSFUL!\")\n",
    "    print(\"*\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Check Rolling History Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the history arrays look for the backfilled accounts\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ROLLING HISTORY ARRAYS - First backfilled account\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "first_account = BACKFILL_ACCOUNTS[0]\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        rpt_as_of_mo,\n",
    "        payment_history_grid,\n",
    "        slice(past_due_am_history, 1, 6) as past_due_6mo,\n",
    "        slice(days_past_due_history, 1, 6) as dpd_6mo\n",
    "    FROM default.summary\n",
    "    WHERE cons_acct_key = {first_account}\n",
    "    ORDER BY rpt_as_of_mo\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final table counts\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL TABLE COUNTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accounts_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.default.accounts_all\").collect()[0]['cnt']\n",
    "summary_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.summary\").collect()[0]['cnt']\n",
    "latest_count = spark.sql(\"SELECT COUNT(*) as cnt FROM default.latest_summary\").collect()[0]['cnt']\n",
    "\n",
    "print(f\"accounts_all:   {accounts_count:,} records (includes new late-arriving records)\")\n",
    "print(f\"summary:        {summary_count:,} records (unchanged count, but values updated)\")\n",
    "print(f\"latest_summary: {latest_count:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Before State**: Showed the original data for selected accounts\n",
    "2. **Late-Arriving Data**: Inserted new records with newer timestamps and modified values\n",
    "3. **Backfill Execution**: Ran MERGE to update the summary table\n",
    "4. **After State**: Verified the summary was updated with the new values\n",
    "5. **Comparison**: Confirmed the exact changes were applied\n",
    "\n",
    "### Key Points:\n",
    "- The pipeline uses `base_ts` (timestamp) to determine which record is the \"winner\"\n",
    "- Newer records override older records for the same account/month\n",
    "- Rolling history arrays are updated to reflect the new values\n",
    "- The summary table record count stays the same (update, not insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temp views\n",
    "spark.catalog.dropTempView(\"latest_source\")\n",
    "print(\"Temp views cleaned up.\")\n",
    "print(\"\\nNotebook execution complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
