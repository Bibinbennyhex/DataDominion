# =============================================================================
# Summary Pipeline v9.1 - Spark Defaults Configuration
# =============================================================================
# Place this file at /etc/spark/conf/spark-defaults.conf on EMR cluster

# -----------------------------------------------------------------------------
# EXECUTOR CONFIGURATION
# -----------------------------------------------------------------------------
spark.executor.instances=150
spark.executor.cores=5
spark.executor.memory=45g
spark.executor.memoryOverhead=8g

# Dynamic allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=50
spark.dynamicAllocation.maxExecutors=250
spark.dynamicAllocation.initialExecutors=100
spark.dynamicAllocation.executorIdleTimeout=120s
spark.dynamicAllocation.schedulerBacklogTimeout=5s

# -----------------------------------------------------------------------------
# DRIVER CONFIGURATION
# -----------------------------------------------------------------------------
spark.driver.cores=8
spark.driver.memory=64g
spark.driver.memoryOverhead=8g
spark.driver.maxResultSize=8g

# -----------------------------------------------------------------------------
# SHUFFLE CONFIGURATION
# -----------------------------------------------------------------------------
spark.sql.shuffle.partitions=8192
spark.default.parallelism=8192
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.shuffle.file.buffer=1m
spark.reducer.maxSizeInFlight=96m
spark.shuffle.io.maxRetries=10
spark.shuffle.io.retryWait=60s

# External shuffle service
spark.shuffle.service.enabled=true
spark.dynamicAllocation.shuffleTracking.enabled=true

# -----------------------------------------------------------------------------
# MEMORY MANAGEMENT
# -----------------------------------------------------------------------------
spark.memory.fraction=0.8
spark.memory.storageFraction=0.3
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionSize=128m
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.skewJoin.skewedPartitionFactor=5
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=512m
spark.sql.adaptive.localShuffleReader.enabled=true

# -----------------------------------------------------------------------------
# JOIN OPTIMIZATION
# -----------------------------------------------------------------------------
spark.sql.autoBroadcastJoinThreshold=500m
spark.sql.broadcastTimeout=1200
spark.sql.join.preferSortMergeJoin=true
spark.sql.optimizer.dynamicPartitionPruning.enabled=true

# -----------------------------------------------------------------------------
# ICEBERG CONFIGURATION
# -----------------------------------------------------------------------------
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.ascend_iceberg=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.ascend_iceberg.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog
spark.sql.catalog.ascend_iceberg.warehouse=s3://in-cs-ivaps-data-bucket-prod-ap-south-1/ivaps_prod/persistent/ascend_summary/summary_complete/
spark.sql.catalog.ascend_iceberg.io-impl=org.apache.iceberg.aws.s3.S3FileIO

# Iceberg write optimization
spark.sql.iceberg.handle-timestamp-without-timezone=true
spark.sql.sources.partitionOverwriteMode=dynamic

# -----------------------------------------------------------------------------
# NETWORK & TIMEOUT
# -----------------------------------------------------------------------------
spark.network.timeout=800s
spark.executor.heartbeatInterval=60s
spark.rpc.askTimeout=600s
spark.rpc.lookupTimeout=300s

# -----------------------------------------------------------------------------
# S3 OPTIMIZATION
# -----------------------------------------------------------------------------
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.threads.max=100
spark.hadoop.fs.s3a.connection.establish.timeout=10000
spark.hadoop.fs.s3a.connection.timeout=200000
spark.hadoop.fs.s3a.multipart.size=512m
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.buffer=bytebuffer
spark.hadoop.fs.s3a.block.size=256m

# -----------------------------------------------------------------------------
# SPECULATION & FAULT TOLERANCE
# -----------------------------------------------------------------------------
spark.speculation=true
spark.speculation.interval=5000ms
spark.speculation.multiplier=2
spark.speculation.quantile=0.9
spark.task.maxFailures=8
