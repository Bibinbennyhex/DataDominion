{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.time.LocalDate\n",
    "\n",
    "object DpdGridJob {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val spark = SparkSession.builder()\n",
    "                .appName(\"Ascend_DPD_Processing\")\n",
    "                .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.9.1\")\n",
    "                .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "                .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "                .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "                .config(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "                .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n",
    "                .config(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "                .config(\"spark.network.timeout\", \"800s\")\n",
    "                .config(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "                .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "                .config(\"spark.sql.defaultCatalog\", \"ascend\")\n",
    "                .config(\"spark.sql.catalog.ascend\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "                .config(\"spark.sql.catalog.ascend.warehouse\", \"s3://lf-test-1-bucket/ascend-dpd\")\n",
    "                .config(\"spark.sql.catalog.ascend.type\", \"hadoop\")\n",
    "                .config(\"spark.sql.catalog.ascend.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "                .config(\"spark.sql.files.maxPartitionBytes\", \"268435456\")\n",
    "                .config(\"spark.sql.parquet.block.size\", \"268435456\")\n",
    "                .enableHiveSupport()\n",
    "                .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    generateDpdGrid(spark)\n",
    "    spark.stop()\n",
    "  }\n",
    "\n",
    "  def generateDpdGrid(spark: SparkSession): Unit = {\n",
    "    import spark.implicits._\n",
    "\n",
    "    val startMonth = LocalDate.of(1998, 1, 1)\n",
    "    val endMonth = LocalDate.of(2000, 12, 15)\n",
    "    var currentMonth = startMonth\n",
    "\n",
    "    while (!currentMonth.isAfter(endMonth)) {\n",
    "      val nextMonth = currentMonth.plusMonths(1)\n",
    "\n",
    "      println(s\"Processing month: $currentMonth ...\\n\")\n",
    "\n",
    "      val currentDf = spark.table(\"ascenddb.accounts_all\")\n",
    "        .filter(col(\"ACCT_DT\").between(lit(currentMonth), lit(nextMonth.minusDays(1))))\n",
    "        .select(\"CONS_ACCT_KEY\",\"BUREAU_MBR_ID\",\"PORT_TYPE_CD\",\"ACCT_TYPE_DTL_CD\",\"ACCT_OPEN_DT\",\"ORIG_LOAN_AMT\",\"ACCT_CLOSED_DT\",\"PYMT_TERMS_CD\",\"PYMT_TERMS_DTL_CD\",\"ACCT_DT\",\"ACCT_STAT_CD\",\"ACCT_PYMT_STAT_CD\",\"ACCT_PYMT_STAT_DTL_CD\",\"ACCT_CREDIT_EXT_AM\",\"ACCT_BAL_AM\",\"PAST_DUE_AM\",\"ACTUAL_PYMT_AM\",\"LAST_PYMT_DT\",\"SCHD_PYMT_DT\",\"NEXT_SCHD_PYMT_AM\",\"INTEREST_RATE\",\"COLLATERAL_CD\",\"ORIG_PYMT_DUE_DT\",\"WRITE_OFF_DT\",\"WRITE_OFF_AM\",\"ASSET_CLASS_CD\",\"DAYS_PAST_DUE_CT\",\"HI_CREDIT_AM\",\"CASH_LIMIT_AM\",\"COLLATERAL_AM\",\"TOTAL_WRITE_OFF_AM\",\"PRINCIPAL_WRITE_OFF_AM\",\"SETTLED_AM\",\"INTEREST_RATE_4IN\",\"SUIT_FILED_WILFUL_DEF_STAT_CD\",\"WO_SETTLED_STAT_CD\",\"MSUBID\",\"CREDITLIMITAM\",\"BALANCE_AM\",\"BALANCE_DT\",\"DFLTSTATUSDT\",\"RESPONSIBILITY_CD\",\"CHARGEOFFAM\",\"EMI_AMT\",\"TENURE\",\"PAYMENTRATINGCD\",\"PINCODE\")\n",
    "\n",
    "      val latestPrev = spark.read.table(\"ascenddb.latest_dpd_summary\")\n",
    "        .withColumnRenamed(\"ACCT_DT\", \"ACCT_DT_prev\")\n",
    "\n",
    "      var mergedDf = currentDf.join(latestPrev, Seq(\"CONS_ACCT_KEY\"), \"left\")\n",
    "        .withColumn(\"MONTH_DIFF\", months_between(col(\"ACCT_DT\"), col(\"ACCT_DT_prev\")).cast(\"int\"))\n",
    "        .withColumn(\"FILLER_ARRAY\",\n",
    "          when(col(\"MONTH_DIFF\") > 1, expr(\"transform(sequence(1, MONTH_DIFF - 1), x -> '?')\"))\n",
    "            .otherwise(array())\n",
    "        )\n",
    "        .withColumn(\"Merged_DPD_Array\",\n",
    "          concat(\n",
    "            array(col(\"DAYS_PAST_DUE_CT\")),\n",
    "            col(\"FILLER_ARRAY\"),\n",
    "            when(col(\"DPD_GRID\").isNotNull, split(col(\"DPD_GRID\"), \"~\")).otherwise(array())\n",
    "          )\n",
    "        )\n",
    "        .withColumn(\"DPD_Array_Trimmed\",\n",
    "          slice(concat(col(\"Merged_DPD_Array\"), expr(\"array_repeat('?', 36)\")), 1, 36)\n",
    "        )\n",
    "        .withColumn(\"DPD_GRID\",\n",
    "          concat_ws(\"~\", col(\"DPD_Array_Trimmed\"))\n",
    "        )\n",
    "        .withColumn(\"DAYS_PAST_DUE\",\n",
    "          col(\"DAYS_PAST_DUE_CT\")\n",
    "        )\n",
    "\n",
    "      mergedDf.select(\"CONS_ACCT_KEY\",\"BUREAU_MBR_ID\",\"PORT_TYPE_CD\",\"ACCT_TYPE_DTL_CD\",\"ACCT_OPEN_DT\",\"ORIG_LOAN_AMT\",\"ACCT_CLOSED_DT\",\"PYMT_TERMS_CD\",\"PYMT_TERMS_DTL_CD\",\"ACCT_DT\",\"ACCT_STAT_CD\",\"ACCT_PYMT_STAT_CD\",\"ACCT_PYMT_STAT_DTL_CD\",\"ACCT_CREDIT_EXT_AM\",\"ACCT_BAL_AM\",\"PAST_DUE_AM\",\"ACTUAL_PYMT_AM\",\"LAST_PYMT_DT\",\"SCHD_PYMT_DT\",\"NEXT_SCHD_PYMT_AM\",\"INTEREST_RATE\",\"COLLATERAL_CD\",\"ORIG_PYMT_DUE_DT\",\"WRITE_OFF_DT\",\"WRITE_OFF_AM\",\"ASSET_CLASS_CD\",\"DAYS_PAST_DUE_CT\",\"HI_CREDIT_AM\",\"CASH_LIMIT_AM\",\"COLLATERAL_AM\",\"TOTAL_WRITE_OFF_AM\",\"PRINCIPAL_WRITE_OFF_AM\",\"SETTLED_AM\",\"INTEREST_RATE_4IN\",\"SUIT_FILED_WILFUL_DEF_STAT_CD\",\"WO_SETTLED_STAT_CD\",\"MSUBID\",\"CREDITLIMITAM\",\"BALANCE_AM\",\"BALANCE_DT\",\"DFLTSTATUSDT\",\"RESPONSIBILITY_CD\",\"CHARGEOFFAM\",\"EMI_AMT\",\"TENURE\",\"PAYMENTRATINGCD\",\"PINCODE\",\"DAYS_PAST_DUE\",\"DPD_GRID\")\n",
    "        .write\n",
    "        .format(\"iceberg\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(\"ascenddb.summary\")\n",
    "\n",
    "      val currentMonthDf = mergedDf.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\")\n",
    "      val latestPrevRenamed = latestPrev.withColumnRenamed(\"ACCT_DT_prev\", \"ACCT_DT\")\n",
    "\n",
    "      val mergedLatest = latestPrevRenamed.union(currentMonthDf)\n",
    "        .groupBy(\"CONS_ACCT_KEY\")\n",
    "        .agg(\n",
    "          max_by(col(\"ACCT_DT\"), col(\"ACCT_DT\")).as(\"ACCT_DT\"),\n",
    "          max_by(col(\"DPD_GRID\"), col(\"ACCT_DT\")).as(\"DPD_GRID\")\n",
    "        )\n",
    "\n",
    "      mergedLatest.write\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(\"ascenddb.latest_dpd_summary\")\n",
    "\n",
    "      println(\"Done\\n\")\n",
    "      currentMonth = nextMonth\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Build a JAR with SBT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create build.sbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name := \"dpd-grid-generator\"\n",
    "version := \"0.1\"\n",
    "scalaVersion := \"2.12.17\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "  \"org.apache.spark\" %% \"spark-core\" % \"3.4.1\" % \"provided\",\n",
    "  \"org.apache.spark\" %% \"spark-sql\" % \"3.4.1\" % \"provided\",\n",
    "  \"org.apache.iceberg\" %% \"iceberg-spark-runtime-3.4\" % \"1.9.1\",\n",
    "  \"org.apache.iceberg\" % \"iceberg-aws-bundle\" % \"1.9.1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compile and package:\n",
    "   `sbt clean package`\n",
    "   Result generated as\n",
    "   `target/scala-2.12/dpd-grid-generator_2.12-0.1.jar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Upload jar to S3 `aws s3 cp target/scala-2.12/dpd-grid-generator_2.12-0.1.jar s3://your-bucket-name/jars/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Submit Job to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "  spark-submit \\\n",
    "  --deploy-mode cluster \\\n",
    "  --master yarn \\\n",
    "  --class DpdGridJob \\\n",
    "  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\n",
    "  --conf spark.sql.adaptive.skewJoin.enabled=true \\\n",
    "  --conf spark.sql.adaptive.enabled=true \\\n",
    "  --conf spark.sql.autoBroadcastJoinThreshold=-1 \\\n",
    "  --conf spark.sql.join.preferSortMergeJoin=true \\\n",
    "  --conf spark.sql.adaptive.localShuffleReader.enabled=true \\\n",
    "  --conf spark.sql.optimizer.dynamicPartitionPruning.enabled=true \\\n",
    "  --conf spark.network.timeout=800s \\\n",
    "  --conf spark.executor.heartbeatInterval=60s \\\n",
    "  --conf spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2 \\\n",
    "  --conf spark.sql.defaultCatalog=ascend \\\n",
    "  --conf spark.sql.catalog.ascend=org.apache.iceberg.spark.SparkCatalog \\\n",
    "  --conf spark.sql.catalog.ascend.warehouse=s3://lf-test-1-bucket/ascend-dpd \\\n",
    "  --conf spark.sql.catalog.ascend.type=hadoop \\\n",
    "  --conf spark.sql.catalog.ascend.io-impl=org.apache.iceberg.aws.s3.S3FileIO \\\n",
    "  --conf spark.sql.files.maxPartitionBytes=268435456 \\\n",
    "  --conf spark.sql.parquet.block.size=268435456 \\\n",
    "  --packages org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.9.1 \\\n",
    "  s3://your-bucket-name/jars/dpd-grid-generator_2.12-0.1.jar\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
