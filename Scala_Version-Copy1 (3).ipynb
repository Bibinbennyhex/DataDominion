{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27329e09-0c6d-46a8-898f-ba9b46264946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://103d525d728a:4041\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1753642250320)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "println(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828ee828-1491-44c3-b4b1-b732c9312809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://9914a2c60656:4042\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1753703845536)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import java.time.{LocalDate, LocalDateTime}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cell 1: Imports and Spark Session Setup\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import java.time.{LocalDate, LocalDateTime}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba47daaa-93ca-45ac-85e8-f5e76b3df5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created with Iceberg and MinIO configuration!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@da6b1f7\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cell 2: Create Spark Session with Iceberg and MinIO Configuration\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Ascend_DPD_Processing\")\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "  .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "  .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "  .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\")\n",
    "  .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "  .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "  .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "  .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "  .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "  .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n",
    "  .config(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "  .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "println(\"✅ Spark session created with Iceberg and MinIO configuration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3bcd19-3c11-4e5f-a299-a1077f8b17a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://9914a2c60656:4043\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1753708122605)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---+\n",
      "|CONS_ACCT_KEY|   ACCT_DT|DPD|\n",
      "+-------------+----------+---+\n",
      "|          336|1998-01-15| 37|\n",
      "|          336|1998-02-15| 38|\n",
      "|          336|1998-03-15| 39|\n",
      "|          336|1998-05-15| 41|\n",
      "|          336|1998-06-15| 42|\n",
      "|          336|1998-07-15| 43|\n",
      "|          336|1998-08-15| 44|\n",
      "|          336|1998-09-15| 45|\n",
      "|          336|1998-10-15| 46|\n",
      "|          336|1998-11-15| 47|\n",
      "|          336|1998-12-15| 48|\n",
      "|          336|1999-01-15| 49|\n",
      "|          336|1999-02-15| 50|\n",
      "|          336|1999-03-15| 51|\n",
      "|          336|1999-04-15| 52|\n",
      "|          336|1999-05-15| 53|\n",
      "|          336|1999-06-15| 54|\n",
      "|          336|1999-07-15| 55|\n",
      "|          336|1999-08-15| 56|\n",
      "|          336|1999-09-15| 57|\n",
      "+-------------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accountsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CONS_ACCT_KEY: int, ACCT_DT: date ... 1 more field]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accountsDF = spark.table(\"ascenddb.dpd_data\")\n",
    "      .filter(col(\"CONS_ACCT_KEY\") === 336)\n",
    "      .orderBy(col(\"ACCT_DT\"))\n",
    "accountsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01c89f58-0db8-4eec-bfac-ca0ca965d183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generateDpdGrid: ()Unit\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateDpdGrid(): Unit = {\n",
    "  val startMonth = LocalDate.of(1998, 1, 1)\n",
    "  val endMonth = LocalDate.of(1998, 2, 15)\n",
    "  \n",
    "  var currentMonth = startMonth\n",
    "  \n",
    "  while (!currentMonth.isAfter(endMonth)) {\n",
    "    val nextMonth = currentMonth.plusMonths(1)\n",
    "    \n",
    "    println(s\"Processing month: $currentMonth ...\")\n",
    "    \n",
    "    val summaryColumnList = \"\"\"CONS_ACCT_KEY, ACCT_DT, DPD\"\"\".replaceAll(\"\\\\s+\", \" \")\n",
    "    \n",
    "    // Load current month data\n",
    "    val currentDf = spark.sql(s\"\"\"\n",
    "      SELECT $summaryColumnList\n",
    "      FROM ascenddb.dpd_data\n",
    "      WHERE ACCT_DT >= DATE '${currentMonth.toString}' AND ACCT_DT < DATE '${nextMonth.toString}'\n",
    "    \"\"\")\n",
    "\n",
    "    // Load only latest record per account\n",
    "    val latestPrev = spark.read.table(\"ascenddb.latest_dpd_summary\")\n",
    "      .withColumnRenamed(\"ACCT_DT\", \"ACCT_DT_prev\")\n",
    "    \n",
    "    // Join datasets\n",
    "    var mergedDf = currentDf.join(latestPrev, Seq(\"CONS_ACCT_KEY\"), \"left\")\n",
    "\n",
    "    // Calculate month difference and process DPD grid\n",
    "    mergedDf = mergedDf.withColumn(\"MONTH_DIFF\",\n",
    "      (month(col(\"ACCT_DT\")) - month(col(\"ACCT_DT_prev\"))) +\n",
    "      (year(col(\"ACCT_DT\")) - year(col(\"ACCT_DT_prev\"))) * 12\n",
    "    ).withColumn(\"FILLER_ARRAY\",\n",
    "      when(col(\"MONTH_DIFF\") > 1, \n",
    "        expr(\"transform(sequence(1, MONTH_DIFF - 1), x -> '?')\")\n",
    "      ).otherwise(array())\n",
    "    ).withColumn(\"Merged_DPD_Array\",\n",
    "          concat(\n",
    "            array(col(\"DPD\")),\n",
    "            col(\"FILLER_ARRAY\"),\n",
    "            when(col(\"DPD_GRID\").isNotNull, split(col(\"DPD_GRID\"), \"~\"))\n",
    "              .otherwise(array())\n",
    "          )\n",
    "        )\n",
    "\n",
    "    mergedDf = mergedDf.withColumn(\"array_size\",\n",
    "                        size(col(\"Merged_DPD_Array\"))).withColumn(\n",
    "                      \"DPD_Array_Trimmed\",\n",
    "                      when(col(\"array_size\") >= 36,\n",
    "                        slice(col(\"Merged_DPD_Array\"), 1, 36)\n",
    "                      ).otherwise(\n",
    "                        concat(\n",
    "                          col(\"Merged_DPD_Array\"),\n",
    "                          expr(\"transform(sequence(1, 36 - array_size), x -> '?')\")\n",
    "                        )\n",
    "                      )\n",
    "                    ).withColumn(\"DPD_GRID\",\n",
    "                        concat_ws(\"~\", slice(col(\"DPD_Array_Trimmed\"), 1, 36))\n",
    "                      )\n",
    "                      .drop(\"array_size\")\n",
    "\n",
    "    // Save historical output\n",
    "        val columnsToSelect = List(\n",
    "          \"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\", \"DPD\"\n",
    "        )\n",
    "        \n",
    "        mergedDf.select(columnsToSelect.head, columnsToSelect.tail: _*)\n",
    "          .write\n",
    "          .format(\"iceberg\")\n",
    "          .mode(\"append\")\n",
    "          .saveAsTable(\"ascenddb.summary\")\n",
    "        \n",
    "        // Load current month processed data\n",
    "        val currentMonthDf = mergedDf.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\")\n",
    "        \n",
    "        // Revert name of ACCT_DT_prev\n",
    "        val latestPrevRenamed = latestPrev.withColumnRenamed(\"ACCT_DT_prev\", \"ACCT_DT\")\n",
    "        \n",
    "        // Merge: Take latest between old and new per account\n",
    "        val windowSpec = Window.partitionBy(\"CONS_ACCT_KEY\").orderBy(col(\"ACCT_DT\").desc)\n",
    "        \n",
    "        val mergedLatest = latestPrevRenamed.union(currentMonthDf)\n",
    "          .withColumn(\"rn\", row_number().over(windowSpec))\n",
    "          .filter(col(\"rn\") === 1)\n",
    "          .drop(\"rn\")\n",
    "        \n",
    "        // Save merged latest back\n",
    "        mergedLatest.write\n",
    "          .mode(\"overwrite\")\n",
    "          .saveAsTable(\"ascenddb.latest_dpd_summary\")\n",
    "    \n",
    "    // mergedDf.show(truncate=false)\n",
    "    mergedDf.select(columnsToSelect.head, columnsToSelect.tail: _*).show()\n",
    "    currentMonth = nextMonth\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "557bbfeb-5865-42b3-9a9a-06f343ce8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing month: 1998-01-01 ...\n",
      "+-------------+----------+--------------------+---+\n",
      "|CONS_ACCT_KEY|   ACCT_DT|            DPD_GRID|DPD|\n",
      "+-------------+----------+--------------------+---+\n",
      "|            1|1998-01-15|2~37~36~35~34~33~...|  2|\n",
      "|            2|1998-01-15|3~38~37~36~35~34~...|  3|\n",
      "|            3|1998-01-15|4~39~38~37~36~35~...|  4|\n",
      "|            4|1998-01-15|5~40~39~38~37~36~...|  5|\n",
      "|            5|1998-01-15|6~41~40~39~38~37~...|  6|\n",
      "|            6|1998-01-15|7~42~41~40~39~38~...|  7|\n",
      "|            7|1998-01-15|8~43~42~41~40~39~...|  8|\n",
      "|            8|1998-01-15|9~44~43~42~41~40~...|  9|\n",
      "|            9|1998-01-15|10~45~44~43~42~41...| 10|\n",
      "|           10|1998-01-15|11~46~45~44~43~42...| 11|\n",
      "|           11|1998-01-15|12~47~46~45~44~43...| 12|\n",
      "|           12|1998-01-15|13~48~47~46~45~44...| 13|\n",
      "|           13|1998-01-15|14~49~48~47~46~45...| 14|\n",
      "|           14|1998-01-15|15~50~49~48~47~46...| 15|\n",
      "|           15|1998-01-15|16~51~50~49~48~47...| 16|\n",
      "|           16|1998-01-15|17~52~51~50~49~48...| 17|\n",
      "|           17|1998-01-15|18~53~52~51~50~49...| 18|\n",
      "|           18|1998-01-15|19~54~53~52~51~50...| 19|\n",
      "|           19|1998-01-15|20~55~54~53~52~51...| 20|\n",
      "|           20|1998-01-15|21~56~55~54~53~52...| 21|\n",
      "+-------------+----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Processing month: 1998-02-01 ...\n",
      "+-------------+----------+--------------------+---+\n",
      "|CONS_ACCT_KEY|   ACCT_DT|            DPD_GRID|DPD|\n",
      "+-------------+----------+--------------------+---+\n",
      "|            1|1998-02-15|3~37~36~35~34~33~...|  3|\n",
      "|            2|1998-02-15|4~38~37~36~35~34~...|  4|\n",
      "|            3|1998-02-15|5~39~38~37~36~35~...|  5|\n",
      "|            4|1998-02-15|6~40~39~38~37~36~...|  6|\n",
      "|            5|1998-02-15|7~41~40~39~38~37~...|  7|\n",
      "|            6|1998-02-15|8~42~41~40~39~38~...|  8|\n",
      "|            7|1998-02-15|9~43~42~41~40~39~...|  9|\n",
      "|            8|1998-02-15|10~44~43~42~41~40...| 10|\n",
      "|            9|1998-02-15|11~45~44~43~42~41...| 11|\n",
      "|           10|1998-02-15|12~46~45~44~43~42...| 12|\n",
      "|           11|1998-02-15|13~47~46~45~44~43...| 13|\n",
      "|           12|1998-02-15|14~48~47~46~45~44...| 14|\n",
      "|           13|1998-02-15|15~49~48~47~46~45...| 15|\n",
      "|           14|1998-02-15|16~50~49~48~47~46...| 16|\n",
      "|           15|1998-02-15|17~51~50~49~48~47...| 17|\n",
      "|           16|1998-02-15|18~52~51~50~49~48...| 18|\n",
      "|           17|1998-02-15|19~53~52~51~50~49...| 19|\n",
      "|           18|1998-02-15|20~54~53~52~51~50...| 20|\n",
      "|           19|1998-02-15|21~55~54~53~52~51...| 21|\n",
      "|           20|1998-02-15|22~56~55~54~53~52...| 22|\n",
      "+-------------+----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateDpdGrid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d9896df-ee68-4b11-a744-c475523a3468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---+--------------------+\n",
      "|CONS_ACCT_KEY|   ACCT_DT|DPD|            DPD_GRID|\n",
      "+-------------+----------+---+--------------------+\n",
      "|          336|1998-01-15| 37|37~?~?~?~?~?~?~?~...|\n",
      "|          336|1998-02-15| 38|38~37~?~?~?~?~?~?...|\n",
      "|          336|1998-03-15| 39|39~38~37~?~?~?~?~...|\n",
      "|          336|1998-05-15| 41|41~?~39~38~37~?~?...|\n",
      "|          336|1998-06-15| 42|42~41~?~39~38~37~...|\n",
      "|          336|1998-07-15| 43|43~42~41~?~39~38~...|\n",
      "|          336|1998-08-15| 44|44~43~42~41~?~39~...|\n",
      "|          336|1998-09-15| 45|45~44~43~42~41~?~...|\n",
      "|          336|1998-10-15| 46|46~45~44~43~42~41...|\n",
      "|          336|1998-11-15| 47|47~46~45~44~43~42...|\n",
      "|          336|1998-12-15| 48|48~47~46~45~44~43...|\n",
      "|          336|1999-01-15| 49|49~48~47~46~45~44...|\n",
      "|          336|1999-02-15| 50|50~49~48~47~46~45...|\n",
      "|          336|1999-03-15| 51|51~50~49~48~47~46...|\n",
      "|          336|1999-04-15| 52|52~51~50~49~48~47...|\n",
      "|          336|1999-05-15| 53|53~52~51~50~49~48...|\n",
      "|          336|1999-06-15| 54|54~53~52~51~50~49...|\n",
      "|          336|1999-07-15| 55|55~54~53~52~51~50...|\n",
      "|          336|1999-08-15| 56|56~55~54~53~52~51...|\n",
      "|          336|1999-09-15| 57|57~56~55~54~53~52...|\n",
      "+-------------+----------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "currentDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CONS_ACCT_KEY: int, ACCT_DT: date ... 2 more fields]\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val currentDf = spark.sql(s\"\"\"\n",
    "      SELECT *\n",
    "      FROM ascenddb.summary\n",
    "      WHERE CONS_ACCT_KEY =336\n",
    "    \"\"\").orderBy(col(\"ACCT_DT\"))\n",
    "\n",
    "    currentDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd5d38-a375-4964-933d-e2076475d99e",
   "metadata": {},
   "source": [
    "### Custom Optimizations I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c07a46ed-e81b-40b3-bd11-d353cb70c18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pad36: Seq[String] = List(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
       "trimAndPadUdf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$6606/0x0000000842129040@6611a0bb,ArrayType(StringType,true),List(Some(class[value[0]: array<string>])),Some(class[value[0]: array<string>]),None,true,true)\n",
       "generateDpdGrid: ()Unit\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pad36 = Seq.fill(36)(\"?\")\n",
    "\n",
    "val trimAndPadUdf = udf((arr: Seq[String]) => {\n",
    "  val padded = arr ++ pad36\n",
    "  padded.take(36)\n",
    "})\n",
    "\n",
    "def generateDpdGrid(): Unit = {\n",
    "  val startMonth = LocalDate.of(1998, 1, 1)\n",
    "  val endMonth = LocalDate.of(2000, 12, 15)\n",
    "  \n",
    "  var currentMonth = startMonth\n",
    "  \n",
    "  while (!currentMonth.isAfter(endMonth)) {\n",
    "    val nextMonth = currentMonth.plusMonths(1)\n",
    "    \n",
    "    println(s\"Processing month: $currentMonth ...\")\n",
    "    \n",
    "    val summaryColumnList = \"\"\"CONS_ACCT_KEY, ACCT_DT, DPD\"\"\"\n",
    "    \n",
    "    // Load current month data\n",
    "    val currentDf = spark.sql(s\"\"\"\n",
    "      SELECT $summaryColumnList\n",
    "      FROM ascenddb.dpd_data\n",
    "      WHERE ACCT_DT >= DATE '${currentMonth.toString}' AND ACCT_DT < DATE '${nextMonth.toString}'\n",
    "    \"\"\")\n",
    "\n",
    "    // Load only latest record per account\n",
    "    val latestPrev = spark.read.table(\"ascenddb.latest_dpd_summary\")\n",
    "      .withColumnRenamed(\"ACCT_DT\", \"ACCT_DT_prev\")\n",
    "    \n",
    "    // Join datasets\n",
    "    var mergedDf = currentDf.join(latestPrev, Seq(\"CONS_ACCT_KEY\"), \"left\")\n",
    "\n",
    "    // Calculate month difference and process DPD grid\n",
    "    mergedDf = mergedDf.withColumn(\"MONTH_DIFF\",\n",
    "      months_between(col(\"ACCT_DT\"), col(\"ACCT_DT_prev\")).cast(\"int\")\n",
    "    ).withColumn(\"FILLER_ARRAY\",\n",
    "      when(col(\"MONTH_DIFF\") > 1, \n",
    "        expr(\"transform(sequence(1, MONTH_DIFF - 1), x -> '?')\")\n",
    "      ).otherwise(array())\n",
    "    ).withColumn(\"Merged_DPD_Array\",\n",
    "          concat(\n",
    "            array(col(\"DPD\")),\n",
    "            col(\"FILLER_ARRAY\"),\n",
    "            when(col(\"DPD_GRID\").isNotNull, split(col(\"DPD_GRID\"), \"~\"))\n",
    "              .otherwise(array())\n",
    "          )\n",
    "        )\n",
    "\n",
    "    mergedDf = mergedDf.withColumn(\"DPD_Array_Trimmed\", \n",
    "                        trimAndPadUdf(col(\"Merged_DPD_Array\"))).withColumn(\"DPD_GRID\",\n",
    "                        concat_ws(\"~\", slice(col(\"DPD_Array_Trimmed\"), 1, 36))\n",
    "                      )\n",
    "                      \n",
    "\n",
    "    // Save historical output\n",
    "        val columnsToSelect = List(\n",
    "          \"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\", \"DPD\"\n",
    "        )\n",
    "        \n",
    "        mergedDf.select(columnsToSelect.head, columnsToSelect.tail: _*)\n",
    "          .write\n",
    "          .format(\"iceberg\")\n",
    "          .mode(\"append\")\n",
    "          .saveAsTable(\"ascenddb.summary\")\n",
    "        \n",
    "        // Load current month processed data\n",
    "        val currentMonthDf = mergedDf.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\")\n",
    "        \n",
    "        // Revert name of ACCT_DT_prev\n",
    "        val latestPrevRenamed = latestPrev.withColumnRenamed(\"ACCT_DT_prev\", \"ACCT_DT\")\n",
    "        \n",
    "        // Merge: Take latest between old and new per account\n",
    "        /* val windowSpec = Window.partitionBy(\"CONS_ACCT_KEY\").orderBy(col(\"ACCT_DT\").desc)\n",
    "        \n",
    "        val mergedLatest = latestPrevRenamed.union(currentMonthDf)\n",
    "          .withColumn(\"rn\", row_number().over(windowSpec))\n",
    "          .filter(col(\"rn\") === 1)\n",
    "          .drop(\"rn\")\n",
    "          */\n",
    "\n",
    "        val mergedLatest = latestPrevRenamed.union(currentMonthDf)\n",
    "                                              .groupBy(\"CONS_ACCT_KEY\")\n",
    "                                              .agg(\n",
    "                                                max_by(col(\"ACCT_DT\"), col(\"ACCT_DT\")).as(\"ACCT_DT\"),\n",
    "                                                max_by(col(\"DPD_GRID\"), col(\"ACCT_DT\")).as(\"DPD_GRID\")\n",
    "                                              )\n",
    "                                                    \n",
    "        // Save merged latest back\n",
    "        mergedLatest.write\n",
    "          .mode(\"overwrite\")\n",
    "          .saveAsTable(\"ascenddb.latest_dpd_summary\")\n",
    "    \n",
    "    // mergedDf.show(truncate=false)\n",
    "    // mergedDf.select(columnsToSelect.head, columnsToSelect.tail: _*).show(2)\n",
    "    print(\"Done\")\n",
    "    currentMonth = nextMonth\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92b501fb-0ad6-4bed-a2ee-1dc76fb31099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing month: 1998-01-01 ...\n",
      "DoneProcessing month: 1998-02-01 ...\n",
      "DoneProcessing month: 1998-03-01 ...\n",
      "DoneProcessing month: 1998-04-01 ...\n",
      "DoneProcessing month: 1998-05-01 ...\n",
      "DoneProcessing month: 1998-06-01 ...\n",
      "DoneProcessing month: 1998-07-01 ...\n",
      "DoneProcessing month: 1998-08-01 ...\n",
      "DoneProcessing month: 1998-09-01 ...\n",
      "DoneProcessing month: 1998-10-01 ...\n",
      "DoneProcessing month: 1998-11-01 ...\n",
      "DoneProcessing month: 1998-12-01 ...\n",
      "DoneProcessing month: 1999-01-01 ...\n",
      "DoneProcessing month: 1999-02-01 ...\n",
      "DoneProcessing month: 1999-03-01 ...\n",
      "DoneProcessing month: 1999-04-01 ...\n",
      "DoneProcessing month: 1999-05-01 ...\n",
      "DoneProcessing month: 1999-06-01 ...\n",
      "DoneProcessing month: 1999-07-01 ...\n",
      "DoneProcessing month: 1999-08-01 ...\n",
      "DoneProcessing month: 1999-09-01 ...\n",
      "DoneProcessing month: 1999-10-01 ...\n",
      "DoneProcessing month: 1999-11-01 ...\n",
      "DoneProcessing month: 1999-12-01 ...\n",
      "DoneProcessing month: 2000-01-01 ...\n",
      "DoneProcessing month: 2000-02-01 ...\n",
      "DoneProcessing month: 2000-03-01 ...\n",
      "DoneProcessing month: 2000-04-01 ...\n",
      "DoneProcessing month: 2000-05-01 ...\n",
      "DoneProcessing month: 2000-06-01 ...\n",
      "DoneProcessing month: 2000-07-01 ...\n",
      "DoneProcessing month: 2000-08-01 ...\n",
      "DoneProcessing month: 2000-09-01 ...\n",
      "DoneProcessing month: 2000-10-01 ...\n",
      "DoneProcessing month: 2000-11-01 ...\n",
      "DoneProcessing month: 2000-12-01 ...\n",
      "DoneTime: 157.20726943016052 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generateDpdGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e5669-17d6-4790-a131-e009cfced6a8",
   "metadata": {},
   "source": [
    "## Optimization II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1b38984-e1e3-4166-ab37-0a11186d3f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generateDpdGrid: ()Unit\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateDpdGrid(): Unit = {\n",
    "  val startMonth = LocalDate.of(1998, 1, 1)\n",
    "  val endMonth = LocalDate.of(2000, 12, 15)\n",
    "  \n",
    "  var currentMonth = startMonth\n",
    "  \n",
    "  while (!currentMonth.isAfter(endMonth)) {\n",
    "    val nextMonth = currentMonth.plusMonths(1)\n",
    "    \n",
    "    println(s\"Processing month: $currentMonth ...\")\n",
    "    \n",
    "    val summaryColumnList = \"\"\"CONS_ACCT_KEY, ACCT_DT, DPD\"\"\"\n",
    "    \n",
    "    // Load current month data\n",
    "    /*\n",
    "    val currentDf = spark.sql(s\"\"\"\n",
    "      SELECT $summaryColumnList\n",
    "      FROM ascenddb.dpd_data\n",
    "      WHERE ACCT_DT >= DATE '${currentMonth.toString}' AND ACCT_DT < DATE '${nextMonth.toString}'\n",
    "    \"\"\")\n",
    "    */\n",
    "\n",
    "    val currentDf = spark.table(\"ascenddb.dpd_data\")\n",
    "      .filter(col(\"ACCT_DT\").between(lit(currentMonth), lit(nextMonth.minusDays(1))))\n",
    "      .selectExpr(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD\")\n",
    "\n",
    "    // Load only latest record per account\n",
    "    val latestPrev = spark.read.table(\"ascenddb.latest_dpd_summary\")\n",
    "      .withColumnRenamed(\"ACCT_DT\", \"ACCT_DT_prev\")\n",
    "    \n",
    "    // Join datasets\n",
    "    var mergedDf = currentDf.join(latestPrev, Seq(\"CONS_ACCT_KEY\"), \"left\")\n",
    "\n",
    "    // Calculate month difference and process DPD grid\n",
    "    mergedDf = mergedDf.withColumn(\"MONTH_DIFF\",\n",
    "      months_between(col(\"ACCT_DT\"), col(\"ACCT_DT_prev\")).cast(\"int\")\n",
    "    ).withColumn(\"FILLER_ARRAY\",\n",
    "      when(col(\"MONTH_DIFF\") > 1, \n",
    "        expr(\"transform(sequence(1, MONTH_DIFF - 1), x -> '?')\")\n",
    "      ).otherwise(array())\n",
    "    ).withColumn(\"Merged_DPD_Array\",\n",
    "          concat(\n",
    "            array(col(\"DPD\")),\n",
    "            col(\"FILLER_ARRAY\"),\n",
    "            when(col(\"DPD_GRID\").isNotNull, split(col(\"DPD_GRID\"), \"~\"))\n",
    "              .otherwise(array())\n",
    "          )\n",
    "        )\n",
    "\n",
    "    mergedDf = mergedDf.withColumn(\"DPD_Array_Trimmed\",\n",
    "                                      slice(\n",
    "                                        concat(\n",
    "                                          col(\"Merged_DPD_Array\"),\n",
    "                                          expr(\"array_repeat('?', 36)\")\n",
    "                                        ),\n",
    "                                        1, 36\n",
    "                                      )\n",
    "                                    ).withColumn(\"DPD_GRID\",\n",
    "                        concat_ws(\"~\", slice(col(\"DPD_Array_Trimmed\"), 1, 36))\n",
    "                      )\n",
    "                      \n",
    "\n",
    "    // Save historical output\n",
    "        val columnsToSelect = List(\n",
    "          \"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\", \"DPD\"\n",
    "        )\n",
    "        \n",
    "        mergedDf.select(columnsToSelect.head, columnsToSelect.tail: _*)\n",
    "          .write\n",
    "          .format(\"iceberg\")\n",
    "          .mode(\"append\")\n",
    "          .saveAsTable(\"ascenddb.summary\")\n",
    "        \n",
    "        // Load current month processed data\n",
    "        val currentMonthDf = mergedDf.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\")\n",
    "        \n",
    "        // Revert name of ACCT_DT_prev\n",
    "        val latestPrevRenamed = latestPrev.withColumnRenamed(\"ACCT_DT_prev\", \"ACCT_DT\")\n",
    "        \n",
    "        // Merge: Take latest between old and new per account\n",
    "        /* val windowSpec = Window.partitionBy(\"CONS_ACCT_KEY\").orderBy(col(\"ACCT_DT\").desc)\n",
    "        \n",
    "        val mergedLatest = latestPrevRenamed.union(currentMonthDf)\n",
    "          .withColumn(\"rn\", row_number().over(windowSpec))\n",
    "          .filter(col(\"rn\") === 1)\n",
    "          .drop(\"rn\")\n",
    "          */\n",
    "\n",
    "        val mergedLatest = latestPrevRenamed.union(currentMonthDf)\n",
    "                                              .groupBy(\"CONS_ACCT_KEY\")\n",
    "                                              .agg(\n",
    "                                                max_by(col(\"ACCT_DT\"), col(\"ACCT_DT\")).as(\"ACCT_DT\"),\n",
    "                                                max_by(col(\"DPD_GRID\"), col(\"ACCT_DT\")).as(\"DPD_GRID\")\n",
    "                                              )\n",
    "                                                    \n",
    "        // Save merged latest back\n",
    "        mergedLatest.write\n",
    "          .mode(\"overwrite\")\n",
    "          .saveAsTable(\"ascenddb.latest_dpd_summary\")\n",
    "    \n",
    "    // mergedDf.show(truncate=false)\n",
    "    // mergedDf.select(columnsToSelect.head, columnsToSelect.tail: _*).show(2)\n",
    "    print(\"Done\")\n",
    "    currentMonth = nextMonth\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a24d00e0-0d49-473a-90d0-e96f122bef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing month: 1998-01-01 ...\n",
      "Time: 0.5732462406158447 seconds.\n",
      "\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException",
     "evalue": " [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `ascenddb`.`summary` because it already exists.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `ascenddb`.`summary` because it already exists.",
      "Choose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.",
      "  at org.apache.spark.sql.errors.QueryCompilationErrors$.tableAlreadyExistsError(QueryCompilationErrors.scala:1328)",
      "  at org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:116)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:634)",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)",
      "  at generateDpdGrid(<console>:106)",
      "  ... 40 elided",
      ""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generateDpdGrid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
