{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc453dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, date_format, expr, floor, to_date,date_sub,trunc, month, rand\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import date\n",
    "import pyspark.sql.functions as F\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ascend_DPD_Processing\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.9.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\") \\\n",
    "    .config(\"spark.sql.join.preferSortMergeJoin\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"ascend\") \\\n",
    "    .config(\"spark.sql.catalog.ascend\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.warehouse\", \"s3://lf-test-1-bucket/ascend-dpd\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "def generate_iceberg_schema():\n",
    "    spark.sql(\"\"\"create table if not exists ascenddb.accounts_all (\n",
    "                        CONS_ACCT_KEY BIGINT,\n",
    "                        ACCT_BAL_AM BIGINT,\n",
    "                        ACCT_CLOSED_DT DATE,\n",
    "                        ACCT_CREDIT_EXT_AM BIGINT,\n",
    "                        ACCT_DT DATE,\n",
    "                        ACCT_OPEN_DT DATE,\n",
    "                        ACCT_PYMT_STAT_CD VARCHAR(255),\n",
    "                        ACCT_PYMT_STAT_DTL_CD VARCHAR(255),\n",
    "                        ACCT_STAT_CD VARCHAR(255),\n",
    "                        ACCT_TYPE_DTL_CD VARCHAR(255),\n",
    "                        ACTUAL_PYMT_AM BIGINT,\n",
    "                        ASSET_CLASS_CD VARCHAR(255),\n",
    "                        BUREAU_MBR_ID VARCHAR(255),\n",
    "                        CASH_LIMIT_AM BIGINT,\n",
    "                        COLLATERAL_AM BIGINT,\n",
    "                        COLLATERAL_CD  VARCHAR(255),\n",
    "                        DAYS_PAST_DUE_CT INT,\n",
    "                        HI_CREDIT_AM BIGINT,\n",
    "                        INTEREST_RATE FLOAT,\n",
    "                        INTEREST_RATE_4IN FLOAT,\n",
    "                        LAST_PYMT_DT DATE,\n",
    "                        NEXT_SCHD_PYMT_AM BIGINT,\n",
    "                        ORIG_LOAN_AMT BIGINT,\n",
    "                        ORIG_PYMT_DUE_DT DATE,\n",
    "                        PAST_DUE_AM BIGINT,\n",
    "                        PORT_TYPE_CD  VARCHAR(255),\n",
    "                        PRINCIPAL_WRITE_OFF_AM BIGINT,\n",
    "                        PYMT_TERMS_CD  VARCHAR(255),\n",
    "                        PYMT_TERMS_DTL_CD  VARCHAR(255),\n",
    "                        SCHD_PYMT_DT DATE,\n",
    "                        SETTLED_AM BIGINT,\n",
    "                        SUIT_FILED_WILFUL_DEF_STAT_CD  VARCHAR(255),\n",
    "                        TOTAL_WRITE_OFF_AM BIGINT,\n",
    "                        WO_SETTLED_STAT_CD  VARCHAR(255),\n",
    "                        WRITE_OFF_AM BIGINT,\n",
    "                        WRITE_OFF_DT DATE,\n",
    "\n",
    "                        MSUBID VARCHAR(255),\n",
    "                        CREDITLIMITAM BIGINT,\n",
    "                        BALANCE_AM BIGINT,\n",
    "                        BALANCE_DT DATE,\n",
    "                        DFLTSTATUSDT DATE,\n",
    "                        RESPONSIBILITY_CD VARCHAR(255),\n",
    "                        CHARGEOFFAM BIGINT,\n",
    "                        EMI_AMT BIGINT,\n",
    "                        TENURE BIGINT,\n",
    "                        PAYMENTRATINGCD VARCHAR(255),\n",
    "                        PINCODE VARCHAR(255)\n",
    "\n",
    "                        )\n",
    "                        USING ICEBERG\n",
    "                            PARTITIONED BY (months(ACCT_DT))\"\"\")\n",
    "\n",
    "    print(\"Created schema for accounts_all\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE if not exists ascenddb.summary(\n",
    "                        CONS_ACCT_KEY BIGINT,\n",
    "                        BUREAU_MBR_ID VARCHAR(255),\n",
    "                        PORT_TYPE_CD VARCHAR(255),\n",
    "                        ACCT_TYPE_DTL_CD VARCHAR(255),\n",
    "                        ACCT_OPEN_DT DATE,\n",
    "                        ORIG_LOAN_AMT BIGINT,\n",
    "                        ACCT_CLOSED_DT DATE,\n",
    "                        PYMT_TERMS_CD VARCHAR(255),\n",
    "                        PYMT_TERMS_DTL_CD VARCHAR(255),\n",
    "                        ACCT_DT DATE,\n",
    "                        DPD_GRID STRING,\n",
    "                        ACCT_STAT_CD VARCHAR(255),\n",
    "                        ACCT_PYMT_STAT_CD VARCHAR(255),\n",
    "                        ACCT_PYMT_STAT_DTL_CD VARCHAR(255),\n",
    "                        ACCT_CREDIT_EXT_AM BIGINT,\n",
    "                        ACCT_BAL_AM BIGINT,\n",
    "                        PAST_DUE_AM BIGINT,\n",
    "                        ACTUAL_PYMT_AM BIGINT,\n",
    "                        LAST_PYMT_DT DATE,\n",
    "                        SCHD_PYMT_DT DATE,\n",
    "                        NEXT_SCHD_PYMT_AM BIGINT,\n",
    "                        INTEREST_RATE FLOAT,\n",
    "                        COLLATERAL_CD VARCHAR(255),\n",
    "                        ORIG_PYMT_DUE_DT DATE,\n",
    "                        WRITE_OFF_DT DATE,\n",
    "                        WRITE_OFF_AM BIGINT,\n",
    "                        ASSET_CLASS_CD VARCHAR(255),\n",
    "                        DAYS_PAST_DUE INT,\n",
    "                        DAYS_PAST_DUE_CT INT,\n",
    "                        HI_CREDIT_AM BIGINT,\n",
    "                        CASH_LIMIT_AM BIGINT,\n",
    "                        COLLATERAL_AM BIGINT,\n",
    "                        TOTAL_WRITE_OFF_AM BIGINT,\n",
    "                        PRINCIPAL_WRITE_OFF_AM BIGINT,\n",
    "                        SETTLED_AM BIGINT,\n",
    "                        INTEREST_RATE_4IN FLOAT,\n",
    "                        SUIT_FILED_WILFUL_DEF_STAT_CD VARCHAR(255),\n",
    "                        WO_SETTLED_STAT_CD VARCHAR(255),\n",
    "              \n",
    "                        MSUBID VARCHAR(255),\n",
    "                        CREDITLIMITAM BIGINT,\n",
    "                        BALANCE_AM BIGINT,\n",
    "                        BALANCE_DT DATE,\n",
    "                        DFLTSTATUSDT DATE,\n",
    "                        RESPONSIBILITY_CD VARCHAR(255),\n",
    "                        CHARGEOFFAM BIGINT,\n",
    "                        EMI_AMT BIGINT,\n",
    "                        TENURE BIGINT,\n",
    "                        PAYMENTRATINGCD VARCHAR(255),\n",
    "                        PINCODE VARCHAR(255)          \n",
    "        )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (months(ACCT_DT))\"\"\")\n",
    "\n",
    "    print(\"Created schema for summary\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE if not exists ascenddb.latest_dpd_summary(\n",
    "        CONS_ACCT_KEY INT,\n",
    "        ACCT_DT DATE, \n",
    "        DPD_GRID STRING\n",
    "        )\n",
    "    USING iceberg\"\"\")\n",
    "\n",
    "    print(\"Created schema for latest_dpd_summary\")\n",
    "\n",
    "def dummy_data_creation():\n",
    "    num_accounts = 720000000\n",
    "    # months_needed = 335  # e.g., 10 years\n",
    "    months_needed = 5  # e.g., 10 years\n",
    "    total_records = num_accounts * months_needed\n",
    "\n",
    "    # Step 1: Create DataFrame for all combinations\n",
    "    df = spark.range(0, total_records)\n",
    "\n",
    "    # Step 2: Generate account keys and month indices\n",
    "    df = df.withColumn(\"CONS_ACCT_KEY\", (col(\"id\") / months_needed).cast(\"int\") + 1) \\\n",
    "        .withColumn(\"month_index\", (col(\"id\") % months_needed).cast(\"int\"))\n",
    "\n",
    "    # Step 3: Generate date for each month (15th of month starting from Jan 2025)\n",
    "    start_date = datetime.date(2018, 1, 15)\n",
    "    dates = [start_date + relativedelta(months=i) for i in range(months_needed)]\n",
    "    date_map = spark.createDataFrame([(i, d.strftime(\"%m/%d/%Y\")) for i, d in enumerate(dates)],\n",
    "                                    [\"month_index\", \"ACCT_DT\"])\n",
    "\n",
    "    # Step 4: Join to get ACCT_DT\n",
    "    df = df.join(date_map, on=\"month_index\", how=\"left\")\n",
    "\n",
    "    # Step 5: Add DPD (random or deterministic)\n",
    "    df = df.withColumn(\"DAYS_PAST_DUE_CT\", ((col(\"CONS_ACCT_KEY\") + col(\"month_index\")) % 100 + 1))\n",
    "\n",
    "\n",
    "    # Add dummy columns\n",
    "    df = df.withColumn(\"BUREAU_MBR_ID\", expr(\"concat('MBR', CONS_ACCT_KEY)\")) \\\n",
    "           .withColumn(\"ACCT_BAL_AM\", (rand() * 100000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ACCT_CLOSED_DT\", to_date(lit(\"2025-12-31\"))) \\\n",
    "           .withColumn(\"ACCT_CREDIT_EXT_AM\", (rand() * 50000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ACCT_OPEN_DT\", to_date(lit(\"2018-01-01\"))) \\\n",
    "           .withColumn(\"ACCT_PYMT_STAT_CD\", lit(\"A\")) \\\n",
    "           .withColumn(\"ACCT_PYMT_STAT_DTL_CD\", lit(\"A1\")) \\\n",
    "           .withColumn(\"ACCT_STAT_CD\", lit(\"O\")) \\\n",
    "           .withColumn(\"ACCT_TYPE_DTL_CD\", lit(\"T1\")) \\\n",
    "           .withColumn(\"ACTUAL_PYMT_AM\", (rand() * 10000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ASSET_CLASS_CD\", lit(\"STD\")) \\\n",
    "           .withColumn(\"CASH_LIMIT_AM\", (rand() * 20000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"COLLATERAL_AM\", (rand() * 15000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"COLLATERAL_CD\", lit(\"C1\")) \\\n",
    "           .withColumn(\"HI_CREDIT_AM\", (rand() * 120000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"INTEREST_RATE\", (rand() * 15).cast(\"float\")) \\\n",
    "           .withColumn(\"INTEREST_RATE_4IN\", (rand() * 15).cast(\"float\")) \\\n",
    "           .withColumn(\"LAST_PYMT_DT\", to_date(lit(\"2025-06-30\"))) \\\n",
    "           .withColumn(\"NEXT_SCHD_PYMT_AM\", (rand() * 8000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ORIG_LOAN_AMT\", (rand() * 100000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ORIG_PYMT_DUE_DT\", to_date(lit(\"2025-01-15\"))) \\\n",
    "           .withColumn(\"PAST_DUE_AM\", (rand() * 5000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"PORT_TYPE_CD\", lit(\"PT1\")) \\\n",
    "           .withColumn(\"PRINCIPAL_WRITE_OFF_AM\", (rand() * 3000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"PYMT_TERMS_CD\", lit(\"M\")) \\\n",
    "           .withColumn(\"PYMT_TERMS_DTL_CD\", lit(\"M1\")) \\\n",
    "           .withColumn(\"SCHD_PYMT_DT\", to_date(lit(\"2025-07-15\"))) \\\n",
    "           .withColumn(\"SETTLED_AM\", (rand() * 2000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"SUIT_FILED_WILFUL_DEF_STAT_CD\", lit(\"N\")) \\\n",
    "           .withColumn(\"TOTAL_WRITE_OFF_AM\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"WO_SETTLED_STAT_CD\", lit(\"Y\")) \\\n",
    "           .withColumn(\"WRITE_OFF_AM\", (rand() * 3500).cast(\"bigint\")) \\\n",
    "           .withColumn(\"WRITE_OFF_DT\", to_date(lit(\"2025-05-31\"))) \\\n",
    "           .withColumn(\"CREDITLIMITAM\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"BALANCE_AM\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"CHARGEOFFAM\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"EMI_AMT\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"TENURE\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"MSUBID\", lit(\"MSUB\")) \\\n",
    "           .withColumn(\"RESPONSIBILITY_CD\", lit(\"PT1\")) \\\n",
    "           .withColumn(\"PAYMENTRATINGCD\", lit(\"PT1\")) \\\n",
    "           .withColumn(\"PINCODE\", lit(\"687562\")) \\\n",
    "           .withColumn(\"BALANCE_DT\", to_date(lit(\"2025-05-31\"))) \\\n",
    "           .withColumn(\"DFLTSTATUSDT\", to_date(lit(\"2025-05-31\")))\n",
    "\n",
    "    # Final selection\n",
    "    # df = df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DAYSPASTDUE\")\n",
    "\n",
    "\n",
    "    df = df.select(\n",
    "        \"CONS_ACCT_KEY\",\n",
    "        \"BUREAU_MBR_ID\",\n",
    "        \"PORT_TYPE_CD\",\n",
    "        \"ACCT_TYPE_DTL_CD\",\n",
    "        \"ACCT_OPEN_DT\",\n",
    "        \"ORIG_LOAN_AMT\",\n",
    "        \"ACCT_CLOSED_DT\",\n",
    "        \"PYMT_TERMS_CD\",\n",
    "        \"PYMT_TERMS_DTL_CD\",\n",
    "        \"ACCT_DT\",\n",
    "        \"ACCT_STAT_CD\",\n",
    "        \"ACCT_PYMT_STAT_CD\",\n",
    "        \"ACCT_PYMT_STAT_DTL_CD\",\n",
    "        \"ACCT_CREDIT_EXT_AM\",\n",
    "        \"ACCT_BAL_AM\",\n",
    "        \"PAST_DUE_AM\",\n",
    "        \"ACTUAL_PYMT_AM\",\n",
    "        \"LAST_PYMT_DT\",\n",
    "        \"SCHD_PYMT_DT\",\n",
    "        \"NEXT_SCHD_PYMT_AM\",\n",
    "        \"INTEREST_RATE\",\n",
    "        \"COLLATERAL_CD\",\n",
    "        \"ORIG_PYMT_DUE_DT\",\n",
    "        \"WRITE_OFF_DT\",\n",
    "        \"WRITE_OFF_AM\",\n",
    "        \"ASSET_CLASS_CD\",\n",
    "        \"DAYS_PAST_DUE_CT\",\n",
    "        \"HI_CREDIT_AM\",\n",
    "        \"CASH_LIMIT_AM\",\n",
    "        \"COLLATERAL_AM\",\n",
    "        \"TOTAL_WRITE_OFF_AM\",\n",
    "        \"PRINCIPAL_WRITE_OFF_AM\",\n",
    "        \"SETTLED_AM\",\n",
    "        \"INTEREST_RATE_4IN\",\n",
    "        \"SUIT_FILED_WILFUL_DEF_STAT_CD\",\n",
    "        \"WO_SETTLED_STAT_CD\",\n",
    "        \"MSUBID\",\n",
    "        \"CREDITLIMITAM\",\n",
    "        \"BALANCE_AM\",\n",
    "        \"BALANCE_DT\",\n",
    "        \"DFLTSTATUSDT\",\n",
    "        \"RESPONSIBILITY_CD\",\n",
    "        \"CHARGEOFFAM\",\n",
    "        \"EMI_AMT\",\n",
    "        \"TENURE\",\n",
    "        \"PAYMENTRATINGCD\",\n",
    "        \"PINCODE\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Verify uniqueness\n",
    "    print(\"Expected Unique Keys:\", df.count())\n",
    "    # print(\"Distinct (CONS_acct_KEY, ACCT_DT):\", df.select(\"CONS_acct_KEY\", \"ACCT_DT\").distinct().count())\n",
    "\n",
    "    print(f\"Existing Partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "    # df = df.repartition(6)\n",
    "    df = df.withColumn(\"ACCT_DT\",to_date(col(\"ACCT_DT\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "    df.printSchema()\n",
    "\n",
    "    # print(f\"New Partitions: {stage_df.rdd.getNumPartitions()}\")\n",
    "    # Save to Parquet\n",
    "    df.write\\\n",
    "    .format(\"iceberg\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"compression\", \"snappy\")\\\n",
    "    .saveAsTable(\"ascenddb.accounts_all\")\n",
    "\n",
    "    df.explain()\n",
    "    print(\"Data generated and saved to accounts_all\")\n",
    "\n",
    "def scenario_creation():\n",
    "    queries = [\n",
    "        \"DELETE from ascenddb.dpd_data where CONS_ACCT_KEY = 336 AND ACCT_DT = '2018-04-15'\",\n",
    "        \"DELETE FROM ascenddb.dpd_data WHERE CONS_ACCT_KEY = 337 AND ACCT_DT = '2018-01-15'\",\n",
    "        \"DELETE FROM ascenddb.dpd_data WHERE CONS_ACCT_KEY = 337 AND ACCT_DT = '2018-04-15'\",\n",
    "        \"DELETE FROM ascenddb.dpd_data WHERE CONS_ACCT_KEY = 337 AND ACCT_DT = '2018-05-15'\",\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        spark.sql(query)    \n",
    "    \n",
    "    print(\"Scenario Created\")\n",
    "\n",
    "def generate_dpd_grid():\n",
    "    start_month = datetime.date(2018, 1, 1)\n",
    "    end_month = datetime.date(2019, 10, 1)  # Adjust end date as needed\n",
    "\n",
    "    month = start_month\n",
    "\n",
    "    while month <= end_month:\n",
    "        next_month = (month.replace(day=28) + datetime.timedelta(days=4)).replace(day=1)\n",
    "        # print(f\"Next Month: {next_month}\")\n",
    "        \n",
    "        print(f\"Processing month: {month} ...\")\n",
    "\n",
    "        summary_column_list = \"CONS_ACCT_KEY, BUREAU_MBR_ID, PORT_TYPE_CD, ACCT_TYPE_DTL_CD, ACCT_OPEN_DT, ORIG_LOAN_AMT, ACCT_CLOSED_DT, PYMT_TERMS_CD, PYMT_TERMS_DTL_CD, ACCT_DT, ACCT_STAT_CD, ACCT_PYMT_STAT_CD, ACCT_PYMT_STAT_DTL_CD, ACCT_CREDIT_EXT_AM, ACCT_BAL_AM, PAST_DUE_AM, ACTUAL_PYMT_AM, LAST_PYMT_DT, SCHD_PYMT_DT, NEXT_SCHD_PYMT_AM, INTEREST_RATE, COLLATERAL_CD, ORIG_PYMT_DUE_DT, WRITE_OFF_DT, WRITE_OFF_AM, ASSET_CLASS_CD, DAYS_PAST_DUE_CT, HI_CREDIT_AM, CASH_LIMIT_AM, COLLATERAL_AM, TOTAL_WRITE_OFF_AM, PRINCIPAL_WRITE_OFF_AM, SETTLED_AM, INTEREST_RATE_4IN, SUIT_FILED_WILFUL_DEF_STAT_CD, WO_SETTLED_STAT_CD\"\n",
    "\n",
    "        # Load current month data\n",
    "        current_df = spark.sql(f\"\"\"\n",
    "            SELECT {summary_column_list}\n",
    "            FROM ascenddb.accounts_all\n",
    "            WHERE ACCT_DT >= DATE '{month}' AND ACCT_DT < DATE '{next_month}'\n",
    "        \"\"\")\n",
    "        \n",
    "        # Load only latest record per account (tiny table)\n",
    "        latest_prev = spark.read.table(\"ascenddb.latest_dpd_summary\").withColumnRenamed(\"ACCT_DT\", \"ACCT_DT_prev\")\n",
    "        \n",
    "        # Join (no longer need to scan historical months)\n",
    "        merged_df = current_df.join(latest_prev, on=\"CONS_ACCT_KEY\", how=\"left\")\n",
    "        \n",
    "        # Continue with DPD_GRID generation as before\n",
    "        merged_df = merged_df.withColumn(\n",
    "            \"MONTH_DIFF\",\n",
    "            (F.month(\"ACCT_DT\") - F.month(\"ACCT_DT_prev\")) +\n",
    "            (F.year(\"ACCT_DT\") - F.year(\"ACCT_DT_prev\")) * 12\n",
    "        ).withColumn(\n",
    "            \"FILLER_ARRAY\",\n",
    "            F.when(F.col(\"MONTH_DIFF\") > 1, F.expr(\"transform(sequence(1, MONTH_DIFF - 1), x -> '?')\"))\n",
    "            .otherwise(F.array())\n",
    "        ).withColumn(\n",
    "            \"Merged_DPD_Array\",\n",
    "            F.concat(\n",
    "                F.array(F.col(\"DAYS_PAST_DUE_CT\")),\n",
    "                F.col(\"FILLER_ARRAY\"),\n",
    "                F.when(F.col(\"DPD_GRID\").isNotNull(), F.split(F.col(\"DPD_GRID\"), \"~\")).otherwise(F.array())\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Pad & trim as usual\n",
    "        merged_df = merged_df.withColumn(\n",
    "            \"DPD_Array_Trimmed\",\n",
    "            F.when(\n",
    "                F.size(\"Merged_DPD_Array\") >= 36,\n",
    "                F.slice(\"Merged_DPD_Array\", 1, 36)\n",
    "            ).otherwise(\n",
    "                F.concat(F.col(\"Merged_DPD_Array\"),\n",
    "                        F.array([F.lit(\"?\") for _ in range(35)])\n",
    "                )\n",
    "            )\n",
    "        ).withColumn(\n",
    "            \"DPD_GRID\",\n",
    "            F.concat_ws(\"~\", \"DPD_Array_Trimmed\")\n",
    "        )\n",
    "\n",
    "        merged_df = merged_df.withColumn(\"DAYS_PAST_DUE\",F.col(\"DAYS_PAST_DUE_CT\"))\n",
    "        \n",
    "        # Save historical output (same as before)\n",
    "\n",
    "        merged_df.select( \"CONS_ACCT_KEY\", \"BUREAU_MBR_ID\", \"PORT_TYPE_CD\", \"ACCT_TYPE_DTL_CD\", \"ACCT_OPEN_DT\", \"ORIG_LOAN_AMT\", \"ACCT_CLOSED_DT\", \"PYMT_TERMS_CD\", \"PYMT_TERMS_DTL_CD\", \"ACCT_DT\", \"DPD_GRID\", \"ACCT_STAT_CD\", \"ACCT_PYMT_STAT_CD\", \"ACCT_PYMT_STAT_DTL_CD\", \"ACCT_CREDIT_EXT_AM\", \"ACCT_BAL_AM\", \"PAST_DUE_AM\", \"ACTUAL_PYMT_AM\", \"LAST_PYMT_DT\", \"SCHD_PYMT_DT\", \"NEXT_SCHD_PYMT_AM\", \"INTEREST_RATE\", \"COLLATERAL_CD\", \"ORIG_PYMT_DUE_DT\", \"WRITE_OFF_DT\", \"WRITE_OFF_AM\", \"ASSET_CLASS_CD\", \"DAYS_PAST_DUE\", \"DAYS_PAST_DUE_CT\", \"HI_CREDIT_AM\", \"CASH_LIMIT_AM\", \"COLLATERAL_AM\", \"TOTAL_WRITE_OFF_AM\", \"PRINCIPAL_WRITE_OFF_AM\", \"SETTLED_AM\", \"INTEREST_RATE_4IN\", \"SUIT_FILED_WILFUL_DEF_STAT_CD\", \"WO_SETTLED_STAT_CD\" )\\\n",
    "        .write.format(\"iceberg\").mode(\"append\").saveAsTable(\"ascenddb.summary\")\n",
    "\n",
    "\n",
    "        # merged_df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DAYS_PAST_DUE\", \"DPD_GRID\") \\\n",
    "        #     .write.format(\"iceberg\").mode(\"append\").saveAsTable(\"ascenddb.summary\")\n",
    "\n",
    "        # Load current month processed data\n",
    "        current_month_df = merged_df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\")\n",
    "\n",
    "        # Revert name of ACCT_DT_prev\n",
    "        latest_prev = spark.read.table(\"ascenddb.latest_dpd_summary\")\n",
    "        \n",
    "        # Merge: Take latest between old and new per account\n",
    "        merged_latest = latest_prev.union(current_month_df) \\\n",
    "            .withColumn(\"rn\", F.row_number().over(\n",
    "                Window.partitionBy(\"CONS_ACCT_KEY\").orderBy(F.col(\"ACCT_DT\").desc())\n",
    "            )) \\\n",
    "            .filter(\"rn = 1\") \\\n",
    "            .drop(\"rn\")\n",
    "        \n",
    "        # Save merged latest back\n",
    "        merged_latest.write.mode(\"overwrite\").saveAsTable(\"ascenddb.latest_dpd_summary\")\n",
    "        \n",
    "        month = next_month    \n",
    "\n",
    "generate_iceberg_schema()\n",
    "dummy_data_creation()\n",
    "scenario_creation()\n",
    "generate_dpd_grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be187871",
   "metadata": {},
   "source": [
    "### Test Snapshot Based Single Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2513056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, date_format, expr, floor, to_date,date_sub,trunc, month, rand\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import date\n",
    "import pyspark.sql.functions as F\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql import Window\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ascend_DPD_Processing\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.9.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\") \\\n",
    "    .config(\"spark.sql.join.preferSortMergeJoin\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"ascend\") \\\n",
    "    .config(\"spark.sql.catalog.ascend\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.warehouse\", \"s3://lf-test-1-bucket/ascend-dpd\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "def generate_bureau_trade():\n",
    "\n",
    "    snapshot_month = datetime.date(2020, 12, 1)\n",
    "    snapshot_end_date = (snapshot_month.replace(day=28) + datetime.timedelta(days=4)).replace(day=1)\n",
    "    snapshot_start_date = snapshot_month.replace(day=1)\n",
    "\n",
    "    max_months=36\n",
    "\n",
    "    end_date = (snapshot_month.replace(day=28) + datetime.timedelta(days=4)).replace(day=1)\n",
    "    start_date = (snapshot_month - relativedelta(months=36)).replace(day=1)\n",
    "\n",
    "\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "        WITH monthly_data AS (\n",
    "            SELECT \n",
    "                CONS_ACCT_KEY AS ACCT_KEY,\n",
    "                ACCT_DT AS SNAPSHOT_DT,\n",
    "                DAYS_PAST_DUE_CT AS DAYSPASTDUE,\n",
    "                DATE_TRUNC('month', ACCT_DT) as month_year\n",
    "            FROM ascenddb.accounts_all\n",
    "            WHERE \n",
    "            -- CONS_ACCT_KEY = 336 AND\n",
    "            ACCT_DT >= DATE '{start_date}'\n",
    "            AND ACCT_DT < DATE '{end_date}'\n",
    "        ),\n",
    "        \n",
    "        -- Create a map of month -> DPD for easy lookup\n",
    "        month_dpd_map AS (\n",
    "            SELECT \n",
    "                ACCT_KEY,\n",
    "                MAP_FROM_ARRAYS(\n",
    "                    COLLECT_LIST(month_year),\n",
    "                    COLLECT_LIST(DAYSPASTDUE)\n",
    "                ) as month_to_dpd_map,\n",
    "                MIN(month_year) as first_month,\n",
    "                MAX(month_year) as last_month\n",
    "            FROM monthly_data\n",
    "            GROUP BY ACCT_KEY\n",
    "        ),\n",
    "        \n",
    "        -- Join back to get arrays for each row\n",
    "        array_builder AS (\n",
    "            SELECT \n",
    "                md.ACCT_KEY,\n",
    "                md.SNAPSHOT_DT,\n",
    "                md.DAYSPASTDUE,\n",
    "                md.month_year,\n",
    "                mm.month_to_dpd_map,\n",
    "                mm.first_month,\n",
    "                -- Calculate months from first to current\n",
    "                CAST(MONTHS_BETWEEN(md.month_year, mm.first_month) AS INT) as months_from_start,\n",
    "                -- Build complete array with gaps filled\n",
    "                TRANSFORM(\n",
    "                    SEQUENCE(0, CAST(MONTHS_BETWEEN(md.month_year, mm.first_month) AS INT)),\n",
    "                    i -> COALESCE(\n",
    "                        mm.month_to_dpd_map[ADD_MONTHS(mm.first_month, i)],\n",
    "                        '?'\n",
    "                    )\n",
    "                ) as complete_history\n",
    "            FROM monthly_data md\n",
    "            JOIN month_dpd_map mm ON md.ACCT_KEY = mm.ACCT_KEY\n",
    "        ),\n",
    "        \n",
    "        final_arrays AS (\n",
    "            SELECT \n",
    "                ACCT_KEY,\n",
    "                SNAPSHOT_DT,\n",
    "                DAYSPASTDUE,\n",
    "                month_year,\n",
    "                -- Reverse (most recent first) and limit/pad\n",
    "                CONCAT_WS(\"~\",\n",
    "                            CASE \n",
    "                                WHEN SIZE(complete_history) > {max_months} THEN\n",
    "                                    SLICE(REVERSE(complete_history), 1, {max_months})\n",
    "                                ELSE\n",
    "                                    CONCAT(\n",
    "                                        REVERSE(complete_history),\n",
    "                                        ARRAY_REPEAT('?', {max_months} - SIZE(complete_history))\n",
    "                                    )\n",
    "                            END) as DPD_GRID\n",
    "            FROM array_builder\n",
    "        )\n",
    "        \n",
    "        SELECT \n",
    "            ACCT_KEY,\n",
    "            --SNAPSHOT_DT,\n",
    "            --DAYSPASTDUE,\n",
    "            DPD_GRID\n",
    "        FROM final_arrays\n",
    "        WHERE SNAPSHOT_DT >= DATE '{snapshot_start_date}' AND SNAPSHOT_DT < DATE '{snapshot_end_date}'\n",
    "        ORDER BY ACCT_KEY, month_year\n",
    "    \"\"\")\n",
    "\n",
    "    assoc_df = spark.sql(\"\"\"SELECT \n",
    "                                CONS_ACCT_KEY AS ACCT_KEY,\n",
    "                                CAST(crc32(CAST(CAST(PIN AS STRING) AS BINARY)) AS STRING) AS CUSTOMER_ID \n",
    "                         FROM ascenddb.assoc_a\"\"\")\n",
    "    \n",
    "    result_df = result_df.join(assoc_df, on = 'ACCT_KEY', how=\"left\")\n",
    "\n",
    "    snapshot_df = spark.sql(f\"\"\"\n",
    "                                SELECT \n",
    "                                    CONS_ACCT_KEY AS ACCT_KEY,\n",
    "                                    ACCT_TYPE_DTL_CD AS ACCTTYPECD,\n",
    "                                    ACCT_OPEN_DT AS OPEN_DT,\n",
    "                                    ORIG_LOAN_AMT AS ORIGLOANAM,\n",
    "                                    ACCT_DT AS SNAPSHOT_DT,\n",
    "                                    DAYS_PAST_DUE_CT AS DAYSPASTDUE,\n",
    "                                    ASSET_CLASS_CD AS ASSETCLASSCD,\n",
    "                                    PAST_DUE_AM AS PASTDUEAM,\n",
    "                                    ACCT_STAT_CD AS ACCOUNT_STATUS,\n",
    "                                    ACCT_CLOSED_DT AS CLOSED_DT,\n",
    "                                    ACTUAL_PYMT_AM AS ACTUALPAYMENTAM,\n",
    "                                    LAST_PYMT_DT AS LASTPAYMENTDT,\n",
    "                                    SUIT_FILED_WILFUL_DEF_STAT_CD AS SUITFILEDWILLFUL_DFLT,\n",
    "                                    WO_SETTLED_STAT_CD AS WRITTENOFFANDSETTLEDSTATUS,\n",
    "                                    WRITE_OFF_DT AS WRITEOFFSTATUS_DT,\n",
    "                                    TOTAL_WRITE_OFF_AM AS TOTALWRITEOFF_AMT,\n",
    "                                    PRINCIPAL_WRITE_OFF_AM AS PRINCIPLEWRITEOFF_AMT,\n",
    "                                    SETTLED_AM AS SETTLED_AMT,\n",
    "                                    INTEREST_RATE AS INTEREST_RATE,\n",
    "                                    PORT_TYPE_CD AS PORTFOLIORATINGTYPE_CD,\n",
    "                                    COLLATERAL_AM AS COLLATERAL_AMT,\n",
    "                                    COLLATERAL_CD AS COLLATERAL_CD,\n",
    "                                    MSUBID,\n",
    "                                    CREDITLIMITAM,\n",
    "                                    BALANCE_AM,\n",
    "                                    BALANCE_DT,\n",
    "                                    DFLTSTATUSDT,\n",
    "                                    RESPONSIBILITY_CD,\n",
    "                                    CHARGEOFFAM,\n",
    "                                    EMI_AMT,\n",
    "                                    TENURE,\n",
    "                                    PAYMENTRATINGCD,\n",
    "                                    PINCODE,\n",
    "                                    DATE_TRUNC('month', ACCT_DT) as month_year\n",
    "                                FROM ascenddb.accounts_all\n",
    "                                WHERE \n",
    "                                -- CONS_ACCT_KEY = 336 AND\n",
    "                                ACCT_DT >= DATE '{snapshot_start_date}'\n",
    "                                AND ACCT_DT < DATE '{snapshot_end_date}'\n",
    "                            \"\"\")\n",
    "\n",
    "    result_df = snapshot_df.join(result_df, on = 'ACCT_KEY', how=\"left\")\n",
    "\n",
    "\n",
    "    result_df.select(\n",
    "                        \"CUSTOMER_ID\",\n",
    "                        \"ACCT_KEY\",\n",
    "                        \"ACCTTYPECD\",\n",
    "                        \"OPEN_DT\",\n",
    "                        \"ORIGLOANAM\",\n",
    "                        \"SNAPSHOT_DT\",\n",
    "                        \"DAYSPASTDUE\",\n",
    "                        \"ASSETCLASSCD\",\n",
    "                        \"PASTDUEAM\",\n",
    "                        \"ACCOUNT_STATUS\",\n",
    "                        \"CLOSED_DT\",\n",
    "                        \"ACTUALPAYMENTAM\",\n",
    "                        \"LASTPAYMENTDT\",\n",
    "                        \"SUITFILEDWILLFUL_DFLT\",\n",
    "                        \"WRITTENOFFANDSETTLEDSTATUS\",\n",
    "                        \"WRITEOFFSTATUS_DT\",\n",
    "                        \"TOTALWRITEOFF_AMT\",\n",
    "                        \"PRINCIPLEWRITEOFF_AMT\",\n",
    "                        \"SETTLED_AMT\",\n",
    "                        \"INTEREST_RATE\",\n",
    "                        \"PORTFOLIORATINGTYPE_CD\",\n",
    "                        \"COLLATERAL_AMT\",\n",
    "                        \"COLLATERAL_CD\",\n",
    "                        \"DPD_GRID\",\n",
    "                        \"MSUBID\",\n",
    "                        \"CREDITLIMITAM\",\n",
    "                        \"BALANCE_AM\",\n",
    "                        \"BALANCE_DT\",\n",
    "                        \"DFLTSTATUSDT\",\n",
    "                        \"RESPONSIBILITY_CD\",\n",
    "                        \"CHARGEOFFAM\",\n",
    "                        \"EMI_AMT\",\n",
    "                        \"TENURE\",\n",
    "                        \"PAYMENTRATINGCD\",\n",
    "                        \"PINCODE\")\\\n",
    "    .write.mode(\"overwrite\").parquet('s3://lf-test-1-bucket/ascend-dpd/bureau_trade.parquet', compression='snappy')\n",
    "\n",
    "    print(f\"Generated Bureau Trade Data for {snapshot_month}\")\n",
    "\n",
    "generate_bureau_trade()\n",
    "\n",
    "end_time = time.time()\n",
    "total_minutes = (end_time-start_time)/60\n",
    "print(f\"Execution Time :{total_minutes:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b7cb1e",
   "metadata": {},
   "source": [
    "### Test Snapshot Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, date_format, expr, floor, to_date,date_sub,trunc, month, rand\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import date\n",
    "import pyspark.sql.functions as F\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ascend_DPD_Processing\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.9.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\") \\\n",
    "    .config(\"spark.sql.join.preferSortMergeJoin\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"ascend\") \\\n",
    "    .config(\"spark.sql.catalog.ascend\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.warehouse\", \"s3://lf-test-1-bucket/ascend-dpd\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"268435456\") \\\n",
    "    .config(\"spark.sql.parquet.block.size\", \"268435456\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def generate_dpd_grid():\n",
    "    start_month = datetime.date(2018, 1, 1)\n",
    "    end_month = datetime.date(2020, 12, 15)  # Adjust end date as needed\n",
    "\n",
    "    month = start_month\n",
    "\n",
    "    while month <= end_month:\n",
    "        next_month = (month.replace(day=28) + datetime.timedelta(days=4)).replace(day=1)\n",
    "        # print(f\"Next Month: {next_month}\")\n",
    "        \n",
    "        print(f\"Processing month: {month} ...\")\n",
    "\n",
    "        summary_column_list = \"\"\"CONS_ACCT_KEY, BUREAU_MBR_ID, PORT_TYPE_CD, ACCT_TYPE_DTL_CD, ACCT_OPEN_DT, ORIG_LOAN_AMT, ACCT_CLOSED_DT, PYMT_TERMS_CD, PYMT_TERMS_DTL_CD, ACCT_DT, ACCT_STAT_CD, ACCT_PYMT_STAT_CD, ACCT_PYMT_STAT_DTL_CD, ACCT_CREDIT_EXT_AM, ACCT_BAL_AM, PAST_DUE_AM, ACTUAL_PYMT_AM, LAST_PYMT_DT, SCHD_PYMT_DT, NEXT_SCHD_PYMT_AM, INTEREST_RATE, COLLATERAL_CD, ORIG_PYMT_DUE_DT, WRITE_OFF_DT, WRITE_OFF_AM, ASSET_CLASS_CD, DAYS_PAST_DUE_CT, HI_CREDIT_AM, CASH_LIMIT_AM, COLLATERAL_AM, TOTAL_WRITE_OFF_AM, PRINCIPAL_WRITE_OFF_AM, SETTLED_AM, INTEREST_RATE_4IN, SUIT_FILED_WILFUL_DEF_STAT_CD, WO_SETTLED_STAT_CD, MSUBID, CREDITLIMITAM, BALANCE_AM, BALANCE_DT, DFLTSTATUSDT, RESPONSIBILITY_CD, CHARGEOFFAM, EMI_AMT, TENURE, PAYMENTRATINGCD, PINCODE \"\"\"\n",
    "        \n",
    "        # Load current month data\n",
    "        current_df = spark.sql(f\"\"\"\n",
    "            SELECT {summary_column_list}\n",
    "            FROM ascenddb.accounts_all\n",
    "            WHERE ACCT_DT >= DATE '{month}' AND ACCT_DT < DATE '{next_month}'\n",
    "        \"\"\")\n",
    "        \n",
    "        # Load only latest record per account (tiny table)\n",
    "        latest_prev = spark.read.table(\"ascenddb.latest_dpd_summary\").withColumnRenamed(\"ACCT_DT\", \"ACCT_DT_prev\")\n",
    "        \n",
    "        # Join (no longer need to scan historical months)\n",
    "        merged_df = current_df.join(latest_prev, on=\"CONS_ACCT_KEY\", how=\"left\")\n",
    "        \n",
    "        # Continue with DPD_GRID generation as before\n",
    "        merged_df = merged_df.withColumn(\n",
    "            \"MONTH_DIFF\",\n",
    "            (F.month(\"ACCT_DT\") - F.month(\"ACCT_DT_prev\")) +\n",
    "            (F.year(\"ACCT_DT\") - F.year(\"ACCT_DT_prev\")) * 12\n",
    "        ).withColumn(\n",
    "            \"FILLER_ARRAY\",\n",
    "            F.when(F.col(\"MONTH_DIFF\") > 1, F.expr(\"transform(sequence(1, MONTH_DIFF - 1), x -> '?')\"))\n",
    "            .otherwise(F.array())\n",
    "        ).withColumn(\n",
    "            \"Merged_DPD_Array\",\n",
    "            F.concat(\n",
    "                F.array(F.col(\"DAYS_PAST_DUE_CT\")),\n",
    "                F.col(\"FILLER_ARRAY\"),\n",
    "                F.when(F.col(\"DPD_GRID\").isNotNull(), F.split(F.col(\"DPD_GRID\"), \"~\")).otherwise(F.array())\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Pad & trim as usual\n",
    "        merged_df = merged_df.withColumn(\n",
    "            \"DPD_Array_Trimmed\",\n",
    "            F.when(\n",
    "                F.size(\"Merged_DPD_Array\") >= 36,\n",
    "                F.slice(\"Merged_DPD_Array\", 1, 36)\n",
    "            ).otherwise(\n",
    "                F.concat(F.col(\"Merged_DPD_Array\"),\n",
    "                        F.array([F.lit(\"?\") for _ in range(35)])\n",
    "                )\n",
    "            )\n",
    "        ).withColumn(\n",
    "            \"DPD_GRID\",\n",
    "            F.concat_ws(\"~\", \"DPD_Array_Trimmed\")\n",
    "        )\n",
    "\n",
    "        merged_df = merged_df.withColumn(\"DAYS_PAST_DUE\",F.col(\"DAYS_PAST_DUE_CT\"))\n",
    "        \n",
    "        # Save historical output (same as before)\n",
    "\n",
    "        merged_df.select('CONS_ACCT_KEY', 'BUREAU_MBR_ID', 'PORT_TYPE_CD', 'ACCT_TYPE_DTL_CD', 'ACCT_OPEN_DT', 'ORIG_LOAN_AMT', 'ACCT_CLOSED_DT', 'PYMT_TERMS_CD', 'PYMT_TERMS_DTL_CD', 'ACCT_DT', 'ACCT_STAT_CD', 'ACCT_PYMT_STAT_CD', 'ACCT_PYMT_STAT_DTL_CD', 'ACCT_CREDIT_EXT_AM', 'ACCT_BAL_AM', 'PAST_DUE_AM', 'ACTUAL_PYMT_AM', 'LAST_PYMT_DT', 'SCHD_PYMT_DT', 'NEXT_SCHD_PYMT_AM', 'INTEREST_RATE', 'COLLATERAL_CD', 'ORIG_PYMT_DUE_DT', 'WRITE_OFF_DT', 'WRITE_OFF_AM', 'ASSET_CLASS_CD', 'DAYS_PAST_DUE_CT', 'HI_CREDIT_AM', 'CASH_LIMIT_AM', 'COLLATERAL_AM', 'TOTAL_WRITE_OFF_AM', 'PRINCIPAL_WRITE_OFF_AM', 'SETTLED_AM', 'INTEREST_RATE_4IN', 'SUIT_FILED_WILFUL_DEF_STAT_CD', 'WO_SETTLED_STAT_CD', 'MSUBID', 'CREDITLIMITAM', 'BALANCE_AM', 'BALANCE_DT', 'DFLTSTATUSDT', 'RESPONSIBILITY_CD', 'CHARGEOFFAM', 'EMI_AMT', 'TENURE', 'PAYMENTRATINGCD', 'PINCODE','DPD_GRID','DAYS_PAST_DUE' )\\\n",
    "        .write.format(\"iceberg\").mode(\"append\").saveAsTable(\"ascenddb.summary\")\n",
    "\n",
    "\n",
    "        # merged_df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DAYS_PAST_DUE\", \"DPD_GRID\") \\\n",
    "        #     .write.format(\"iceberg\").mode(\"append\").saveAsTable(\"ascenddb.summary\")\n",
    "\n",
    "        # Load current month processed data\n",
    "        current_month_df = merged_df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\")\n",
    "\n",
    "        # Revert name of ACCT_DT_prev\n",
    "        latest_prev = latest_prev.withColumnRenamed(\"ACCT_DT_prev\", \"ACCT_DT\")\n",
    "        \n",
    "        # Merge: Take latest between old and new per account\n",
    "        merged_latest = latest_prev.union(current_month_df) \\\n",
    "            .withColumn(\"rn\", F.row_number().over(\n",
    "                Window.partitionBy(\"CONS_ACCT_KEY\").orderBy(F.col(\"ACCT_DT\").desc())\n",
    "            )) \\\n",
    "            .filter(\"rn = 1\") \\\n",
    "            .drop(\"rn\")\n",
    "        \n",
    "        # Save merged latest back\n",
    "        merged_latest.write.mode(\"overwrite\").saveAsTable(\"ascenddb.latest_dpd_summary\")\n",
    "        \n",
    "        month = next_month    \n",
    "\n",
    "# generate_iceberg_schema()\n",
    "# dummy_data_creation()\n",
    "# scenario_creation()\n",
    "generate_dpd_grid()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
