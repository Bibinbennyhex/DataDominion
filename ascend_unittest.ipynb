{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc453dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, date_format, expr, floor, to_date,date_sub,trunc, month, rand\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import date\n",
    "import pyspark.sql.functions as F\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ascend_DPD_Processing\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.9.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\") \\\n",
    "    .config(\"spark.sql.join.preferSortMergeJoin\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"ascend\") \\\n",
    "    .config(\"spark.sql.catalog.ascend\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.warehouse\", \"s3://lf-test-1-bucket/ascend-dpd\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.ascend.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "def generate_iceberg_schema():\n",
    "    spark.sql(\"\"\"create table if not exists ascenddb.accounts_all (\n",
    "                        CONS_ACCT_KEY BIGINT,\n",
    "                        ACCT_BAL_AM BIGINT,\n",
    "                        ACCT_CLOSED_DT DATE,\n",
    "                        ACCT_CREDIT_EXT_AM BIGINT,\n",
    "                        ACCT_DT DATE,\n",
    "                        ACCT_OPEN_DT DATE,\n",
    "                        ACCT_PYMT_STAT_CD VARCHAR(255),\n",
    "                        ACCT_PYMT_STAT_DTL_CD VARCHAR(255),\n",
    "                        ACCT_STAT_CD VARCHAR(255),\n",
    "                        ACCT_TYPE_DTL_CD VARCHAR(255),\n",
    "                        ACTUAL_PYMT_AM BIGINT,\n",
    "                        ASSET_CLASS_CD VARCHAR(255),\n",
    "                        BUREAU_MBR_ID VARCHAR(255),\n",
    "                        CASH_LIMIT_AM BIGINT,\n",
    "                        COLLATERAL_AM BIGINT,\n",
    "                        COLLATERAL_CD  VARCHAR(255),\n",
    "                        DAYS_PAST_DUE_CT INT,\n",
    "                        HI_CREDIT_AM BIGINT,\n",
    "                        INTEREST_RATE FLOAT,\n",
    "                        INTEREST_RATE_4IN FLOAT,\n",
    "                        LAST_PYMT_DT DATE,\n",
    "                        NEXT_SCHD_PYMT_AM BIGINT,\n",
    "                        ORIG_LOAN_AMT BIGINT,\n",
    "                        ORIG_PYMT_DUE_DT DATE,\n",
    "                        PAST_DUE_AM BIGINT,\n",
    "                        PORT_TYPE_CD  VARCHAR(255),\n",
    "                        PRINCIPAL_WRITE_OFF_AM BIGINT,\n",
    "                        PYMT_TERMS_CD  VARCHAR(255),\n",
    "                        PYMT_TERMS_DTL_CD  VARCHAR(255),\n",
    "                        SCHD_PYMT_DT DATE,\n",
    "                        SETTLED_AM BIGINT,\n",
    "                        SUIT_FILED_WILFUL_DEF_STAT_CD  VARCHAR(255),\n",
    "                        TOTAL_WRITE_OFF_AM BIGINT,\n",
    "                        WO_SETTLED_STAT_CD  VARCHAR(255),\n",
    "                        WRITE_OFF_AM BIGINT,\n",
    "                        WRITE_OFF_DT DATE,\n",
    "\n",
    "                        MSUBID VARCHAR(255),\n",
    "                        CREDITLIMITAM BIGINT,\n",
    "                        BALANCE_AM BIGINT,\n",
    "                        BALANCE_DT DATE,\n",
    "                        DFLTSTATUSDT DATE,\n",
    "                        RESPONSIBILITY_CD VARCHAR(255),\n",
    "                        CHARGEOFFAM BIGINT,\n",
    "                        EMI_AMT BIGINT,\n",
    "                        TENURE BIGINT,\n",
    "                        PAYMENTRATINGCD VARCHAR(255),\n",
    "                        PINCODE VARCHAR(255)\n",
    "\n",
    "                        )\n",
    "                        USING ICEBERG\n",
    "                            PARTITIONED BY (months(ACCT_DT))\"\"\")\n",
    "\n",
    "    print(\"Created schema for accounts_all\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE if not exists ascenddb.summary(\n",
    "                        CONS_ACCT_KEY BIGINT,\n",
    "                        BUREAU_MBR_ID VARCHAR(255),\n",
    "                        PORT_TYPE_CD VARCHAR(255),\n",
    "                        ACCT_TYPE_DTL_CD VARCHAR(255),\n",
    "                        ACCT_OPEN_DT DATE,\n",
    "                        ORIG_LOAN_AMT BIGINT,\n",
    "                        ACCT_CLOSED_DT DATE,\n",
    "                        PYMT_TERMS_CD VARCHAR(255),\n",
    "                        PYMT_TERMS_DTL_CD VARCHAR(255),\n",
    "                        ACCT_DT DATE,\n",
    "                        DPD_GRID STRING,\n",
    "                        ACCT_STAT_CD VARCHAR(255),\n",
    "                        ACCT_PYMT_STAT_CD VARCHAR(255),\n",
    "                        ACCT_PYMT_STAT_DTL_CD VARCHAR(255),\n",
    "                        ACCT_CREDIT_EXT_AM BIGINT,\n",
    "                        ACCT_BAL_AM BIGINT,\n",
    "                        PAST_DUE_AM BIGINT,\n",
    "                        ACTUAL_PYMT_AM BIGINT,\n",
    "                        LAST_PYMT_DT DATE,\n",
    "                        SCHD_PYMT_DT DATE,\n",
    "                        NEXT_SCHD_PYMT_AM BIGINT,\n",
    "                        INTEREST_RATE FLOAT,\n",
    "                        COLLATERAL_CD VARCHAR(255),\n",
    "                        ORIG_PYMT_DUE_DT DATE,\n",
    "                        WRITE_OFF_DT DATE,\n",
    "                        WRITE_OFF_AM BIGINT,\n",
    "                        ASSET_CLASS_CD VARCHAR(255),\n",
    "                        DAYS_PAST_DUE INT,\n",
    "                        DAYS_PAST_DUE_CT INT,\n",
    "                        HI_CREDIT_AM BIGINT,\n",
    "                        CASH_LIMIT_AM BIGINT,\n",
    "                        COLLATERAL_AM BIGINT,\n",
    "                        TOTAL_WRITE_OFF_AM BIGINT,\n",
    "                        PRINCIPAL_WRITE_OFF_AM BIGINT,\n",
    "                        SETTLED_AM BIGINT,\n",
    "                        INTEREST_RATE_4IN FLOAT,\n",
    "                        SUIT_FILED_WILFUL_DEF_STAT_CD VARCHAR(255),\n",
    "                        WO_SETTLED_STAT_CD VARCHAR(255),\n",
    "              \n",
    "                        MSUBID VARCHAR(255),\n",
    "                        CREDITLIMITAM BIGINT,\n",
    "                        BALANCE_AM BIGINT,\n",
    "                        BALANCE_DT DATE,\n",
    "                        DFLTSTATUSDT DATE,\n",
    "                        RESPONSIBILITY_CD VARCHAR(255),\n",
    "                        CHARGEOFFAM BIGINT,\n",
    "                        EMI_AMT BIGINT,\n",
    "                        TENURE BIGINT,\n",
    "                        PAYMENTRATINGCD VARCHAR(255),\n",
    "                        PINCODE VARCHAR(255)          \n",
    "        )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (months(ACCT_DT))\"\"\")\n",
    "\n",
    "    print(\"Created schema for summary\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE if not exists ascenddb.latest_dpd_summary(\n",
    "        CONS_ACCT_KEY INT,\n",
    "        ACCT_DT DATE, \n",
    "        DPD_GRID STRING\n",
    "        )\n",
    "    USING iceberg\"\"\")\n",
    "\n",
    "    print(\"Created schema for latest_dpd_summary\")\n",
    "\n",
    "def dummy_data_creation():\n",
    "    num_accounts = 720000000\n",
    "    # months_needed = 335  # e.g., 10 years\n",
    "    months_needed = 5  # e.g., 10 years\n",
    "    total_records = num_accounts * months_needed\n",
    "\n",
    "    # Step 1: Create DataFrame for all combinations\n",
    "    df = spark.range(0, total_records)\n",
    "\n",
    "    # Step 2: Generate account keys and month indices\n",
    "    df = df.withColumn(\"CONS_ACCT_KEY\", (col(\"id\") / months_needed).cast(\"int\") + 1) \\\n",
    "        .withColumn(\"month_index\", (col(\"id\") % months_needed).cast(\"int\"))\n",
    "\n",
    "    # Step 3: Generate date for each month (15th of month starting from Jan 2025)\n",
    "    start_date = datetime.date(2018, 1, 15)\n",
    "    dates = [start_date + relativedelta(months=i) for i in range(months_needed)]\n",
    "    date_map = spark.createDataFrame([(i, d.strftime(\"%m/%d/%Y\")) for i, d in enumerate(dates)],\n",
    "                                    [\"month_index\", \"ACCT_DT\"])\n",
    "\n",
    "    # Step 4: Join to get ACCT_DT\n",
    "    df = df.join(date_map, on=\"month_index\", how=\"left\")\n",
    "\n",
    "    # Step 5: Add DPD (random or deterministic)\n",
    "    df = df.withColumn(\"DAYS_PAST_DUE_CT\", ((col(\"CONS_ACCT_KEY\") + col(\"month_index\")) % 100 + 1))\n",
    "\n",
    "\n",
    "    # Add dummy columns\n",
    "    df = df.withColumn(\"BUREAU_MBR_ID\", expr(\"concat('MBR', CONS_ACCT_KEY)\")) \\\n",
    "           .withColumn(\"ACCT_BAL_AM\", (rand() * 100000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ACCT_CLOSED_DT\", to_date(lit(\"2025-12-31\"))) \\\n",
    "           .withColumn(\"ACCT_CREDIT_EXT_AM\", (rand() * 50000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ACCT_OPEN_DT\", to_date(lit(\"2018-01-01\"))) \\\n",
    "           .withColumn(\"ACCT_PYMT_STAT_CD\", lit(\"A\")) \\\n",
    "           .withColumn(\"ACCT_PYMT_STAT_DTL_CD\", lit(\"A1\")) \\\n",
    "           .withColumn(\"ACCT_STAT_CD\", lit(\"O\")) \\\n",
    "           .withColumn(\"ACCT_TYPE_DTL_CD\", lit(\"T1\")) \\\n",
    "           .withColumn(\"ACTUAL_PYMT_AM\", (rand() * 10000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ASSET_CLASS_CD\", lit(\"STD\")) \\\n",
    "           .withColumn(\"CASH_LIMIT_AM\", (rand() * 20000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"COLLATERAL_AM\", (rand() * 15000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"COLLATERAL_CD\", lit(\"C1\")) \\\n",
    "           .withColumn(\"HI_CREDIT_AM\", (rand() * 120000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"INTEREST_RATE\", (rand() * 15).cast(\"float\")) \\\n",
    "           .withColumn(\"INTEREST_RATE_4IN\", (rand() * 15).cast(\"float\")) \\\n",
    "           .withColumn(\"LAST_PYMT_DT\", to_date(lit(\"2025-06-30\"))) \\\n",
    "           .withColumn(\"NEXT_SCHD_PYMT_AM\", (rand() * 8000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ORIG_LOAN_AMT\", (rand() * 100000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"ORIG_PYMT_DUE_DT\", to_date(lit(\"2025-01-15\"))) \\\n",
    "           .withColumn(\"PAST_DUE_AM\", (rand() * 5000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"PORT_TYPE_CD\", lit(\"PT1\")) \\\n",
    "           .withColumn(\"PRINCIPAL_WRITE_OFF_AM\", (rand() * 3000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"PYMT_TERMS_CD\", lit(\"M\")) \\\n",
    "           .withColumn(\"PYMT_TERMS_DTL_CD\", lit(\"M1\")) \\\n",
    "           .withColumn(\"SCHD_PYMT_DT\", to_date(lit(\"2025-07-15\"))) \\\n",
    "           .withColumn(\"SETTLED_AM\", (rand() * 2000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"SUIT_FILED_WILFUL_DEF_STAT_CD\", lit(\"N\")) \\\n",
    "           .withColumn(\"TOTAL_WRITE_OFF_AM\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"WO_SETTLED_STAT_CD\", lit(\"Y\")) \\\n",
    "           .withColumn(\"WRITE_OFF_AM\", (rand() * 3500).cast(\"bigint\")) \\\n",
    "           .withColumn(\"WRITE_OFF_DT\", to_date(lit(\"2025-05-31\"))) \\\n",
    "           .withColumn(\"CREDITLIMITAM\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"BALANCE_AM\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"CHARGEOFFAM\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"EMI_AMT\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"TENURE\", (rand() * 4000).cast(\"bigint\")) \\\n",
    "           .withColumn(\"MSUBID\", lit(\"MSUB\")) \\\n",
    "           .withColumn(\"RESPONSIBILITY_CD\", lit(\"PT1\")) \\\n",
    "           .withColumn(\"PAYMENTRATINGCD\", lit(\"PT1\")) \\\n",
    "           .withColumn(\"PINCODE\", lit(\"687562\")) \\\n",
    "           .withColumn(\"BALANCE_DT\", to_date(lit(\"2025-05-31\"))) \\\n",
    "           .withColumn(\"DFLTSTATUSDT\", to_date(lit(\"2025-05-31\")))\n",
    "\n",
    "    # Final selection\n",
    "    # df = df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DAYSPASTDUE\")\n",
    "\n",
    "\n",
    "    df = df.select(\n",
    "        \"CONS_ACCT_KEY\",\n",
    "        \"BUREAU_MBR_ID\",\n",
    "        \"PORT_TYPE_CD\",\n",
    "        \"ACCT_TYPE_DTL_CD\",\n",
    "        \"ACCT_OPEN_DT\",\n",
    "        \"ORIG_LOAN_AMT\",\n",
    "        \"ACCT_CLOSED_DT\",\n",
    "        \"PYMT_TERMS_CD\",\n",
    "        \"PYMT_TERMS_DTL_CD\",\n",
    "        \"ACCT_DT\",\n",
    "        \"ACCT_STAT_CD\",\n",
    "        \"ACCT_PYMT_STAT_CD\",\n",
    "        \"ACCT_PYMT_STAT_DTL_CD\",\n",
    "        \"ACCT_CREDIT_EXT_AM\",\n",
    "        \"ACCT_BAL_AM\",\n",
    "        \"PAST_DUE_AM\",\n",
    "        \"ACTUAL_PYMT_AM\",\n",
    "        \"LAST_PYMT_DT\",\n",
    "        \"SCHD_PYMT_DT\",\n",
    "        \"NEXT_SCHD_PYMT_AM\",\n",
    "        \"INTEREST_RATE\",\n",
    "        \"COLLATERAL_CD\",\n",
    "        \"ORIG_PYMT_DUE_DT\",\n",
    "        \"WRITE_OFF_DT\",\n",
    "        \"WRITE_OFF_AM\",\n",
    "        \"ASSET_CLASS_CD\",\n",
    "        \"DAYS_PAST_DUE_CT\",\n",
    "        \"HI_CREDIT_AM\",\n",
    "        \"CASH_LIMIT_AM\",\n",
    "        \"COLLATERAL_AM\",\n",
    "        \"TOTAL_WRITE_OFF_AM\",\n",
    "        \"PRINCIPAL_WRITE_OFF_AM\",\n",
    "        \"SETTLED_AM\",\n",
    "        \"INTEREST_RATE_4IN\",\n",
    "        \"SUIT_FILED_WILFUL_DEF_STAT_CD\",\n",
    "        \"WO_SETTLED_STAT_CD\",\n",
    "        \"MSUBID\",\n",
    "        \"CREDITLIMITAM\",\n",
    "        \"BALANCE_AM\",\n",
    "        \"BALANCE_DT\",\n",
    "        \"DFLTSTATUSDT\",\n",
    "        \"RESPONSIBILITY_CD\",\n",
    "        \"CHARGEOFFAM\",\n",
    "        \"EMI_AMT\",\n",
    "        \"TENURE\",\n",
    "        \"PAYMENTRATINGCD\",\n",
    "        \"PINCODE\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Verify uniqueness\n",
    "    print(\"Expected Unique Keys:\", df.count())\n",
    "    # print(\"Distinct (CONS_acct_KEY, ACCT_DT):\", df.select(\"CONS_acct_KEY\", \"ACCT_DT\").distinct().count())\n",
    "\n",
    "    print(f\"Existing Partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "    # df = df.repartition(6)\n",
    "    df = df.withColumn(\"ACCT_DT\",to_date(col(\"ACCT_DT\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "    df.printSchema()\n",
    "\n",
    "    # print(f\"New Partitions: {stage_df.rdd.getNumPartitions()}\")\n",
    "    # Save to Parquet\n",
    "    df.write\\\n",
    "    .format(\"iceberg\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"compression\", \"snappy\")\\\n",
    "    .saveAsTable(\"ascenddb.accounts_all\")\n",
    "\n",
    "    df.explain()\n",
    "    print(\"Data generated and saved to accounts_all\")\n",
    "\n",
    "def scenario_creation():\n",
    "    queries = [\n",
    "        \"DELETE from ascenddb.dpd_data where CONS_ACCT_KEY = 336 AND ACCT_DT = '2018-04-15'\",\n",
    "        \"DELETE FROM ascenddb.dpd_data WHERE CONS_ACCT_KEY = 337 AND ACCT_DT = '2018-01-15'\",\n",
    "        \"DELETE FROM ascenddb.dpd_data WHERE CONS_ACCT_KEY = 337 AND ACCT_DT = '2018-04-15'\",\n",
    "        \"DELETE FROM ascenddb.dpd_data WHERE CONS_ACCT_KEY = 337 AND ACCT_DT = '2018-05-15'\",\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        spark.sql(query)    \n",
    "    \n",
    "    print(\"Scenario Created\")\n",
    "\n",
    "def generate_dpd_grid():\n",
    "    start_month = datetime.date(2018, 1, 1)\n",
    "    end_month = datetime.date(2019, 10, 1)  # Adjust end date as needed\n",
    "\n",
    "    month = start_month\n",
    "\n",
    "    while month <= end_month:\n",
    "        next_month = (month.replace(day=28) + datetime.timedelta(days=4)).replace(day=1)\n",
    "        # print(f\"Next Month: {next_month}\")\n",
    "        \n",
    "        print(f\"Processing month: {month} ...\")\n",
    "\n",
    "        summary_column_list = \"CONS_ACCT_KEY, BUREAU_MBR_ID, PORT_TYPE_CD, ACCT_TYPE_DTL_CD, ACCT_OPEN_DT, ORIG_LOAN_AMT, ACCT_CLOSED_DT, PYMT_TERMS_CD, PYMT_TERMS_DTL_CD, ACCT_DT, ACCT_STAT_CD, ACCT_PYMT_STAT_CD, ACCT_PYMT_STAT_DTL_CD, ACCT_CREDIT_EXT_AM, ACCT_BAL_AM, PAST_DUE_AM, ACTUAL_PYMT_AM, LAST_PYMT_DT, SCHD_PYMT_DT, NEXT_SCHD_PYMT_AM, INTEREST_RATE, COLLATERAL_CD, ORIG_PYMT_DUE_DT, WRITE_OFF_DT, WRITE_OFF_AM, ASSET_CLASS_CD, DAYS_PAST_DUE_CT, HI_CREDIT_AM, CASH_LIMIT_AM, COLLATERAL_AM, TOTAL_WRITE_OFF_AM, PRINCIPAL_WRITE_OFF_AM, SETTLED_AM, INTEREST_RATE_4IN, SUIT_FILED_WILFUL_DEF_STAT_CD, WO_SETTLED_STAT_CD\"\n",
    "\n",
    "        # Load current month data\n",
    "        current_df = spark.sql(f\"\"\"\n",
    "            SELECT {summary_column_list}\n",
    "            FROM ascenddb.accounts_all\n",
    "            WHERE ACCT_DT >= DATE '{month}' AND ACCT_DT < DATE '{next_month}'\n",
    "        \"\"\")\n",
    "        \n",
    "        # Load only latest record per account (tiny table)\n",
    "        latest_prev = spark.read.table(\"ascenddb.latest_dpd_summary\").withColumnRenamed(\"ACCT_DT\", \"ACCT_DT_prev\")\n",
    "        \n",
    "        # Join (no longer need to scan historical months)\n",
    "        merged_df = current_df.join(latest_prev, on=\"CONS_ACCT_KEY\", how=\"left\")\n",
    "        \n",
    "        # Continue with DPD_GRID generation as before\n",
    "        merged_df = merged_df.withColumn(\n",
    "            \"MONTH_DIFF\",\n",
    "            (F.month(\"ACCT_DT\") - F.month(\"ACCT_DT_prev\")) +\n",
    "            (F.year(\"ACCT_DT\") - F.year(\"ACCT_DT_prev\")) * 12\n",
    "        ).withColumn(\n",
    "            \"FILLER_ARRAY\",\n",
    "            F.when(F.col(\"MONTH_DIFF\") > 1, F.expr(\"transform(sequence(1, MONTH_DIFF - 1), x -> '?')\"))\n",
    "            .otherwise(F.array())\n",
    "        ).withColumn(\n",
    "            \"Merged_DPD_Array\",\n",
    "            F.concat(\n",
    "                F.array(F.col(\"DAYS_PAST_DUE_CT\")),\n",
    "                F.col(\"FILLER_ARRAY\"),\n",
    "                F.when(F.col(\"DPD_GRID\").isNotNull(), F.split(F.col(\"DPD_GRID\"), \"~\")).otherwise(F.array())\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Pad & trim as usual\n",
    "        merged_df = merged_df.withColumn(\n",
    "            \"DPD_Array_Trimmed\",\n",
    "            F.when(\n",
    "                F.size(\"Merged_DPD_Array\") >= 36,\n",
    "                F.slice(\"Merged_DPD_Array\", 1, 36)\n",
    "            ).otherwise(\n",
    "                F.concat(F.col(\"Merged_DPD_Array\"),\n",
    "                        F.array([F.lit(\"?\") for _ in range(35)])\n",
    "                )\n",
    "            )\n",
    "        ).withColumn(\n",
    "            \"DPD_GRID\",\n",
    "            F.concat_ws(\"~\", \"DPD_Array_Trimmed\")\n",
    "        )\n",
    "\n",
    "        merged_df = merged_df.withColumn(\"DAYS_PAST_DUE\",F.col(\"DAYS_PAST_DUE_CT\"))\n",
    "        \n",
    "        # Save historical output (same as before)\n",
    "\n",
    "        merged_df.select( \"CONS_ACCT_KEY\", \"BUREAU_MBR_ID\", \"PORT_TYPE_CD\", \"ACCT_TYPE_DTL_CD\", \"ACCT_OPEN_DT\", \"ORIG_LOAN_AMT\", \"ACCT_CLOSED_DT\", \"PYMT_TERMS_CD\", \"PYMT_TERMS_DTL_CD\", \"ACCT_DT\", \"DPD_GRID\", \"ACCT_STAT_CD\", \"ACCT_PYMT_STAT_CD\", \"ACCT_PYMT_STAT_DTL_CD\", \"ACCT_CREDIT_EXT_AM\", \"ACCT_BAL_AM\", \"PAST_DUE_AM\", \"ACTUAL_PYMT_AM\", \"LAST_PYMT_DT\", \"SCHD_PYMT_DT\", \"NEXT_SCHD_PYMT_AM\", \"INTEREST_RATE\", \"COLLATERAL_CD\", \"ORIG_PYMT_DUE_DT\", \"WRITE_OFF_DT\", \"WRITE_OFF_AM\", \"ASSET_CLASS_CD\", \"DAYS_PAST_DUE\", \"DAYS_PAST_DUE_CT\", \"HI_CREDIT_AM\", \"CASH_LIMIT_AM\", \"COLLATERAL_AM\", \"TOTAL_WRITE_OFF_AM\", \"PRINCIPAL_WRITE_OFF_AM\", \"SETTLED_AM\", \"INTEREST_RATE_4IN\", \"SUIT_FILED_WILFUL_DEF_STAT_CD\", \"WO_SETTLED_STAT_CD\" )\\\n",
    "        .write.format(\"iceberg\").mode(\"append\").saveAsTable(\"ascenddb.summary\")\n",
    "\n",
    "\n",
    "        # merged_df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DAYS_PAST_DUE\", \"DPD_GRID\") \\\n",
    "        #     .write.format(\"iceberg\").mode(\"append\").saveAsTable(\"ascenddb.summary\")\n",
    "\n",
    "        # Load current month processed data\n",
    "        current_month_df = merged_df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\")\n",
    "\n",
    "        # Revert name of ACCT_DT_prev\n",
    "        latest_prev = spark.read.table(\"ascenddb.latest_dpd_summary\")\n",
    "        \n",
    "        # Merge: Take latest between old and new per account\n",
    "        merged_latest = latest_prev.union(current_month_df) \\\n",
    "            .withColumn(\"rn\", F.row_number().over(\n",
    "                Window.partitionBy(\"CONS_ACCT_KEY\").orderBy(F.col(\"ACCT_DT\").desc())\n",
    "            )) \\\n",
    "            .filter(\"rn = 1\") \\\n",
    "            .drop(\"rn\")\n",
    "        \n",
    "        # Save merged latest back\n",
    "        merged_latest.write.mode(\"overwrite\").saveAsTable(\"ascenddb.latest_dpd_summary\")\n",
    "        \n",
    "        month = next_month    \n",
    "\n",
    "generate_iceberg_schema()\n",
    "dummy_data_creation()\n",
    "scenario_creation()\n",
    "generate_dpd_grid()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
