{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db805426-ddb4-4d96-addc-a966e02f3eae",
   "metadata": {},
   "source": [
    "### Generate the lastest_dpd_summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1ef7209-f79f-4402-9c5d-478c0a45c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/09 17:16:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, date_format, expr, floor, to_date,date_sub,trunc, month\n",
    "from pyspark.sql.types import IntegerType\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('Ascend')\\\n",
    "                            .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\")\\\n",
    "                            .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\\\n",
    "                            .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\\\n",
    "                            .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\")\\\n",
    "                            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "                            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "                            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\\\n",
    "                            .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "                            .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\\\n",
    "                            .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\\\n",
    "                            .config(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\\\n",
    "                            .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Read full dpd_summary history (yes, one-time full scan)\n",
    "dpd_summary_df = spark.read.table(\"ascenddb2.dpd_summary\")\n",
    "\n",
    "# Window to pick latest ACCT_DT per account\n",
    "window_spec = Window.partitionBy(\"CONS_ACCT_KEY\").orderBy(F.col(\"ACCT_DT\").desc())\n",
    "\n",
    "# Pick latest record per account\n",
    "latest_dpd = dpd_summary_df.withColumn(\"rn\", F.row_number().over(window_spec)) \\\n",
    "    .filter(\"rn = 1\") \\\n",
    "    .select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\")\n",
    "\n",
    "# Save it as initial latest_dpd_summary\n",
    "latest_dpd.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\"ascenddb2.latest_dpd_summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99100193-fadd-4624-b9b7-f8886577efb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/09 17:16:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>CONS_ACCT_KEY</th>\n",
       "            <th>ACCT_DT</th>\n",
       "            <th>DPD_GRID</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+---------+----------+\n",
       "| CONS_ACCT_KEY | ACCT_DT | DPD_GRID |\n",
       "+---------------+---------+----------+\n",
       "+---------------+---------+----------+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from ascenddb2.latest_dpd_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "734917cf-0259-432b-90ae-ecf13d80bfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing month: 1998-01-01 ...\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `ascenddb2`.`dpd_summary` because it already exists.\nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:62\u001b[0m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `ascenddb2`.`dpd_summary` because it already exists.\nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define month range\n",
    "start_month = datetime.date(1998, 1, 1)\n",
    "end_month = datetime.date(1998, 1, 1)  # Adjust end date as needed\n",
    "\n",
    "month = start_month\n",
    "result_table = \"ascenddb2.dpd_summary\"\n",
    "\n",
    "while month <= end_month:\n",
    "    next_month = (month.replace(day=28) + datetime.timedelta(days=4)).replace(day=1)\n",
    "    # print(f\"Next Month: {next_month}\")\n",
    "    \n",
    "    print(f\"Processing month: {month} ...\")\n",
    "\n",
    "    # Load current month data\n",
    "    current_df = spark.sql(f\"\"\"\n",
    "        SELECT CONS_ACCT_KEY, ACCT_DT, DPD\n",
    "        FROM ascenddb2.dpd_data\n",
    "        WHERE ACCT_DT >= DATE '{month}' AND ACCT_DT < DATE '{next_month}'\n",
    "    \"\"\")\n",
    "    \n",
    "    # Load only latest record per account (tiny table)\n",
    "    latest_prev = spark.read.table(\"ascenddb2.latest_dpd_summary\").withColumnRenamed(\"ACCT_DT\", \"ACCT_DT_prev\")\n",
    "    \n",
    "    # Join (no longer need to scan historical months)\n",
    "    merged_df = current_df.join(latest_prev, on=\"CONS_ACCT_KEY\", how=\"left\")\n",
    "    \n",
    "    # Continue with DPD_GRID generation as before\n",
    "    merged_df = merged_df.withColumn(\n",
    "        \"MONTH_DIFF\",\n",
    "        (F.month(\"ACCT_DT\") - F.month(\"ACCT_DT_prev\")) +\n",
    "        (F.year(\"ACCT_DT\") - F.year(\"ACCT_DT_prev\")) * 12\n",
    "    ).withColumn(\n",
    "        \"FILLER_ARRAY\",\n",
    "        F.when(F.col(\"MONTH_DIFF\") > 1, F.expr(\"transform(sequence(1, MONTH_DIFF - 1), x -> '?')\"))\n",
    "        .otherwise(F.array())\n",
    "    ).withColumn(\n",
    "        \"Merged_DPD_Array\",\n",
    "        F.concat(\n",
    "            F.array(F.col(\"DPD\")),\n",
    "            F.col(\"FILLER_ARRAY\"),\n",
    "            F.when(F.col(\"DPD_GRID\").isNotNull(), F.split(F.col(\"DPD_GRID\"), \"~\")).otherwise(F.array())\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Pad & trim as usual\n",
    "    merged_df = merged_df.withColumn(\n",
    "        \"DPD_Array_Trimmed\",\n",
    "        F.when(\n",
    "            F.size(\"Merged_DPD_Array\") >= 36,\n",
    "            F.slice(\"Merged_DPD_Array\", 1, 36)\n",
    "        ).otherwise(\n",
    "            F.concat(F.col(\"Merged_DPD_Array\"),\n",
    "                     F.array([F.lit(\"?\") for _ in range(35)])\n",
    "            )\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"DPD_GRID\",\n",
    "        F.concat_ws(\"~\", \"DPD_Array_Trimmed\")\n",
    "    )\n",
    "    \n",
    "    # Save historical output (same as before)\n",
    "    merged_df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD\", \"DPD_GRID\") \\\n",
    "        .write.format(\"iceberg\").mode(\"append\").saveAsTable(\"ascenddb2.dpd_summary\")\n",
    "\n",
    "\n",
    "    # Load current month processed data\n",
    "    current_month_df = merged_df.select(\"CONS_ACCT_KEY\", \"ACCT_DT\", \"DPD_GRID\")\n",
    "    \n",
    "    # Merge: Take latest between old and new per account\n",
    "    merged_latest = latest_df.union(current_month_df) \\\n",
    "        .withColumn(\"rn\", F.row_number().over(\n",
    "            Window.partitionBy(\"CONS_ACCT_KEY\").orderBy(F.col(\"ACCT_DT\").desc())\n",
    "        )) \\\n",
    "        .filter(\"rn = 1\") \\\n",
    "        .drop(\"rn\")\n",
    "    \n",
    "    # 4. Save merged latest back\n",
    "    merged_latest.write.mode(\"overwrite\").saveAsTable(\"ascenddb2.latest_dpd_summary\")\n",
    "    \n",
    "    month = next_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "891a9b7c-e242-49f5-b60d-b13d3619a884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE ascenddb2.dpd_summary;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
